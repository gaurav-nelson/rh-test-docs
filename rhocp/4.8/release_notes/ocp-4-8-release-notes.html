<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>OpenShift Container Platform 4.8 release notes :: Red Hat Docs</title>
    <meta name="generator" content="Antora 2.3.4">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="../../..">Red Hat Docs</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="#">Home</a>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Products</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="#">Product A</a>
            <a class="navbar-item" href="#">Product B</a>
            <a class="navbar-item" href="#">Product C</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Services</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="#">Service A</a>
            <a class="navbar-item" href="#">Service B</a>
            <a class="navbar-item" href="#">Service C</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Resources</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="#">Resource A</a>
            <a class="navbar-item" href="#">Resource B</a>
            <a class="navbar-item" href="#">Resource C</a>
          </div>
        </div>
        <div class="navbar-item">
          <span class="control">
            <a class="button is-primary" href="#">Download</a>
          </span>
        </div>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="rhocp" data-version="4.8">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Red Hat Openshift Container Platform</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">About</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../index.html">Welcome</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../learn_more_about_openshift.html">Learn more about OpenShift</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../oke_about.html">About OpenShift Kubernetes Engine</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../legal-notice.html">Legal notice</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Release notes</span>
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="ocp-4-8-release-notes.html">OpenShift Container Platform 4.8 release notes</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="versioning-policy.html">Versioning policy</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Architectue</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../architecture/architecture.html">Product architecture</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../architecture/architecture-installation.html">Installation and update</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../architecture/control-plane.html">The control plane</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../architecture/understanding-development.html">Understand OpenShift development</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../architecture/architecture-rhcos.html">Red Hat Enterprise Linux CoreOS</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../architecture/admission-plug-ins.html">Admission plug-ins</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Red Hat Openshift Container Platform</span>
    <span class="version">4.8</span>
  </div>
  <ul class="components">
    <li class="component">
      <a class="title" href="../../../rhel8/8/index.html">Red Hat Enterprise Linux</a>
      <ul class="versions">
        <li class="version is-latest">
          <a href="../../../rhel8/8/index.html">8</a>
        </li>
      </ul>
    </li>
    <li class="component is-current">
      <a class="title" href="../index.html">Red Hat Openshift Container Platform</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">4.8</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Red Hat Openshift Container Platform</a></li>
    <li>Release notes</li>
    <li><a href="ocp-4-8-release-notes.html">OpenShift Container Platform 4.8 release notes</a></li>
  </ul>
</nav>
  <div class="edit-this-page"><a href="file:///Users/ganelson/Documents/alt/openshift-docs/modules/release_notes/pages/ocp-4-8-release-notes.adoc">Edit this Page</a></div>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">OpenShift Container Platform 4.8 release notes</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Red Hat OpenShift Container Platform provides developers and IT organizations with a hybrid cloud application platform for deploying both new and existing applications on secure, scalable resources with minimal configuration and management overhead. OpenShift Container Platform supports a wide selection of programming languages and frameworks, such as Java, JavaScript, Python, Ruby, and PHP.</p>
</div>
<div class="paragraph">
<p>Built on Red Hat Enterprise Linux (RHEL) and Kubernetes, OpenShift Container Platform provides a more secure and scalable multi-tenant operating system for today&#8217;s enterprise-class applications, while delivering integrated application runtimes and libraries. OpenShift Container Platform enables organizations to meet security, privacy, compliance, and governance requirements.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="ocp-4-8-about-this-release"><a class="anchor" href="#ocp-4-8-about-this-release"></a>About this release</h2>
<div class="sectionbody">
<div class="paragraph">
<p>OpenShift Container Platform (<a href="https://access.redhat.com/errata/RHSA-2021:2438">RHSA-2021:2438</a>) is now available. This release uses <a href="https://kubernetes.io/docs/setup/release/notes/">Kubernetes 1.21</a> with CRI-O runtime. New features, changes, and known issues that pertain to OpenShift Container Platform 4.8 are included in this topic.</p>
</div>
<div class="paragraph">
<p>Red Hat did not publicly release OpenShift Container Platform 4.8.0 as the GA version and, instead, is releasing OpenShift Container Platform 4.8.2 as the GA version.</p>
</div>
<div class="paragraph">
<p>OpenShift Container Platform 4.8 clusters are available at <a href="https://cloud.redhat.com/openshift" class="bare">https://cloud.redhat.com/openshift</a>. The Red Hat OpenShift Cluster Manager application for OpenShift Container Platform allows you to deploy OpenShift clusters to either on-premise or cloud environments.</p>
</div>
<div class="paragraph">
<p>OpenShift Container Platform 4.8 is supported on Red Hat Enterprise Linux (RHEL) 7.9 or later, as well as on Red Hat Enterprise Linux CoreOS (RHCOS) 4.8.</p>
</div>
<div class="paragraph">
<p>You must use RHCOS machines for the control plane, and you can use either RHCOS or Red Hat Enterprise Linux (RHEL) 7.9 or later for compute machines.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Because only RHEL 7.9 or later is supported for compute machines, you must not upgrade the RHEL compute machines to RHEL 8.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>With the release of OpenShift Container Platform 4.8, version 4.5 is now end of life. For more information, see the <a href="https://access.redhat.com/support/policy/updates/openshift">Red Hat OpenShift Container Platform Life Cycle Policy</a>.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="ocp-4-8-inclusive-language"><a class="anchor" href="#ocp-4-8-inclusive-language"></a>Making open source more inclusive</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Red Hat is committed to replacing problematic language in our code, documentation, and web properties.</p>
</div>
<div class="paragraph">
<p>As part of that effort, with this release the following changes are in place:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The <a href="https://github.com/openshift/openshift-docs">OpenShift Docs GitHub repository</a> <code>master</code> branch has been renamed to <code>main</code>.</p>
</li>
<li>
<p>We have begun to progressively replace the terminology of "master" with "control plane". You will notice throughout the documentation that we use both terms, with "master" in parenthesis. For example "&#8230;&#8203; the control plane node (also known as the master node)". In a future release, we will update this to be "the control plane node".</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="ocp-4-8-add-on-support-status"><a class="anchor" href="#ocp-4-8-add-on-support-status"></a>OpenShift Container Platform layered and dependant component support and compatibility</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The scope of support for layered and dependant components of OpenShift Container Platform changes independently of the OpenShift Container Platform version. To determine the current support status and compatibility for an add-on, refer to its release notes. For more information, see the <a href="https://access.redhat.com/support/policy/updates/openshift">Red Hat OpenShift Container Platform Life Cycle Policy</a>.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="ocp-4-8-new-features-and-enhancements"><a class="anchor" href="#ocp-4-8-new-features-and-enhancements"></a>New features and enhancements</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This release adds improvements related to the following components and concepts.</p>
</div>
<div class="sect2">
<h3 id="ocp-4-8-rhcos"><a class="anchor" href="#ocp-4-8-rhcos"></a>Red Hat Enterprise Linux CoreOS (RHCOS)</h3>
<div class="sect3">
<h4 id="ocp-4-8-rhcos-rhel-8-4-packages"><a class="anchor" href="#ocp-4-8-rhcos-rhel-8-4-packages"></a>RHCOS now supports RHEL 8.4</h4>
<div class="paragraph">
<p>RHCOS is now using Red Hat Enterprise Linux (RHEL) 8.4 packages. This enables you to have the latest fixes, features, and enhancements, as well as the latest hardware support and driver updates. OpenShift Container Platform 4.7 is currently using RHEL 8.3 packages and will upgrade to RHEL 8.4 in a future update. OpenShift Container Platform 4.6 is an Extended Update Support (EUS) release that will continue to use RHEL 8.2 EUS packages for the entirety of its lifecycle.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-stream-metadata"><a class="anchor" href="#ocp-4-8-stream-metadata"></a>Using stream metadata for improved boot image automation</h4>
<div class="paragraph">
<p>Stream metadata provides a standardized JSON format for injecting metadata into the cluster during OpenShift Container Platform installation. For improved automation, the new <code>openshift-install coreos print-stream-json</code> command provides a method for printing stream metadata in a scriptable, machine-readable format.</p>
</div>
<div class="paragraph">
<p>For user-provisioned installations, the <code>openshift-install</code> binary contains references to the version of RHCOS boot images that are tested for use with OpenShift Container Platform, such as the AWS AMI. You can now parse the stream metadata from a Go program by using the official <code>stream-metadata-go</code> library at <a href="https://github.com/coreos/stream-metadata-go" class="bare">https://github.com/coreos/stream-metadata-go</a>.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../installing/installing_aws/installing-aws-user-infra.adoc#installation-aws-ami-stream-metadata_installing-aws-user-infra" class="page unresolved">Accessing RHCOS AMIs with stream metadata</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-rhcos-butane"><a class="anchor" href="#ocp-4-8-rhcos-butane"></a>Butane config transpiler simplifies creation of machine configs</h4>
<div class="paragraph">
<p>OpenShift Container Platform now includes the Butane config transpiler to assist with producing and validating machine configs. Documentation now recommends using Butane to create machine configs for LUKS disk encryption, boot disk mirroring, and custom kernel modules.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../installing/install_config/installing-customizing.adoc#installation-special-config-butane_installing-customizing" class="page unresolved">Creating machine configs with Butane</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-rhcos-chrony-default"><a class="anchor" href="#ocp-4-8-rhcos-chrony-default"></a>Change to custom chrony.conf default on cloud platforms</h4>
<div class="paragraph">
<p>If a cloud administrator has already set a custom <code>/etc/chrony.conf</code> configuration, RHCOS no longer sets the <code>PEERNTP=no</code> option by default on cloud platforms. Otherwise, the <code>PEERNTP=no</code> option is still set by default. See <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1924869">BZ#1924869</a> for more information.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-rhcos-multipath-install"><a class="anchor" href="#ocp-4-8-rhcos-multipath-install"></a>Enabling multipathing at bare metal installation time</h4>
<div class="paragraph">
<p>Enabling multipathing during bare metal installation is now supported for nodes provisioned in OpenShift Container Platform 4.8 or higher. You can enable multipathing by appending kernel arguments to the <code>coreos-installer install</code> command so that the installed system itself uses multipath beginning from the first boot. While post-installation support is still available by activating multipathing via the machine config, enabling multipathing during installation is recommended for nodes provisioned starting in OpenShift Container Platform 4.8.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../installing/installing_bare_metal/installing-bare-metal.adoc#rhcos-enabling-multipath_installing-bare-metal" class="page unresolved">Enabling multipathing with kernel arguments on RHCOS</a>.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="ocp-4-8-installation-and-upgrade"><a class="anchor" href="#ocp-4-8-installation-and-upgrade"></a>Installation and upgrade</h3>
<div class="sect3">
<h4 id="ocp-4-8-installation-azure-empty-rg"><a class="anchor" href="#ocp-4-8-installation-azure-empty-rg"></a>Installing a cluster to an existing, empty resource group on Azure</h4>
<div class="paragraph">
<p>You can now define an already existing resource group to install your cluster to on Azure by defining the <code>platform.azure.resourceGroupName</code> field in the <code>install-config.yaml</code> file. This resource group must be empty and only used for a single cluster; the cluster components assume ownership of all resources in the resource group.</p>
</div>
<div class="paragraph">
<p>If you limit the service principal scope of the installation program to this resource group, you must ensure all other resources used by the installation program in your environment have the necessary permissions, such as the public DNS zone and virtual network. Destroying the cluster using the installation program deletes the user-defined resource group.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-installation-existing-iam-roles-aws"><a class="anchor" href="#ocp-4-8-installation-existing-iam-roles-aws"></a>Using existing IAM roles for clusters on AWS</h4>
<div class="paragraph">
<p>You can now define a pre-existing Amazon Web Services (AWS) IAM role for your machine instance profiles by setting the <code>compute.platform.aws.iamRole</code> and <code>controlPlane.platform.aws.iamRole</code> fields in the <code>install-config.yaml</code> file. This allows you to do the following for your IAM roles:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Match naming schemes</p>
</li>
<li>
<p>Include predefined permissions boundaries</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-installation-pre-existing-hosted-zones-aws"><a class="anchor" href="#ocp-4-8-installation-pre-existing-hosted-zones-aws"></a>Using pre-existing Route53 hosted private zones on AWS</h4>
<div class="paragraph">
<p>You can now define an existing Route 53 private hosted zone for your cluster by setting the <code>platform.aws.hostedZone</code> field in the <code>install-config.yaml</code> file. You can only use a pre-existing hosted zone when also supplying your own VPC.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-installation-increase-gcp-subnets-within-machine-cidr"><a class="anchor" href="#ocp-4-8-installation-increase-gcp-subnets-within-machine-cidr"></a>Increasing the size of GCP subnets within the machine CIDR</h4>
<div class="paragraph">
<p>The OpenShift Container Platform installation program for Google Cloud Platform (GCP) now creates subnets as large as possible within the machine CIDR. This allows the cluster to use a machine CIDR appropriately sized to accommodate the number of nodes in the cluster.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-improved-upgrade-duration"><a class="anchor" href="#ocp-4-8-improved-upgrade-duration"></a>Improved upgrade duration</h4>
<div class="paragraph">
<p>With this release, the upgrade duration for cluster Operators that deploy daemon sets to all nodes is significantly reduced. For example, the upgrade duration of a 250-node test cluster is reduced from 7.5 hours to 1.5 hours, resulting in upgrade duration scaling of less than one minute per additional node.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>This change does not affect machine config pool rollout duration.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-mco-upgrade-complete"><a class="anchor" href="#ocp-4-8-mco-upgrade-complete"></a>MCO waits for all machine config pools to update before reporting the update is complete</h4>
<div class="paragraph">
<p>When updating, the Machine Config Operator (MCO) now reports an <code>Upgradeable=False</code> condition in the machine-config Cluster Operator if any machine config pool has not completed updating. This status blocks future minor updates, but does not block future patch updates, or the current update. Previously, the MCO reported the <code>Upgradeable</code> status based only on the state of the control plane machine config pool, even if the worker pools were not done updating.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-installation-fujitsu-irmc"><a class="anchor" href="#ocp-4-8-installation-fujitsu-irmc"></a>Using Fujitsu iRMC for installation on bare metal nodes</h4>
<div class="paragraph">
<p>In OpenShift Container Platform 4.8, you can use Fujitsu hardware and the Fujitsu iRMC base board management controller protocol when deploying installer-provisioned clusters on bare metal. Currently Fujitsu supports iRMC S5 firmware version <code>3.05P</code> and above for installer-provisioned installation on bare metal. Enhancements and bug fixes for OpenShift Container Platform 4.8 include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Supporting soft power-off on iRMC hardware.</p>
</li>
<li>
<p>Stopping the provisioning services once the installer deploys the control plane on the bare metal nodes. See <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1949859">BZ#1949859</a> for more information.</p>
</li>
<li>
<p>Adding an Ironic health check to the bootstrap <code>keepalived</code> checks. See <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1949859">BZ#1949859</a> for more information.</p>
</li>
<li>
<p>Verifying that the unicast peers list isn&#8217;t empty on the control plane nodes. See <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1957708">BZ#1957708</a> for more information.</p>
</li>
<li>
<p>Updating the Bare Metal Operator to align the iRMC PowerInterface. See <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1957869">BZ#1957869</a> for more information.</p>
</li>
<li>
<p>Updating the <code>pyghmi</code> library version. See <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1920294">BZ#1920294</a> for more information.</p>
</li>
<li>
<p>Updating the Bare Metal Operator to address missing IPMI credentials. See <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1965182">BZ#1965182</a> for more information.</p>
</li>
<li>
<p>Removing iRMC from <code>enabled_bios_interfaces</code>. See <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1969212">BZ#1969212</a> for more information.</p>
</li>
<li>
<p>Adding <code>ironicTlsMount</code> and <code>inspectorTlsMount</code> to the
the bare metal pod definition. See <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1968701">BZ#1968701</a> for more information.</p>
</li>
<li>
<p>Disabling the RAID feature for iRMC server. See <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1969487">BZ#1969487</a> for more information.</p>
</li>
<li>
<p>Disabling RAID for all drivers. See <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1969487">BZ#1969487</a> for more information.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-installation-openstack-installer-provisioned-sr-iov"><a class="anchor" href="#ocp-4-8-installation-openstack-installer-provisioned-sr-iov"></a>SR-IOV network support for clusters with installer-provisioned infrastructure on RHOSP</h4>
<div class="paragraph">
<p>You can now deploy clusters on RHOSP that use single-root I/O virtualization (SR-IOV) networks for compute machines.</p>
</div>
<div class="paragraph">
<p>See <a href="#../installing/installing_openstack/installing-openstack-installer-sr-iov.adoc#installing-openstack-installer-sr-iov" class="page unresolved">
Installing a cluster on OpenStack that supports SR-IOV-connected compute machines
</a> for more information.</p>
</div>
</div>
<div class="sect3">
<h4 id="installation-ironic-agent-vlan-support"><a class="anchor" href="#installation-ironic-agent-vlan-support"></a>Ironic Python Agent support for VLAN interfaces</h4>
<div class="paragraph">
<p>With this update, the Ironic Python Agent now reports VLAN interfaces in the list of interfaces during introspection. Additionally, the IP address is included on the interfaces, which allows for proper creation of a CSR. As a result, a CSR can be obtained for all interfaces, including VLAN interfaces. For more information, see <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1888712">BZ#1888712</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-osus"><a class="anchor" href="#ocp-4-8-osus"></a>Over-the-air updates with the OpenShift Update Service</h4>
<div class="paragraph">
<p>The OpenShift Update Service (OSUS) provides over-the-air updates to OpenShift Container Platform, including Red Hat Enterprise Linux CoreOS (RHCOS). It was previously only accessible as a Red Hat hosted service located behind public APIs, but can now be installed locally. The OpenShift Update Service is composed of an Operator and one or more application instances and is now generally available in OpenShift Container Platform 4.6 and higher.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../updating/understanding-the-update-service.adoc#understanding-the-update-service" class="page unresolved">Understanding the OpenShift Update Service</a>.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="ocp-4-8-web-console"><a class="anchor" href="#ocp-4-8-web-console"></a>Web console</h3>
<div class="sect3">
<h4 id="ocp-4-8-custom-console-routes-use-custom-domains-cluster-api"><a class="anchor" href="#ocp-4-8-custom-console-routes-use-custom-domains-cluster-api"></a>Custom console routes now use the new CustomDomains cluster API</h4>
<div class="paragraph">
<p>For <code>console</code> and <code>downloads</code> routes, custom routes functionality is now implemented to use the new <code>ingress</code> config route configuration API <code>spec.componentRoutes</code>. The Console Operator config already contained custom route customization, but for the <code>console</code> route only. The route configuration via <code>console-operator</code> config is being deprecated. Therefore, if the <code>console</code> custom route is set up in both the <code>ingress</code> config and <code>console-operator</code> config, then the new <code>ingress</code> config custom route configuration takes precedent.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../web_console/customizing-the-web-console.adoc#customizing-the-web-console-url_customizing-web-console" class="page unresolved">Customizing console routes</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-access-code-snippet-from-quick-start"><a class="anchor" href="#ocp-4-8-access-code-snippet-from-quick-start"></a>Access a code snippet from a quick start</h4>
<div class="paragraph">
<p>You can now execute a CLI snippet when it is included in a quick start from the web console. To use this feature, you must first install the Web Terminal Operator. The web terminal and code snippet actions that execute in the web terminal are not present if you do not install the Web Terminal Operator. Alternatively, you can copy a code snippet to the clipboard regardless of whether you have the Web Terminal Operator installed or not.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-improved-presentation-quick-start-prereqs"><a class="anchor" href="#ocp-4-8-improved-presentation-quick-start-prereqs"></a>Improved presentation of quick start prerequisites</h4>
<div class="paragraph">
<p>Previously, quick start prerequisites were displayed as combined text instead of a list on the quick start card. With scalability in mind, the prerequisites are now presented in a popover rather than on the card.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="../_images/quick-start-prerequisites.png" alt="quick start prerequisites displayed as popover" width="338" height="336">
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="ocp-4-8-ibm-z"><a class="anchor" href="#ocp-4-8-ibm-z"></a>IBM Z and LinuxONE</h3>
<div class="paragraph">
<p>With this release, IBM Z and LinuxONE are now compatible with OpenShift Container Platform 4.8. The installation can be performed with z/VM or RHEL KVM. For installation instructions, see the following documentation:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="#../installing/installing_ibm_z/installing-ibm-z.adoc#installing-ibm-z" class="page unresolved">Installing a cluster with z/VM on IBM Z and LinuxONE</a></p>
</li>
<li>
<p><a href="#../installing/installing_ibm_z/installing-restricted-networks-ibm-z.adoc#installing-restricted-networks-ibm-z" class="page unresolved">Installing a cluster with z/VM on IBM Z and LinuxONE in a restricted network</a></p>
</li>
<li>
<p><a href="#../installing/installing_ibm_z/installing-ibm-z-kvm.adoc#installing-ibm-z-kvm" class="page unresolved">Installing a cluster with RHEL KVM on IBM Z and LinuxONE</a></p>
</li>
<li>
<p><a href="#../installing/installing_ibm_z/installing-restricted-networks-ibm-z-kvm.adoc#installing-restricted-networks-ibm-z-kvm" class="page unresolved">Installing a cluster with RHEL KVM on IBM Z and LinuxONE in a restricted network</a></p>
</li>
</ul>
</div>
<h4 id="_notable_enhancements" class="discrete">Notable enhancements</h4>
<div class="paragraph">
<p>The following new features are supported on IBM Z and LinuxONE with OpenShift Container Platform 4.8:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>KVM on RHEL 8.3 or later is supported as a hypervisor for user-provisioned installation of OpenShift Container Platform 4.8 on IBM Z and LinuxONE. Installation with static IP addresses as well as installation in a restricted network are now also supported.</p>
</li>
<li>
<p>Encrypting data stored in etcd.</p>
</li>
<li>
<p>4K FCP block device.</p>
</li>
<li>
<p>Three-node cluster support.</p>
</li>
</ul>
</div>
<h4 id="_supported_features" class="discrete">Supported features</h4>
<div class="paragraph">
<p>The following features are also supported on IBM Z and LinuxONE:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Multipathing</p>
</li>
<li>
<p>Persistent storage using iSCSI</p>
</li>
<li>
<p>Persistent storage using local volumes (Local Storage Operator)</p>
</li>
<li>
<p>Persistent storage using hostPath</p>
</li>
<li>
<p>Persistent storage using Fibre Channel</p>
</li>
<li>
<p>Persistent storage using Raw Block</p>
</li>
<li>
<p>OVN-Kubernetes with an initial installation of OpenShift Container Platform 4.8</p>
</li>
<li>
<p>z/VM Emulated FBA devices on SCSI disks</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>These features are available only for OpenShift Container Platform on IBM Z for 4.8:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>HyperPAV enabled on IBM Z /LinuxONE for the virtual machines for FICON attached ECKD storage</p>
</li>
</ul>
</div>
<h4 id="_restrictions" class="discrete">Restrictions</h4>
<div class="paragraph">
<p>Note the following restrictions for OpenShift Container Platform on IBM Z and LinuxONE:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>OpenShift Container Platform for IBM Z does not include the following Technology Preview features:</p>
<div class="ulist">
<ul>
<li>
<p>Precision Time Protocol (PTP) hardware</p>
</li>
</ul>
</div>
</li>
<li>
<p>The following OpenShift Container Platform features are unsupported:</p>
<div class="ulist">
<ul>
<li>
<p>Automatic repair of damaged machines with machine health checking</p>
</li>
<li>
<p>CodeReady Containers (CRC)</p>
</li>
<li>
<p>Controlling overcommit and managing container density on nodes</p>
</li>
<li>
<p>CSI volume cloning</p>
</li>
<li>
<p>CSI volume snapshots</p>
</li>
<li>
<p>FIPS cryptography</p>
</li>
<li>
<p>Helm command-line interface (CLI) tool</p>
</li>
<li>
<p>Multus CNI plug-in</p>
</li>
<li>
<p>NVMe</p>
</li>
<li>
<p>OpenShift Metering</p>
</li>
<li>
<p>OpenShift Virtualization</p>
</li>
<li>
<p>Tang mode disk encryption during OpenShift Container Platform deployment</p>
</li>
</ul>
</div>
</li>
<li>
<p>Worker nodes must run Red Hat Enterprise Linux CoreOS (RHCOS)</p>
</li>
<li>
<p>Persistent shared storage must be provisioned by using either NFS or other supported storage protocols</p>
</li>
<li>
<p>Persistent non-shared storage must be provisioned using local storage, like iSCSI, FC, or using LSO with DASD, FCP, or EDEV/FBA</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="ocp-4-8-ibm-power"><a class="anchor" href="#ocp-4-8-ibm-power"></a>IBM Power Systems</h3>
<div class="paragraph">
<p>With this release, IBM Power Systems are now compatible with OpenShift Container Platform 4.8. For installation instructions, see the following documentation:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="#../installing/installing_ibm_power/installing-ibm-power.adoc#installing-ibm-power_installing-ibm-power" class="page unresolved">Installing a cluster on IBM Power Systems</a></p>
</li>
<li>
<p><a href="#../installing/installing_ibm_power/installing-restricted-networks-ibm-power.adoc#installing-restricted-networks-ibm-power_installing-restricted-networks-ibm-power" class="page unresolved">Installing a cluster on IBM Power Systems in a restricted network</a></p>
</li>
</ul>
</div>
<h4 id="_notable_enhancements_2" class="discrete">Notable enhancements</h4>
<div class="paragraph">
<p>The following new features are supported on IBM Power Systems with OpenShift Container Platform 4.8:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Encrypting data stored in etcd</p>
</li>
<li>
<p>Three-node cluster support</p>
</li>
<li>
<p>Multus SR-IOV</p>
</li>
</ul>
</div>
<h4 id="_supported_features_2" class="discrete">Supported features</h4>
<div class="paragraph">
<p>The following features are also supported on IBM Power Systems:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Currently, five Operators are supported:</p>
<div class="ulist">
<ul>
<li>
<p>Cluster-Logging-Operator</p>
</li>
<li>
<p>Cluster-NFD-Operator</p>
</li>
<li>
<p>Elastic Search-Operator</p>
</li>
<li>
<p>Local Storage Operator</p>
</li>
<li>
<p>SR-IOV Network Operator</p>
</li>
</ul>
</div>
</li>
<li>
<p>Multipathing</p>
</li>
<li>
<p>Persistent storage using iSCSI</p>
</li>
<li>
<p>Persistent storage using local volumes (Local Storage Operator)</p>
</li>
<li>
<p>Persistent storage using hostPath</p>
</li>
<li>
<p>Persistent storage using Fibre Channel</p>
</li>
<li>
<p>Persistent storage using Raw Block</p>
</li>
<li>
<p>OVN-Kubernetes with an initial installation of OpenShift Container Platform 4.8</p>
</li>
<li>
<p>4K Disk Support</p>
</li>
<li>
<p>NVMe</p>
</li>
</ul>
</div>
<h4 id="_restrictions_2" class="discrete">Restrictions</h4>
<div class="paragraph">
<p>Note the following restrictions for OpenShift Container Platform on IBM Power Systems:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>OpenShift Container Platform for IBM Power Systems does not include the following Technology Preview features:</p>
<div class="ulist">
<ul>
<li>
<p>Precision Time Protocol (PTP) hardware</p>
</li>
</ul>
</div>
</li>
<li>
<p>The following OpenShift Container Platform features are unsupported:</p>
<div class="ulist">
<ul>
<li>
<p>Automatic repair of damaged machines with machine health checking</p>
</li>
<li>
<p>CodeReady Containers (CRC)</p>
</li>
<li>
<p>Controlling overcommit and managing container density on nodes</p>
</li>
<li>
<p>FIPS cryptography</p>
</li>
<li>
<p>Helm command-line interface (CLI) tool</p>
</li>
<li>
<p>OpenShift Metering</p>
</li>
<li>
<p>OpenShift Virtualization</p>
</li>
<li>
<p>Tang mode disk encryption during OpenShift Container Platform deployment</p>
</li>
</ul>
</div>
</li>
<li>
<p>Worker nodes must run Red Hat Enterprise Linux CoreOS (RHCOS)</p>
</li>
<li>
<p>Persistent storage must be of the Filesystem type that uses local volumes, Network File System (NFS), or Container Storage Interface (CSI)</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="ocp-4-8-security"><a class="anchor" href="#ocp-4-8-security"></a>Security and compliance</h3>
<div class="sect3">
<h4 id="ocp-4-8-security-log-oauth-tokens"><a class="anchor" href="#ocp-4-8-security-log-oauth-tokens"></a>Audit logging for OAuth access token logout requests</h4>
<div class="paragraph">
<p>The <code>Default</code> audit log policy now logs request bodies for OAuth access token creation (login) and deletion (logout) requests. Previously, deletion request bodies were not logged.</p>
</div>
<div class="paragraph">
<p>For more information about audit log policies, see <a href="#../security/audit-log-policy-config.adoc#audit-log-policy-config" class="page unresolved">Configuring the node audit log policy</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-security-wildcard-subjects-headless-services"><a class="anchor" href="#ocp-4-8-security-wildcard-subjects-headless-services"></a>Wildcard subject for service serving certificates for headless services</h4>
<div class="paragraph">
<p>Generating a service serving certificate for headless services now includes a wildcard subject in the format of <code>*.&lt;service.name&gt;.&lt;service.namespace&gt;.svc</code>. This allows for TLS-protected connections to individual stateful set pods without having to manually generate certificates for these pods.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Because the generated certificates contain wildcard subjects for headless services, do not use the service CA if your client must differentiate between individual pods. In this case:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Generate individual TLS certificates by using a different CA.</p>
</li>
<li>
<p>Do not accept the service CA as a trusted CA for connections that are directed to individual pods and must not be impersonated by other pods. These connections must be configured to trust the CA that was used to generate the individual TLS certificates.</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../security/certificates/service-serving-certificate.adoc#add-service-certificate_service-serving-certificate" class="page unresolved">Add a service certificate</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-security-oc-compliance-plug-in"><a class="anchor" href="#ocp-4-8-security-oc-compliance-plug-in"></a>The oc-compliance plug-in is now available</h4>
<div class="paragraph">
<p>The <a href="#../security/compliance_operator/compliance-operator-understanding.adoc#understanding-compliance-operator" class="page unresolved">Compliance Operator</a> automates many of the checks and remediations for an OpenShift Container Platform cluster. However, the full process of bringing a cluster into compliance often requires administrator interaction with the Compliance Operator API and other components. The <code>oc-compliance</code> plug-in is now available and makes the process easier.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../security/oc_compliance_plug_in/oc-compliance-plug-in-using.adoc#using-oc-compliance-plug-in" class="page unresolved">Using the <code>oc-compliance</code> plug-in</a></p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-security-tls-security-profile-control-plane"><a class="anchor" href="#ocp-4-8-security-tls-security-profile-control-plane"></a>TLS security profile for the Kubernetes control plane</h4>
<div class="paragraph">
<p>The Kubernetes API server TLS security profile setting is now also honored by the Kubernetes scheduler and Kubernetes controller manager.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../security/tls-security-profiles.adoc#tls-security-profiles" class="page unresolved">Configuring TLS security profiles</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-security-tls-security-profile-kubelet"><a class="anchor" href="#ocp-4-8-security-tls-security-profile-kubelet"></a>TLS security profile for the kubelet as a server</h4>
<div class="paragraph">
<p>You can now set a TLS security profile for kubelet when it acts as an HTTP server for the Kubernetes API server.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../security/tls-security-profiles.adoc#tls-security-profiles" class="page unresolved">Configuring TLS security profiles</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-security-bcrypt-hashing"><a class="anchor" href="#ocp-4-8-security-bcrypt-hashing"></a>Support for <code>bcrypt</code> password hashing</h4>
<div class="paragraph">
<p>Previously, the <code>oauth-proxy</code> command only allowed the use of SHA-1 hashed passwords in <code>htpasswd</code> files used for authentication. <code>oauth-proxy</code> now includes support for <code>htpasswd</code> entries that use <code>bcrypt</code> password hashing. For more information, see <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1874322">BZ#1874322</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-managed-secure-boot"><a class="anchor" href="#ocp-4-8-managed-secure-boot"></a>Enabling managed Secure Boot with installer-provisioned clusters</h4>
<div class="paragraph">
<p>OpenShift Container Platform 4.8 supports automatically turning on UEFI Secure Boot mode for provisioned control plane and worker nodes and turning it back off when removing the nodes. To use this feature, set the node&#8217;s <code>bootMode</code> configuration setting to <code>UEFISecureBoot</code> in the <code>install-config.yaml</code> file. Red Hat only supports installer-provisioned installation with managed Secure Boot on 10th generation HPE hardware or 13th generation Dell hardware running firmware version <code>2.75.75.75</code> or greater. For additional details, see <a href="#../installing/installing_bare_metal_ipi/ipi-install-installation-workflow.html#configuring-managed-secure-boot-in-the-install-config-file_ipi-install-configuration-files" class="page unresolved">Configuring managed Secure Boot in the install-config.yaml file</a>.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="ocp-4-8-networking"><a class="anchor" href="#ocp-4-8-networking"></a>Networking</h3>
<div class="sect3">
<h4 id="ocp-4-8-dual-stack-support-bare-metal-ovn-kubernetes-network-provider"><a class="anchor" href="#ocp-4-8-dual-stack-support-bare-metal-ovn-kubernetes-network-provider"></a>Dual-stack support on installer-provisioned bare metal infrastructure with the OVN-Kubernetes cluster network provider</h4>
<div class="paragraph">
<p>For clusters on installer-provisioned <a href="#../installing/installing_bare_metal/installing-bare-metal-network-customizations.adoc#installation-configuration-parameters-network_installing-bare-metal-network-customizations" class="page unresolved">bare metal infrastructure</a>, the OVN-Kubernetes cluster network provider supports both IPv4 and IPv6 address families.</p>
</div>
<div class="paragraph">
<p>For installer-provisioned bare metal clusters upgrading from previous versions of OpenShift Container Platform, you must convert your cluster to support dual-stack networking. For more information, see <a href="#../networking/ovn_kubernetes_network_provider/converting-to-dual-stack.adoc#converting-to-dual-stack" class="page unresolved">Converting to IPv4/IPv6 dual stack networking</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-migrate-openshift-sdn-ovn-kubernetes-user-provisioned"><a class="anchor" href="#ocp-4-8-migrate-openshift-sdn-ovn-kubernetes-user-provisioned"></a>Migrate from OpenShift SDN to OVN-Kubernetes on user-provisioned infrastructure</h4>
<div class="paragraph">
<p>An OpenShift SDN cluster network provider migration to the OVN-Kubernetes cluster network provider is supported for user-provisioned clusters. For more information, see <a href="#../networking/ovn_kubernetes_network_provider/migrate-from-openshift-sdn.adoc#nw-ovn-kubernetes-migration-about_migrate-from-openshift-sdn" class="page unresolved">Migrate from the OpenShift SDN cluster network provider</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-openshift-sdn-egress-ips-balance"><a class="anchor" href="#ocp-4-8-openshift-sdn-egress-ips-balance"></a>OpenShift SDN cluster network provider egress IP feature balances across nodes</h4>
<div class="paragraph">
<p>The egress IP feature of OpenShift SDN now balances network traffic roughly equally across nodes for a given namespace, if that namespace is assigned multiple egress IP addresses. Each IP address must reside on a different node.
For more information, refer to <a href="#../networking/openshift_sdn/assigning-egress-ips.adoc#assigning-egress-ips" class="page unresolved">Configuring egress IPs for a project</a> for OpenShift SDN.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-network-policy-host-network-ingress-controllers"><a class="anchor" href="#ocp-4-8-network-policy-host-network-ingress-controllers"></a>Network policy supports selecting host network Ingress Controllers</h4>
<div class="paragraph">
<p>When using the OpenShift SDN or OVN-Kubernetes cluster network providers, you can select traffic from Ingress Controllers in a network policy rule regardless of whether an Ingress Controller runs on the cluster network or the host network.
In a network policy rule, the <code>network.openshift.io/policy-group: ingress</code> namespace selector matches traffic from an Ingress Controller.</p>
</div>
<div class="paragraph">
<p>In earlier releases of OpenShift Container Platform the following limitations existed:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A cluster that uses the OpenShift SDN cluster network provider could select traffic from an Ingress Controller on the host network only by applying the <code>network.openshift.io/policy-group: ingress</code> label to the <code>default</code> namespace.</p>
</li>
<li>
<p>A cluster that uses the OVN-Kubernetes cluster network provider could not select traffic from an Ingress Controller on the host network.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For more information, refer to <a href="#../networking/network_policy/about-network-policy.adoc#about-network-policy" class="page unresolved">About network policy</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-network-policy-host-network-policy-group"><a class="anchor" href="#ocp-4-8-network-policy-host-network-policy-group"></a>Network policy supports selecting host network traffic</h4>
<div class="paragraph">
<p>When using either the OVN-Kubernetes cluster network provider or the OpenShift SDN cluster network provider, you can use the <code>policy-group.network.openshift.io/host-network: ""</code> namespace selector to select host network traffic in a network policy rule.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-network-policy-audit-logs"><a class="anchor" href="#ocp-4-8-network-policy-audit-logs"></a>Network policy audit logs</h4>
<div class="paragraph">
<p>If you use the OVN-Kubernetes cluster network provider, you can enable audit logging for network policies in a namespace. The logs are in a syslog compatible format and can be saved locally, sent over a UDP connection, or directed to a UNIX domain socket. You can specify whether to log allowed, dropped, or both allowed and dropped connections.
For more information, see <a href="#../networking/network_policy/logging-network-policy.adoc#logging-network-policy" class="page unresolved">Logging network policy events</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-macvlan-multi-network-policy"><a class="anchor" href="#ocp-4-8-macvlan-multi-network-policy"></a>Network policy support for macvlan additional networks</h4>
<div class="paragraph">
<p>You can create network policies that apply to macvlan additional networks by using the <code>MultiNetworkPolicy</code> API, which implements the <code>NetworkPolicy</code> API.
For more information, see <a href="#../networking/multiple_networks/configuring-multi-network-policy.adoc#configuring-multi-network-policy" class="page unresolved">Configuring multi-network policy</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-supported-hardware-sriov"><a class="anchor" href="#ocp-4-8-supported-hardware-sriov"></a>Supported hardware for SR-IOV</h4>
<div class="paragraph">
<p>OpenShift Container Platform 4.8 adds support for additional Intel and Mellanox hardware.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Intel X710 and XL710 controllers</p>
</li>
<li>
<p>Four Intel E810 family controllers: E810-CQDA2, E810-2CQDA2, E810-XXVDA2, E810-XXVDA4</p>
</li>
<li>
<p>Mellanox ConnectX-5 Ex</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For more information, see the <a href="#../networking/hardware_networks/about-sriov.adoc#supported-devices_about-sriov" class="page unresolved">supported devices</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-enhancements-sriov-network-operator"><a class="anchor" href="#ocp-4-8-enhancements-sriov-network-operator"></a>Enhancements to the SR-IOV Network Operator</h4>
<div class="paragraph">
<p>The Network Resources Injector that is deployed with the Operator is enhanced to expose information about huge pages requests and limits with the Downward API. When a pod specification includes a huge pages request or limit, the information is exposed in the <code>/etc/podnetinfo</code> path.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../networking/hardware_networks/about-sriov.adoc#nw-sriov-hugepages_about-sriov" class="page unresolved">Huge pages resource injection for Downward API</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-tracking-network-flows"><a class="anchor" href="#ocp-4-8-tracking-network-flows"></a>Tracking network flows</h4>
<div class="paragraph">
<p>OpenShift Container Platform 4.8 adds support for sending the metadata about network flows on the pod network to a network flows collector. The following protocols are supported:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>NetFlow</p>
</li>
<li>
<p>sFlow</p>
</li>
<li>
<p>IPFIX</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Packet data is not sent to the network flows collector. Packet-level metadata such as the protocol, source address, destination address, port numbers, number of bytes, and other packet-level information is sent to the network flows collector.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../networking/ovn_kubernetes_network_provider/tracking-network-flows.adoc#tracking-network-flows" class="page unresolved">Tracking network flows</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-coredns-mdns"><a class="anchor" href="#ocp-4-8-coredns-mdns"></a>CoreDNS-mDNS no longer used to resolve node names to IP addresses</h4>
<div class="paragraph">
<p>OpenShift Container Platform 4.8 and later releases include functionality that uses cluster membership information to generate A/AAAA records. This resolves the node names to their IP addresses. Once the nodes are registered with the API, the cluster can disperse node information without using CoreDNS-mDNS. This eliminates the network traffic associated with multicast DNS.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-preparation-haproxy-2.2"><a class="anchor" href="#ocp-4-8-preparation-haproxy-2.2"></a>Converting HTTP header names to support upgrading to OpenShift Container Platform 4.8</h4>
<div class="paragraph">
<p>OpenShift Container Platform updated to HAProxy 2.2, which changes HTTP header names to lowercase by default, for example, changing <code>Host: xyz.com</code> to <code>host: xyz.com</code>. For legacy applications that are sensitive to the capitalization of HTTP header names, use the Ingress Controller <code>spec.httpHeaders.headerNameCaseAdjustments</code> API field to accommodate legacy applications until they can be fixed. Make sure to add the necessary configuration by using <code>spec.httpHeaders.headerNameCaseAdjustments</code> before upgrading OpenShift Container Platform now that HAProxy 2.2 is available.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../networking/ingress-operator.adoc#nw-ingress-converting-http-header-case_configuring-ingress" class="page unresolved">Converting HTTP header case</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-ingress-configuring-global-access-gcp"><a class="anchor" href="#ocp-4-8-ingress-configuring-global-access-gcp"></a>Configuring global access for an Ingress Controller on GCP</h4>
<div class="paragraph">
<p>OpenShift Container Platform 4.8 adds support for the global access option for Ingress Controllers created on GCP with an internal load balancer. When the global access option is enabled, clients in any region within the same VPC network and compute region as the load balancer can reach the workloads running on your cluster.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../networking/ingress-operator.adoc#nw-ingress-controller-configuration-gcp-global-access_configuring-ingress" class="page unresolved">Configuring global access for an Ingress Controller on GCP</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-setting-ingress-configuring-thread-count"><a class="anchor" href="#ocp-4-8-setting-ingress-configuring-thread-count"></a>Setting Ingress Controller thread count</h4>
<div class="paragraph">
<p>OpenShift Container Platform 4.8 adds support for setting the thread count to increase the amount of incoming connections a cluster can handle.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../networking/ingress-operator.adoc#nw-ingress-setting-thread-count_configuring-ingress" class="page unresolved">Setting Ingress Controller thread count</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-ingress-configuring-proxy-protocol"><a class="anchor" href="#ocp-4-8-ingress-configuring-proxy-protocol"></a>Configuring the PROXY protocol for an Ingress Controller</h4>
<div class="paragraph">
<p>OpenShift Container Platform 4.8 adds support for configuring the PROXY protocol for the Ingress Controller on non-cloud platforms, specifically for <code>HostNetwork</code> or <code>NodePortService</code> endpoint publishing strategy types.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../networking/ingress-operator.adoc#nw-ingress-controller-configuration-proxy-protocol_configuring-ingress" class="page unresolved">Configuring PROXY protocol for an Ingress Controller</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-networking-ntp-on-control-plane"><a class="anchor" href="#ocp-4-8-networking-ntp-on-control-plane"></a>NTP servers on control plane nodes</h4>
<div class="paragraph">
<p>In OpenShift Container Platform 4.8, installer-provisioned clusters can configure and deploy Network Time Protocol (NTP) servers on the control plane nodes and NTP clients on worker nodes. This enables workers to retrieve the date and time from the NTP servers on the control plane nodes, even when disconnected from a routable network. You can also configure and deploy NTP servers and NTP clients after deployment.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-enhancements-kuryr-kubernetes"><a class="anchor" href="#ocp-4-8-enhancements-kuryr-kubernetes"></a>Changes to default API load balancer management for Kuryr</h4>
<div class="paragraph">
<p>In OpenShift Container Platform 4.8 deployments on Red Hat OpenStack Platform (RHOSP) with Kuryr-Kubernetes, the API load balancer for the <code>default/kubernetes</code> service is no longer managed by the Cluster Network Operator (CNO), but instead by the kuryr-controller itself. This means that:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>When upgrading to OpenShift Container Platform 4.8, the <code>default/kubernetes</code> service will have downtime.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>In deployments where no Open Virtual Network (OVN) Octavia is available, more downtime should be expected</p>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>The <code>default/kubernetes</code> load balancer is no longer required to use the Octavia Amphora driver. Instead, OVN Octavia will be used to implement the <code>default/kubernetes</code> service if it is available in the OpenStack cloud.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-networking-enabling-provisioning-network-day2"><a class="anchor" href="#ocp-4-8-networking-enabling-provisioning-network-day2"></a>Enabling a provisioning network after installation</h4>
<div class="paragraph">
<p>The assisted installer and installer-provisioned installation for bare metal clusters provide the ability to deploy a cluster without a <code>provisioning</code> network. In OpenShift Container Platform 4.8 and later, you can enable a <code>provisioning</code> network after installation by using the Cluster Baremetal Operator (CBO).</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-configure-vips-to-run-on-control-plane"><a class="anchor" href="#ocp-4-8-configure-vips-to-run-on-control-plane"></a>Configure network components to run on the control plane</h4>
<div class="paragraph">
<p>If you need the virtual IP (VIP) addresses to run on the control plane nodes in a bare metal installation, you must configure the <code>apiVIP</code> and <code>ingressVIP</code> VIP addresses to run exclusively on the control plane nodes. By default, OpenShift Container Platform allows any node in the worker machine configuration pool to host the <code>apiVIP</code> and <code>ingressVIP</code> VIP addresses. Because many bare metal environments deploy worker nodes in separate subnets from the control plane nodes, configuring the <code>apiVIP</code> and <code>ingressVIP</code> virtual IP addresses to run exclusively on the control plane nodes prevents issues from arising due to deploying worker nodes in separate subnets. For additional details, see <a href="#../installing/installing_bare_metal_ipi/ipi-install-installation-workflow.html#configure-network-components-to-run-on-the-control-plane_ipi-install-configuration-files" class="page unresolved">Configure network components to run on the control plane</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-networking-external-load-balancer"><a class="anchor" href="#ocp-4-8-networking-external-load-balancer"></a>Configuring an external load balancer for apiVIP and ingressVIP traffic</h4>
<div class="paragraph">
<p>In OpenShift Container Platform 4.8, you can configure an external load balancer to handle <code>apiVIP</code> and <code>ingressVIP</code> traffic to the control plane of installer-provisioned clusters. External load balancing services and the control plane nodes must run on the same L2 network, and on the same VLAN when using VLANs to route traffic between the load balancing services and the control plane nodes.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-IPsec-support-dual-stack"><a class="anchor" href="#ocp-4-8-IPsec-support-dual-stack"></a>OVN-Kubernetes IPsec support for dual-stack networking</h4>
<div class="paragraph">
<p>OpenShift Container Platform 4.8 adds OVN-Kubernetes IPsec support for clusters that are configured to use dual-stack networking.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-egress-router-ovn-kubernetes"><a class="anchor" href="#ocp-4-8-egress-router-ovn-kubernetes"></a>Egress router CNI for OVN-Kubernetes</h4>
<div class="paragraph">
<p>The egress router CNI plug-in is generally available. The Cluster Network Operator is enhanced to support an <code>EgressRouter</code> API object. The process for adding an egress router on a cluster that uses OVN-Kubernetes is simplified. When you create an egress router object, the Operator automatically adds a network attachment definition and a deployment. The pod for the deployment acts as the egress router.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../networking/ovn_kubernetes_network_provider/using-an-egress-router-ovn.adoc" class="page unresolved">Considerations for the use of an egress router pod</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-ip-failover-support"><a class="anchor" href="#ocp-4-8-ip-failover-support"></a>IP failover support on OpenShift Container Platform</h4>
<div class="paragraph">
<p>IP failover is now supported on OpenShift Container Platform clusters on bare metal. IP failover uses <code>keepalived</code> to host a set of externally accessible VIP addresses on a set of hosts. Each VIP is only serviced by a single host at a time. <code>keepalived</code> uses the Virtual Router Redundancy Protocol (VRRP) to determine which host, from the set of hosts, services which VIP. If a host becomes unavailable, or if the service that <code>keepalived</code> is watching does not respond, the VIP is switched to another host from the set. This means a VIP is always serviced as long as a host is available.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../networking/configuring-ipfailover.adoc#configuring-ipfailover" class="page unresolved">Configuring IP failover</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-ingress-controller-power-of-two-random-choices"><a class="anchor" href="#ocp-4-8-ingress-controller-power-of-two-random-choices"></a>Power of Two Random Choices balancing algorithm for Ingress Controller</h4>
<div class="paragraph">
<p>OpenShift Container Platform Ingress Controllers now use the <a href="https://www.haproxy.com/blog/power-of-two-load-balancing/">Power of Two Random Choices</a> balancing algorithm by default for more even balancing after reloading. As in earlier releases, the balancing algorithm for a specific route can still be configured using the <code>haproxy.router.openshift.io/balance</code> route annotation. For more information, see <a href="#../networking/routes/route-configuration.adoc#nw-route-specific-annotations_route-configuration" class="page unresolved">Route-specific annotations</a>.</p>
</div>
<div class="paragraph">
<p>To revert the Ingress Controller back to use the Least Connections balancing algorithm, use the following <code>oc patch</code> command to specify <code>leastconn</code> as the default:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight nowrap"><code class="language-terminal hljs" data-lang="terminal">$ oc -n openshift-ingress-operator patch ingresscontroller/default --type=merge --patch='{"spec":{"unsupportedConfigOverrides":{"loadBalancingAlgorithm":"leastconn"}}}'</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-control-dns-pod-placement"><a class="anchor" href="#ocp-4-8-control-dns-pod-placement"></a>Control DNS pod placement</h4>
<div class="paragraph">
<p>In OpenShift Container Platform 4.8, you can use a custom node selector and tolerations to configure the daemon set for CoreDNS to run or not run on certain nodes.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../networking/dns-operator.adoc#nw-controlling-dns-pod-placement_dns-operator" class="page unresolved">Controlling DNS pod placement</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-networking-openstack-provider-networks"><a class="anchor" href="#ocp-4-8-networking-openstack-provider-networks"></a>Provider networks support for clusters that run on RHOSP</h4>
<div class="paragraph">
<p>OpenShift Container Platform clusters on Red Hat OpenStack Platform (RHOSP) now support provider networks for all deployment types.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-configurable-headerBufferMaxRewriteByte-headerBufferBytes-parameters"><a class="anchor" href="#ocp-4-8-configurable-headerBufferMaxRewriteByte-headerBufferBytes-parameters"></a>Configurable tune.maxrewrite and tune.bufsize for HAProxy</h4>
<div class="paragraph">
<p>Cluster Administrators can now set <code>headerBufferMaxRewriteByte</code> and <code>headerBufferBytes</code> Ingress Controller tuning parameters to configure <code>tune.maxrewrite</code> and <code>tune.bufsize</code> HAProxy memory options per-Ingress Controller.</p>
</div>
<div class="paragraph">
<p>See <a href="#../networking/ingress-operator.html#nw-ingress-controller-configuration-parameters_configuring-ingress" class="page unresolved">Ingress Controller configuration parameters</a> for more information.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="ocp-4-8-storage"><a class="anchor" href="#ocp-4-8-storage"></a>Storage</h3>
<div class="sect3">
<h4 id="ocp-4-8-storage-gcp-pd-csi-ga"><a class="anchor" href="#ocp-4-8-storage-gcp-pd-csi-ga"></a>Persistent storage using GCP PD CSI driver operator is generally available</h4>
<div class="paragraph">
<p>The Google Cloud Platform (GCP) persistent disk (PD) Container Storage Interface (CSI) driver is automatically deployed and managed on GCP environments, allowing you to dynamically provision these volumes without having to install the driver manually.
This feature was previously introduced as a Technology Preview feature in OpenShift Container Platform 4.7 and is now generally available and enabled by default in OpenShift Container Platform 4.8.</p>
</div>
<div class="paragraph">
<p>For more information, <a href="#../storage/container_storage_interface/persistent-storage-csi-gcp-pd.adoc#persistent-storage-csi-gcp-pd" class="page unresolved">GCP PD CSI Driver Operator</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-storage-azure-csi-tp"><a class="anchor" href="#ocp-4-8-storage-azure-csi-tp"></a>Persistent storage using the Azure Disk CSI Driver Operator (Technology Preview)</h4>
<div class="paragraph">
<p>The Azure Disk CSI Driver Operator provides a storage class by default that you can use to create persistent volume claims (PVCs).
The Azure Disk CSI Driver Operator that manages this driver is in Technology Preview.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../storage/container_storage_interface/persistent-storage-csi-azure.adoc#persistent-storage-csi-azure" class="page unresolved">Azure Disk CSI Driver Operator</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-storage-vsphere-csi-tp"><a class="anchor" href="#ocp-4-8-storage-vsphere-csi-tp"></a>Persistent storage using the vSphere CSI Driver Operator (Technology Preview)</h4>
<div class="paragraph">
<p>The vSphere CSI Driver Operator provides a storage class by default that you can use to create persistent volume claims (PVCs).
The vSphere CSI Driver Operator that manages this driver is in Technology Preview.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../storage/container_storage_interface/persistent-storage-csi-vsphere.adoc#persistent-storage-csi-vsphere" class="page unresolved">vSphere CSI Driver Operator</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-storage-csi-migration"><a class="anchor" href="#ocp-4-8-storage-csi-migration"></a>Automatic CSI migration (Technology Preview)</h4>
<div class="paragraph">
<p>Starting with OpenShift Container Platform 4.8, automatic migration for the following in-tree volume plug-ins to their equivalent CSI drivers is available as a Technology Preview feature:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Amazon Web Services (AWS) Elastic Block Storage (EBS)</p>
</li>
<li>
<p>OpenStack Cinder</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../storage/container_storage_interface/persistent-storage-csi-migration.adoc#persistent-storage-csi-migration" class="page unresolved">Automatic CSI Migration</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-storage-aws-efs-removed-1st"><a class="anchor" href="#ocp-4-8-storage-aws-efs-removed-1st"></a>External provisioner for AWS EFS (Technology Preview) feature has been removed</h4>
<div class="paragraph">
<p>The Amazon Web Services (AWS) Elastic File System (EFS) Technology Preview feature has been removed and is no longer supported.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-storage-openstack-cinder-availability-zones"><a class="anchor" href="#ocp-4-8-storage-openstack-cinder-availability-zones"></a>Improved control over Cinder volume availability zones for clusters that run on RHOSP</h4>
<div class="paragraph">
<p>You can now select availability zones for Cinder volumes during installation. You can also use Cinder volumes in particular availability zones for your <a href="#../registry/configuring_registry_storage/configuring-registry-storage-osp.adoc#installation-registry-osp-creating-custom-pvc_configuring-registry-storage-openstack" class="page unresolved">image registry</a>.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="ocp-4-8-registry"><a class="anchor" href="#ocp-4-8-registry"></a>Registry</h3>

</div>
<div class="sect2">
<h3 id="ocp-4-8-olm"><a class="anchor" href="#ocp-4-8-olm"></a>Operator lifecycle</h3>
<div class="sect3">
<h4 id="ocp-4-8-admin-error-reporting"><a class="anchor" href="#ocp-4-8-admin-error-reporting"></a>Enhanced error reporting for administrators</h4>
<div class="paragraph">
<p>A cluster administrator using Operator Lifecycle Manager (OLM) to install an Operator can encounter error conditions that are related either to the current API or low-level APIs. Previously, there was little insight into why OLM could not fulfill a request to install or update an Operator. These errors could range from trivial issues like typos in object properties or missing RBAC, to more complex issues where items could not be loaded from the catalog due to metadata parsing.</p>
</div>
<div class="paragraph">
<p>Because administrators should not require understanding of the interaction process between the various low-level APIs or access to the OLM pod logs to successfully debug such issues, OpenShift Container Platform 4.8 introduces the following enhancements in OLM to provide administrators with more comprehensible error reporting and messages:</p>
</div>
<div id="ocp-4-8-installplan-errors" class="dlist">
<dl>
<dt class="hdlist1">Retrying install plans</dt>
<dd>
<p>Install plans, defined by an <code>InstallPlan</code> object, can encounter transient errors, for example, due to API server availability or conflicts with other writers. Previously, these errors would result in the termination of partially-applied install plans that required manual cleanup. With this enhancement, the Catalog Operator now retries errors during install plan execution for up to one minute. The new <code>.status.message</code> field provides a human-readable indication when retries are occurring.</p>
</dd>
</dl>
</div>
<div id="ocp-4-8-operatorgroup-errors" class="dlist">
<dl>
<dt class="hdlist1">Indicating invalid Operator groups</dt>
<dd>
<p>Creating a subscription in a namespace with no Operator groups or multiple Operator groups would previously result in a stalled Operator installation with an install plan that stays in <code>phase=Installing</code> forever. With this enhancement, the install plan immediately transitions to <code>phase=Failed</code> so that the administrator can correct the invalid Operator group, and then delete and re-create the subscription again.</p>
</dd>
</dl>
</div>
<div id="ocp-4-8-candidate-operator-errors" class="dlist">
<dl>
<dt class="hdlist1">Specific reporting when no candidate Operators found</dt>
<dd>
<p><code>ResolutionFailed</code> events, which are created when dependency resolution in a namespace fails, now provide more specific text when the namespace contains a subscription that references a package or channel that does not exist in the referenced catalog source. Previously, this message was generic:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight nowrap"><code class="language-terminal hljs" data-lang="terminal">no candidate operators found matching the spec of subscription '&lt;name&gt;'</code></pre>
</div>
</div>
<div class="paragraph">
<p>With this enhancement, the messages are more specific:</p>
</div>
<div class="listingblock">
<div class="title">Operator does not exist</div>
<div class="content">
<pre class="highlightjs highlight nowrap"><code class="language-terminal hljs" data-lang="terminal">no operators found in package &lt;name&gt; in the catalog referenced by subscription &lt;name&gt;</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Catalog does not exist</div>
<div class="content">
<pre class="highlightjs highlight nowrap"><code class="language-terminal hljs" data-lang="terminal">no operators found from catalog &lt;name&gt; in namespace openshift-marketplace referenced by subscription &lt;name&gt;</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Channel does not exist</div>
<div class="content">
<pre class="highlightjs highlight nowrap"><code class="language-terminal hljs" data-lang="terminal">no operators found in channel &lt;name&gt; of package &lt;name&gt; in the catalog referenced by subscription &lt;name&gt;</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Cluster service version (CSV) does not exist</div>
<div class="content">
<pre class="highlightjs highlight nowrap"><code class="language-terminal hljs" data-lang="terminal">no operators found with name &lt;name&gt;.&lt;version&gt; in channel &lt;name&gt; of package &lt;name&gt; in the catalog referenced by subscription &lt;name&gt;</code></pre>
</div>
</div>
</dd>
</dl>
</div>
</div>
</div>
<div class="sect2">
<h3 id="ocp-4-8-osdk"><a class="anchor" href="#ocp-4-8-osdk"></a>Operator development</h3>
<div class="sect3">
<h4 id="ocp-4-8-pkgman-to-bundle"><a class="anchor" href="#ocp-4-8-pkgman-to-bundle"></a>Migration of Operator projects from package manifest format to bundle format</h4>
<div class="paragraph">
<p>Support for the legacy package manifest format for Operators is removed in OpenShift Container Platform 4.8 and later. The bundle format is the preferred Operator packaging format for Operator Lifecycle Manager (OLM) starting in OpenShift Container Platform 4.6. If you have an Operator project that was initially created in the package manifest format, which has been deprecated, you can now use the Operator SDK <code>pkgman-to-bundle</code> command to migrate the project to the bundle format.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../operators/operator_sdk/osdk-pkgman-to-bundle.adoc#osdk-pkgman-to-bundle" class="page unresolved">Migrating package manifest projects to bundle format</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-catalog-build-push"><a class="anchor" href="#ocp-4-8-catalog-build-push"></a>Publishing a catalog containing a bundled Operator</h4>
<div class="paragraph">
<p>To install and manage Operators, Operator Lifecycle Manager (OLM) requires that Operator bundles are listed in an index image, which is referenced by a catalog on the cluster. As an Operator author, you can use the Operator SDK to create an index containing the bundle for your Operator and all of its dependencies. This is useful for testing on remote clusters and publishing to container registries.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../operators/operator_sdk/osdk-working-bundle-images.adoc#osdk-publish-catalog_osdk-working-bundle-images" class="page unresolved">Publishing a catalog containing a bundled Operator</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-bundle-upgrade"><a class="anchor" href="#ocp-4-8-bundle-upgrade"></a>Enhanced Operator upgrade testing</h4>
<div class="paragraph">
<p>The Operator SDK&#8217;s <code>run bundle-upgrade</code> subcommand automates triggering an installed Operator to upgrade to a later version by specifying a bundle image for the later version. Previously, the subcommand could only upgrade Operators that were initially installed using the <code>run bundle</code> subcommand. With this enhancement, the <code>run bundle-upgrade</code> now also works with Operators that were initially installed with the traditional Operator Lifecycle Manager (OLM) workflow.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../operators/operator_sdk/osdk-working-bundle-images.adoc#osdk-bundle-upgrade-olm_osdk-working-bundle-images" class="page unresolved">Testing an Operator upgrade on Operator Lifecycle Manager</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-osdk-compat"><a class="anchor" href="#ocp-4-8-osdk-compat"></a>Controlling Operator compatibility with OpenShift Container Platform versions</h4>
<div class="paragraph">
<p>When an API is removed from an OpenShift Container Platform version, Operators running on that cluster version that are still using removed APIs will no longer work properly. As an Operator author, you should plan to update your Operator projects to accommodate API deprecation and removal to avoid interruptions for users of your Operator.</p>
</div>
<div class="paragraph">
<p>For more details, see <a href="#../operators/operator_sdk/osdk-working-bundle-images.adoc#osdk-control-compat_osdk-working-bundle-images" class="page unresolved">Controlling Operator compatibility with OpenShift Container Platform versions</a>.</p>
</div>
<h3 id="ocp-4-8-builds" class="discrete">Builds</h3>
</div>
<div class="sect3">
<h4 id="ocp-4-8-builds-telemetry-metric"><a class="anchor" href="#ocp-4-8-builds-telemetry-metric"></a>New Telemetry metric for number of builds by strategy</h4>
<div class="paragraph">
<p>Telemetry includes a new <code>openshift:build_by_strategy:sum</code> gauge metric, which sends the number of builds by strategy type to the Telemeter Client. This metric gives site reliability engineers (SREs) and product managers visibility into the kinds of builds that run on OpenShift Container Platform clusters.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-builds-mount-custom-pki-ca"><a class="anchor" href="#ocp-4-8-builds-mount-custom-pki-ca"></a>Mount custom PKI certificate authorities</h4>
<div class="paragraph">
<p>Previously, builds could not use the cluster PKI certificate authorities that were sometimes required to access corporate artifact repositories. Now, you can configure the <code>BuildConfig</code> object to mount cluster custom PKI certificate authorities by setting <code>mountTrustedCA</code> to <code>true</code>.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="ocp-4-8-images"><a class="anchor" href="#ocp-4-8-images"></a>Images</h3>

</div>
<div class="sect2">
<h3 id="ocp-4-8-machine-api"><a class="anchor" href="#ocp-4-8-machine-api"></a>Machine API</h3>
<div class="sect3">
<h4 id="ocp-4-8-vsphere-machine-autoscaler-to-from-zero"><a class="anchor" href="#ocp-4-8-vsphere-machine-autoscaler-to-from-zero"></a>Scaling machines running in vSphere to and from zero with the cluster autoscaler</h4>
<div class="paragraph">
<p>When running machines in vSphere, you can now set the <code>minReplicas</code> value to <code>0</code> in the <code>MachineAutoscaler</code> resource definition. When this value is set to <code>0</code>, the cluster autoscaler scales the machine set to and from zero depending on if the machines are in use. For more information, see the <a href="#../machine_management/applying-autoscaling.html#machine-autoscaler-cr_applying-autoscaling" class="page unresolved">MachineAutoscaler resource definition</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-mco-no-reboot-cert"><a class="anchor" href="#ocp-4-8-mco-no-reboot-cert"></a>Automatic rotation of kubelet-ca.crt does not require node draining or reboot</h4>
<div class="paragraph">
<p>The automatic rotation of the <code>/etc/kubernetes/kubelet-ca.crt</code> certificate authority (CA) no longer requires the Machine Config Operator (MCO) to drain nodes or reboot the cluster.</p>
</div>
<div class="paragraph">
<p>As part of this change, the following modifications do not require the MCO to drain nodes:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Changes to the SSH key in the <code>spec.config.ignition.passwd.users.sshAuthorizedKeys</code> parameter of a machine config</p>
</li>
<li>
<p>Changes to the global pull secret or pull secret in the <code>openshift-config</code> namespace</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>When the MCO detects any of these changes, it applies the changes and uncordons the node.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../architecture/control-plane.adoc#understanding-machine-config-operator_control-plane" class="page unresolved">Understanding the Machine Config Operator</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-machine-set-policy-enhancement"><a class="anchor" href="#ocp-4-8-machine-set-policy-enhancement"></a>Machine set policy enhancement</h4>
<div class="paragraph">
<p>Previously, creating machine sets required users to manually configure their CPU pinning settings, NUMA pinning settings, and CPU topology changes to get better performance from the host. With this enhancement, users can select a policy in the <code>MachineSet</code> resource to populate settings automatically. For more information, see <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1941334"><strong>BZ#1941334</strong></a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-machine-set-hugepage-enhancement"><a class="anchor" href="#ocp-4-8-machine-set-hugepage-enhancement"></a>Machine set hugepage enhancement</h4>
<div class="paragraph">
<p>Providing a <code>hugepages</code> property into the <code>MachineSet</code> resource is now possible. This enhancement creates the <code>MachineSet</code> resource&#8217;s nodes with a custom property in oVirt and instructs those nodes to use the <code>hugepages</code> of the hypervisor. For more information, see  <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1948963"><strong>BZ#1948963</strong></a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-machine-config-operator-image-content-source-object-enhancement"><a class="anchor" href="#ocp-4-8-machine-config-operator-image-content-source-object-enhancement"></a>Machine Config Operator ImageContentSourcePolicy object enhancement</h4>
<div class="paragraph">
<p>OpenShift Container Platform 4.8 avoids workload disruption for selected <code>ImageContentSourcePolicy</code> object changes. This feature helps users and teams add additional mirrors and registries without workload disruption. As a result, workload distruption will no longer occur for the following changes in <code>/etc/containers/registries.conf</code> files:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Addition of a registry with <code>mirror-by-digest-only=true</code></p>
</li>
<li>
<p>Addition of a mirror in a registry with <code>mirror-by-digest-only=true</code></p>
</li>
<li>
<p>Appending items in <code>unqualified-search-registries</code> list</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For any other changes in <code>/etc/containers/registries.conf</code> files, the Machine Config Operator will default to draining nodes to apply changes. For more information, see <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1943315"><strong>BZ#1943315</strong></a>.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="ocp-4-8-nodes"><a class="anchor" href="#ocp-4-8-nodes"></a>Nodes</h3>
<div class="sect3">
<h4 id="ocp-4-8-nodes-descheduler-apigroup"><a class="anchor" href="#ocp-4-8-nodes-descheduler-apigroup"></a>Descheduler operator.openshift.io/v1 API group is now available</h4>
<div class="paragraph">
<p>The <code>operator.openshift.io/v1</code> API group is now available for the descheduler. Support for the <code>operator.openshift.io/v1beta1</code> API group for the descheduler might be removed in a future release.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-nodes-descheduler-metrics"><a class="anchor" href="#ocp-4-8-nodes-descheduler-metrics"></a>Prometheus metrics for the descheduler</h4>
<div class="paragraph">
<p>You can now enable Prometheus metrics for the descheduler by adding the <code>openshift.io/cluster-monitoring=true</code> label to the <code>openshift-kube-descheduler-operator</code> namespace where you installed the descheduler.</p>
</div>
<div class="paragraph">
<p>The following descheduler metrics are available:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>descheduler_build_info</code> - Provides build information about the descheduler.</p>
</li>
<li>
<p><code>descheduler_pods_evicted</code> - Provides the number of pods that have been evicted for each combination of strategy, namespace, and result. There must be at least one evicted pod for this metric to appear.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-nodes-huge-pages-downward-api"><a class="anchor" href="#ocp-4-8-nodes-huge-pages-downward-api"></a>Support for huge pages with the Downward API</h4>
<div class="paragraph">
<p>With this release, when you set requests and limits for huge pages in a pod specification, you can use the Downward API to view the allocation for the pod from within a container. This enhancements relies on the <code>DownwardAPIHugePages</code> feature gate. OpenShift Container Platform 4.8 enables the feature gate.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../scalability_and_performance/what-huge-pages-do-and-how-they-are-consumed-by-apps.adoc#consuming-huge-pages-resource-using-the-downward-api_huge-pages" class="page unresolved">Consuming huge pages resources using the Downward API</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-nodes-nfd-operator-new-labels_release-notes"><a class="anchor" href="#ocp-4-8-nodes-nfd-operator-new-labels_release-notes"></a>New labels for the Node Feature Discovery Operator</h4>
<div class="paragraph">
<p>The Node Feature Discovery (NFD) Operator detects hardware features available on each node in an OpenShift Container Platform cluster. Then, it modifies node objects with node labels. This enables the NFD Operator to advertise the features of specific nodes. OpenShift Container Platform 4.8 supports three additional labels for the NFD Operator.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>pstate intel-pstate</code>: When the Intel <code>pstate</code> driver is enabled and in use, the <code>pstate intel-pstate</code> label reflects the status of the Intel <code>pstate</code> driver. The status is either <code>active</code> or <code>passive</code>.</p>
</li>
<li>
<p><code>pstate scaling_governor</code>: When the Intel <code>pstate</code> driver status is <code>active</code>, the <code>pstate scaling_governor</code> label reflects the scaling governor algorithm. The algorithm is either <code>powersave</code> or <code>performance</code>.</p>
</li>
<li>
<p><code>cstate status</code>: If the <code>intel_idle</code> driver has C-states or idle states, the <code>cstate status</code> label is <code>true</code>. Otherwise, it is <code>false</code>.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-nodes-no-reboot-cert"><a class="anchor" href="#ocp-4-8-nodes-no-reboot-cert"></a>Automatic rotation of kubelet-ca.crt does not require reboot</h4>
<div class="paragraph">
<p>The automatic rotation of the <code>/etc/kubernetes/kubelet-ca.crt</code> certificate authority (CA) no longer requires the Machine Config Operator (MCO) to drain nodes or reboot the cluster.</p>
</div>
<div class="paragraph">
<p>As part of this change, the following modifications do not require the MCO to drain nodes:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Changes to the SSH key in the <code>spec.config.ignition.passwd.users.sshAuthorizedKeys</code> parameter of a machine config</p>
</li>
<li>
<p>Changes to the global pull secret or pull secret in the <code>openshift-config</code> namespace</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>When the MCO detects any of these changes, it applies the changes and uncordons the node.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../architecture/control-plane.adoc#understanding-machine-config-operator_control-plane" class="page unresolved">Understanding the Machine Config Operator</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-nodes-vpa-ga"><a class="anchor" href="#ocp-4-8-nodes-vpa-ga"></a>Vertical pod autoscaling is generally available</h4>
<div class="paragraph">
<p>The OpenShift Container Platform vertical pod autoscaler (VPA) is now generally available. The VPA automatically reviews the historic and current CPU and memory resources for containers in pods and can update the resource limits and requests based on the usage values it learns.</p>
</div>
<div class="paragraph">
<p>You can also use the VPA with pods that require only one replica by modifying the <code>VerticalPodAutoscalerController</code> object as described below. Previously, the VPA worked only with pods that required two or more replicas.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../nodes/pods/nodes-pods-vertical-autoscaler.adoc#nodes-pods-vertical-autoscaler" class="page unresolved">Automatically adjust pod resource levels with the vertical pod autoscaler</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-nodes-vpa-replicas"><a class="anchor" href="#ocp-4-8-nodes-vpa-replicas"></a>Vertical pod autoscaling minimum can be configured</h4>
<div class="paragraph">
<p>By default, workload objects must specify a minimum of two replicas in order for the VPA to automatically update pods. As a result, workload objects that specify fewer than two replicas are not acted upon by the VPA. You can change this cluster-wide minimum value by modifying the <code>VerticalPodAutoscalerController</code> object to add the <code>minReplicas</code> parameter.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../nodes/pods/nodes-pods-vertical-autoscaler.adoc#nodes-pods-vertical-autoscaler" class="page unresolved">Automatically adjust pod resource levels with the vertical pod autoscaler</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-nodes-resources-configuring-auto_release-notes"><a class="anchor" href="#ocp-4-8-nodes-resources-configuring-auto_release-notes"></a>Automatically allocate CPU and memory resources for nodes</h4>
<div class="paragraph">
<p>OpenShift Container Platform can automatically determine the optimal sizing value of the <code>system-reserved</code> setting when a node starts. Previously, the CPU and memory allocations in the <code>system-reserved</code> setting were fixed limits that you needed to manually determine and set.</p>
</div>
<div class="paragraph">
<p>When automatic resource allocation is enabled, a script on each node calculates the optimal values for the respective reserved resources based on the installed CPU and memory capacity on the node.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../nodes/nodes/nodes-nodes-resources-configuring.adoc#nodes-nodes-resources-configuring-auto_nodes-nodes-resources-configuring" class="page unresolved">Automatically allocating resources for nodes</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-nodes-registry-allowed-repo_release-notes"><a class="anchor" href="#ocp-4-8-nodes-registry-allowed-repo_release-notes"></a>Adding specific repositories to pull images</h4>
<div class="paragraph">
<p>You can now specify an individual repository within a registry when creating lists of allowed and blocked registries for pulling and pushing images. Previously, you could specify only a registry.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../openshift_images/image-configuration.adoc#images-configuration-allowed_image-configuration" class="page unresolved">Adding specific registries</a> and <a href="#../openshift_images/image-configuration.adoc#images-configuration-blocked_image-configuration" class="page unresolved">Blocking specific registries</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-nodes-cronjob-qa_release-notes"><a class="anchor" href="#ocp-4-8-nodes-cronjob-qa_release-notes"></a>Cron jobs are generally available</h4>
<div class="paragraph">
<p>The cron job custom resource is now generally available. As part of this change, a new controller has been implemented that substantially improves the performance of cron jobs. For more information on cron jobs, see <a href="#../nodes/jobs/nodes-nodes-jobs.adoc#nodes-nodes-jobs-about_nodes-nodes-jobs" class="page unresolved">Understanding jobs and cron jobs</a>.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="ocp-4-8-logging"><a class="anchor" href="#ocp-4-8-logging"></a>Red Hat OpenShift Logging</h3>
<div class="paragraph">
<p>In OpenShift Container Platform 4.7, <em>Cluster Logging</em> became <em>Red Hat OpenShift Logging</em>. For more information, see <a href="#../logging/cluster-logging-release-notes.adoc" class="page unresolved">Release notes for Red Hat OpenShift Logging</a>.</p>
</div>
</div>
<div class="sect2">
<h3 id="ocp-4-8-monitoring"><a class="anchor" href="#ocp-4-8-monitoring"></a>Monitoring</h3>
<div class="sect3">
<h4 id="ocp-4-8-monitoring-alerting-rule-changes"><a class="anchor" href="#ocp-4-8-monitoring-alerting-rule-changes"></a>Alerting rule changes</h4>
<div class="paragraph">
<p>OpenShift Container Platform 4.8 includes the following alerting rule changes:</p>
</div>
<div class="exampleblock">
<div class="title">Example 1. Alerting rule changes</div>
<div class="content">
<div class="ulist">
<ul>
<li>
<p>The <code>ThanosSidecarPrometheusDown</code> alert severity is updated from <em>critical</em> to <em>warning</em>.</p>
</li>
<li>
<p>The <code>ThanosSidecarUnhealthy</code> alert severity is updated from <em>critical</em> to <em>warning</em>.</p>
</li>
<li>
<p>The <code>ThanosQueryHttpRequestQueryErrorRateHigh</code> alert severity is updated from <em>critical</em> to <em>warning</em>.</p>
</li>
<li>
<p>The <code>ThanosQueryHttpRequestQueryRangeErrorRateHigh</code> alert severity is updated from <em>critical</em> to <em>warning</em>.</p>
</li>
<li>
<p>The <code>ThanosQueryInstantLatencyHigh</code> critical alert is removed. This alert fired if Thanos Querier had a high latency for instant queries.</p>
</li>
<li>
<p>The <code>ThanosQueryRangeLatencyHigh</code> critical alert is removed. This alert fired if Thanos Querier had a high latency for range queries.</p>
</li>
<li>
<p>For all Thanos Querier alerts, the <code>for</code> duration is increased to 1 hour.</p>
</li>
<li>
<p>For all Thanos sidecar alerts, the <code>for</code> duration is increased to 1 hour.</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Red Hat does not guarantee backward compatibility for metrics, recording rules, or alerting rules.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-monitoring-removed-API-alerts"><a class="anchor" href="#ocp-4-8-monitoring-removed-API-alerts"></a>Alerts and information on APIs in use that will be removed in the next release</h4>
<div class="paragraph">
<p>OpenShift Container Platform 4.8 introduces two new alerts that fire when an API that will be removed in the next release is in use:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>APIRemovedInNextReleaseInUse</code> - for APIs that will be removed in the next OpenShift Container Platform release.</p>
</li>
<li>
<p><code>APIRemovedInNextEUSReleaseInUse</code> - for APIs that will be removed in the next OpenShift Container Platform <a href="https://access.redhat.com/support/policy/updates/openshift#ocp4_phases">Extended Update Support</a> (EUS) release.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>You can use the new <code>APIRequestCount</code> API to track what is using the deprecated APIs. This allows you to plan whether any actions are required in order to upgrade to the next release.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-monitoring-stack-dependency-version-updates"><a class="anchor" href="#ocp-4-8-monitoring-stack-dependency-version-updates"></a>Version updates to monitoring stack components and dependencies</h4>
<div class="paragraph">
<p>OpenShift Container Platform 4.8 includes version updates to the following monitoring stack components and dependencies:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The Prometheus Operator is now on version 0.48.1.</p>
</li>
<li>
<p>Prometheus is now on version 2.26.1.</p>
</li>
<li>
<p>The <code>node-exporter</code> agent is now on version 1.1.2.</p>
</li>
<li>
<p>Thanos is now on version 0.20.2.</p>
</li>
<li>
<p>Grafana is now on version 7.5.5.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-monitoring-kube-state-metrics-v2.0.0-upgrade"><a class="anchor" href="#ocp-4-8-monitoring-kube-state-metrics-v2.0.0-upgrade"></a>kube-state-metrics upgraded to version 2.0.0</h4>
<div class="paragraph">
<p><code>kube-state-metrics</code> is upgraded to version 2.0.0. The following metrics were deprecated in <code>kube-state-metrics</code> version 1.9 and are effectively removed in version 2.0.0:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Non-generic resource metrics for pods:</p>
<div class="ulist">
<ul>
<li>
<p>kube_pod_container_resource_requests_cpu_cores</p>
</li>
<li>
<p>kube_pod_container_resource_limits_cpu_cores</p>
</li>
<li>
<p>kube_pod_container_resource_requests_memory_bytes</p>
</li>
<li>
<p>kube_pod_container_resource_limits_memory_bytes</p>
</li>
</ul>
</div>
</li>
<li>
<p>Non-generic resource metrics for nodes:</p>
<div class="ulist">
<ul>
<li>
<p>kube_node_status_capacity_pods</p>
</li>
<li>
<p>kube_node_status_capacity_cpu_cores</p>
</li>
<li>
<p>kube_node_status_capacity_memory_bytes</p>
</li>
<li>
<p>kube_node_status_allocatable_pods</p>
</li>
<li>
<p>kube_node_status_allocatable_cpu_cores</p>
</li>
<li>
<p>kube_node_status_allocatable_memory_bytes</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-monitoring-removed-grafana-alertmanager-ui-links"><a class="anchor" href="#ocp-4-8-monitoring-removed-grafana-alertmanager-ui-links"></a>Removed Grafana and Alertmanager UI links</h4>
<div class="paragraph">
<p>The link to the third-party Alertmanager UI is removed from the <strong>Monitoring</strong> &#8594; <strong>Alerting</strong> page in the OpenShift Container Platform web console. Additionally, the link to the third-party Grafana UI is removed from the <strong>Monitoring</strong> &#8594; <strong>Dashboards</strong> page. You can still access the routes to the Grafana and Alertmanager UIs in the web console in the <strong>Administrator</strong> perspective by navigating to the <strong>Networking</strong> &#8594; <strong>Routes</strong> page in the <code>openshift-monitoring</code> project.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-monitoring-dashboards-updates-web-console"><a class="anchor" href="#ocp-4-8-monitoring-dashboards-updates-web-console"></a>Monitoring dashboard enhancements in the web console</h4>
<div class="paragraph">
<p>New enhancements are available on the <strong>Monitoring</strong> &#8594; <strong>Dashboards</strong> page in the OpenShift Container Platform web console:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>When you zoom in on a single graph by selecting an area with the mouse, all other graphs now update to reflect the same time range.</p>
</li>
<li>
<p>Dashboard panels are now organized into groups, which you can expand and collapse.</p>
</li>
<li>
<p>Single-value panels now support changing color depending on their value.</p>
</li>
<li>
<p>Dashboard labels now display in the <strong>Dashboard</strong> drop-down list.</p>
</li>
<li>
<p>You can now specify a custom time range for a dashboard by selecting <strong>Custom time range</strong> in the <strong>Time Range</strong> drop-down list.</p>
</li>
<li>
<p>When applicable, you can now select the <strong>All</strong> option in a dashboard filter drop-down menu to display data for all of the options in that filter.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="ocp-4-8-metering"><a class="anchor" href="#ocp-4-8-metering"></a>Metering</h3>
<div class="paragraph">
<p>The Metering Operator is deprecated as of OpenShift Container Platform 4.6, and is scheduled to be removed in OpenShift Container Platform 4.9.</p>
</div>
</div>
<div class="sect2">
<h3 id="ocp-4-8-scale"><a class="anchor" href="#ocp-4-8-scale"></a>Scale</h3>
<div class="sect3">
<h4 id="ocp-4-8-scale-running-in-a-single-node-cluster"><a class="anchor" href="#ocp-4-8-scale-running-in-a-single-node-cluster"></a>Running on a single node cluster</h4>
<div class="paragraph">
<p>Running tests on a single node cluster causes longer timeouts for certain tests, including SR-IOV and SCTP tests, and tests requiring control plane and worker nodes are skipped. Reconfiguration requiring node reboots causes a reboot of the entire environment, including the OpenShift control plane, and therefore takes longer to complete. All PTP tests requiring a control plane node and a worker node are skipped. No additional configuration is needed because the tests check for the number of nodes at startup and adjust test behavior accordingly.</p>
</div>
<div class="paragraph">
<p>PTP tests can run in Discovery mode. The tests look for a PTP control plane configured outside of the cluster. The following parameters are required:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>ROLE_WORKER_CNF=master</code> - Required because the control plane (<code>master</code>) is the only machine pool to which the node will belong.</p>
</li>
<li>
<p><code>XT_U32TEST_HAS_NON_CNF_WORKERS=false</code> - Required to instruct the <code>xt_u32</code> negative test to skip because there are only nodes where the module is loaded.</p>
</li>
<li>
<p><code>SCTPTEST_HAS_NON_CNF_WORKERS=false</code> - Required to instruct the SCTP negative test to skip because there are only nodes where the module is loaded.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-reducing-nic-queues-using-the-performance-addon-operator"><a class="anchor" href="#ocp-4-8-reducing-nic-queues-using-the-performance-addon-operator"></a>Reducing NIC using the Performance Addon Operator</h4>
<div class="paragraph">
<p>The Performance Addon Operator allows you to adjust the Network Interface Card (NIC) queue count for each network device by configuring the performance profile. Device network queues allow packets to be distributed among different physical queues, and each queue gets a separate thread for packet processing.</p>
</div>
<div class="paragraph">
<p>For Data Plane Development Kit (DPDK) based workloads, it is important to reduce the NIC queues to only the number of reserved or housekeeping CPUs to ensure the desired low latency is achieved.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../scalability_and_performance/cnf-performance-addon-operator-for-low-latency-nodes.adoc#reducing-nic-queues-using-the-performance-addon-operator_cnf-master" class="page unresolved">Reducing NIC queues using the Performance Addon Operator</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-scale-cluster-maximums"><a class="anchor" href="#ocp-4-8-scale-cluster-maximums"></a>Cluster maximums</h4>
<div class="paragraph">
<p>Updated guidance around <a href="#../scalability_and_performance/planning-your-environment-according-to-object-maximums.adoc#planning-your-environment-according-to-object-maximums" class="page unresolved">cluster maximums</a> for OpenShift Container Platform 4.8 is now available.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>No large scale testing for performance against OVN-Kubernetes testing was executed for this release.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Use the <a href="https://access.redhat.com/labs/ocplimitscalculator/">OpenShift Container Platform Limit Calculator</a> to estimate cluster limits for your environment.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-performance-creator-tool"><a class="anchor" href="#ocp-4-8-performance-creator-tool"></a>Creating a performance profile</h4>
<div class="paragraph">
<p>You can now create a performance profile using the Performance Profile Creator (PPC) tool. The tool consumes <code>must-gather</code> data from the cluster and several user-supplied profile arguments, and using this information it generates a performance profile that is appropriate for your hardware and topology.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../scalability_and_performance/cnf-create-performance-profiles.adoc#cnf-create-performance-profiles" class="page unresolved">Creating a Performance Profile</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-nfd-operator"><a class="anchor" href="#ocp-4-8-nfd-operator"></a>Node Feature Discovery Operator</h4>
<div class="paragraph">
<p>The <a href="#../scalability_and_performance/psap-node-feature-discovery-operator.adoc#node-feature-discovery-operator" class="page unresolved">Node Feature Discovery (NFD) Operator</a> is now available. Use it to expose node-level information by orchestrating Node Feature Discovery, a Kubernetes add-on for detecting hardware features and system configuration.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-driver-toolkit"><a class="anchor" href="#ocp-4-8-driver-toolkit"></a>The Driver Toolkit</h4>
<div class="paragraph">
<p>You can now use <a href="#../scalability_and_performance/psap-driver-toolkit.adoc#driver-toolkit" class="page unresolved">the Driver Toolkit</a> as a base image for driver containers so that you can enable special software and hardware devices on Kubernetes.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="ocp-4-8-backup-and-restore"><a class="anchor" href="#ocp-4-8-backup-and-restore"></a>Backup and restore</h3>
<div class="sect3">
<h4 id="ocp-4-8-etcd-snapshot"><a class="anchor" href="#ocp-4-8-etcd-snapshot"></a>etcd snapshot enhancement</h4>
<div class="paragraph">
<p>A new enhancement validates the status of the etcd snapshot after backup and before restore. Previously, the backup process did not validate that the snapshot taken was complete, and the restore process did not verify that the snapshot being restored was valid, not corrupted. Now, if the disk is corrupted during backup or restore, the error is clearly reported to the admin. For more information, see <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1965024">BZ#1965024</a>.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="ocp-4-8-dev-exp"><a class="anchor" href="#ocp-4-8-dev-exp"></a>Developer experience</h3>

</div>
<div class="sect2">
<h3 id="ocp-4-8-insights-operator"><a class="anchor" href="#ocp-4-8-insights-operator"></a>Insights Operator</h3>
<div class="sect3">
<h4 id="ocp-4-8-insights-operator-restricted-network"><a class="anchor" href="#ocp-4-8-insights-operator-restricted-network"></a>Insights Advisor recommendations for restricted networks</h4>
<div class="paragraph">
<p>In OpenShift Container Platform 4.8, users operating in restricted networks can gather and upload Insights Operator archives to Insights Advisor to diagnose potential issues. Additionally, users can obfuscate sensitive data contained in the Insights Operator archive before upload.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../support/remote_health_monitoring/remote-health-reporting-from-restricted-network.adoc" class="page unresolved">Using remote health reporting in a restricted network</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-insights-advisor-improvements"><a class="anchor" href="#ocp-4-8-insights-advisor-improvements"></a>Insights Advisor improvements</h4>
<div class="paragraph">
<p>Insights Advisor in the OpenShift Container Platform web console now correctly reports 0 issues found. Previously, Insights Advisor gave no information.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-insights-operator-data-collection-enhancements"><a class="anchor" href="#ocp-4-8-insights-operator-data-collection-enhancements"></a>Insights Operator data collection enhancements</h4>
<div class="paragraph">
<p>In OpenShift Container Platform 4.8, the Insights Operator collects the following additional information:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Non-identifiable cluster workload information to find known security and version issues.</p>
</li>
<li>
<p>The <code>MachineHealthCheck</code> and <code>MachineAutoscaler</code> definitions.</p>
</li>
<li>
<p>The <code>virt_platform</code> and <code>vsphere_node_hw_version_total</code> metrics.</p>
</li>
<li>
<p>Information about unhealthy SAP pods to assist in the installation of SAP Smart Data Integration.</p>
</li>
<li>
<p>The <code>datahubs.installers.datahub.sap.com</code> resources to identfy SAP clusters.</p>
</li>
<li>
<p>A summary of failed <code>PodNetworkConnectivityChecks</code> to enhance networking.</p>
</li>
<li>
<p>Information about the <code>cluster-version</code> pods and events from the <code>openshift-cluster-operator</code> namespace to debug issues with the <code>cluster-version</code> Operator.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>With this additional information, Red Hat can provide improved remediation steps in Insights Advisor.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-insights-operator-gathering-sap"><a class="anchor" href="#ocp-4-8-insights-operator-gathering-sap"></a>Insights Operator enhancement for unhealthy SAP pods</h4>
<div class="paragraph">
<p>The Insights Operator can now gather data for unhealthy SAP pods. When the SDI installation fails, it is possible to detect the problem by looking at which of the initialization pods have failed. The Insights Operator now gathers information about failed pods in the SAP/SDI namespaces. For more information, see <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1930393"><strong>BZ#1930393</strong></a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-gathering-sap-datahubs"><a class="anchor" href="#ocp-4-8-gathering-sap-datahubs"></a>Insights Operator enhancement for gathering SAP pod data</h4>
<div class="paragraph">
<p>The Insights Operator can now gather <code>Datahubs</code> resources from SAP clusters. This data allows SAP clusters to be distinguished from non-SAP clusters in the Insights Operator archives, even in situations in which all of the data gathered exclusively from SAP clusters is missing and it would otherwise be impossible to determine if a cluster has an SDI installation. For more information, see <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1940432"><strong>BZ#1940432</strong></a>.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="ocp-4-8-auth"><a class="anchor" href="#ocp-4-8-auth"></a>Authentication and authorization</h3>
<div class="sect3">
<h4 id="ocp-4-8-auth-tp-aws-sts"><a class="anchor" href="#ocp-4-8-auth-tp-aws-sts"></a>Running OpenShift Container Platform using AWS Security Token Service (STS) for credentials is generally available</h4>
<div class="paragraph">
<p>You can now use the Cloud Credential Operator (CCO) utility (<code>ccoctl</code>) to configure the CCO to use the Amazon Web Services Security Token Service (AWS STS). When the CCO is configured to use STS, it assigns IAM roles that provide short-term, limited-privilege security credentials to components.</p>
</div>
<div class="paragraph">
<p>This feature was previously introduced as a Technology Preview feature in OpenShift Container Platform 4.7, and is now generally available in OpenShift Container Platform 4.8.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../authentication/managing_cloud_provider_credentials/cco-mode-sts.adoc#cco-mode-sts" class="page unresolved">Using manual mode with STS</a>.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="ocp-4.8-sandboxed-containers"><a class="anchor" href="#ocp-4.8-sandboxed-containers"></a>OpenShift sandboxed containers</h3>
<div class="sect3">
<h4 id="ocp-4.8-sandboxed-containers-tp"><a class="anchor" href="#ocp-4.8-sandboxed-containers-tp"></a>OpenShift sandboxed containers support on OpenShift Container Platform (Technology Preview)</h4>
<div class="paragraph">
<p>OpenShift sandboxed containers 1.0.0 Technology Preview release introduces built-in support for running Kata Containers as an additional runtime. OpenShift sandboxed containers enables users to choose Kata Containers as an additional runtime to provide additional isolation for their workloads. The OpenShift sandboxed containers Operator automates the tasks of installing, removing, and updating Kata Containers. It allows for tracking the state of those tasks by describing the <code>KataConfig</code> custom resource.</p>
</div>
<div class="paragraph">
<p>OpenShift sandboxed containers are only supported on bare metal. Red Hat Enterprise Linux CoreOS (RHCOS) is the only supported operating system for OpenShift sandboxed containers 1.0.0. Disconnected environments are not supported in OpenShift Container Platform 4.8.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="#../sandboxed_containers/understanding-sandboxed-containers.adoc#understanding-sandboxed-containers" class="page unresolved">Understanding OpenShift sandboxed containers</a></p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="ocp-4-8-notable-technical-changes"><a class="anchor" href="#ocp-4-8-notable-technical-changes"></a>Notable technical changes</h2>
<div class="sectionbody">
<div class="paragraph">
<p>OpenShift Container Platform 4.8 introduces the following notable technical changes.</p>
</div>
<h4 id="ocp-4-8-openstack-kuryr-service-changes" class="discrete">Kuryr service subnet creation Changes</h4>
<div class="paragraph">
<p>New installations of OpenShift Container Platform on Red Hat OpenStack Platform (RHOSP) with Open Virtual Network configured to use Kuryr no longer create a <code>services</code> subnet that is twice the size requested in <code>networking.serviceCIDR</code>. The subnet created is now the same as the requested size. For more information, see <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1955548"><strong>BZ#1955548</strong></a>.</p>
</div>
<h4 id="ocp-4-8-oauth-tokens" class="discrete">OAuth tokens without a SHA-256 prefix can no longer be used</h4>
<div class="paragraph">
<p>Prior to OpenShift Container Platform 4.6, OAuth access and authorize tokens used secret information for the object names.</p>
</div>
<div class="paragraph">
<p>Starting with OpenShift Container Platform 4.6, OAuth access token and authorize token object names are stored as non-sensitive object names, with a SHA-256 prefix. OAuth tokens that do not contain a SHA-256 prefix can no longer be used or created in OpenShift Container Platform 4.8.</p>
</div>
<h4 id="ocp-4-8-moderate-profiles" class="discrete">The Federal Risk and Authorization Management Program (FedRAMP) moderate controls</h4>
<div class="paragraph">
<p>In OpenShift Container Platform 4.8, the <code>rhcos4-moderate</code> profile is now complete. The <code>ocp4-moderate</code> profile will be completed in a future release.</p>
</div>
<h4 id="ocp-4-8-haproxy-2.2.13-upgrade" class="discrete">Ingress Controller upgraded to HAProxy 2.2.13</h4>
<div class="paragraph">
<p>The OpenShift Container Platform Ingress Controller is upgraded to HAProxy version 2.2.13.</p>
</div>
<h4 id="ocp-4-8-coreDNS-version-update" class="discrete">CoreDNS update to version 1.8.1</h4>
<div class="paragraph">
<p>In OpenShift Container Platform 4.8, CoreDNS uses version 1.8.1, which has several bug fixes, renamed metrics, and dual-stack IPv6 enablement.</p>
</div>
<h4 id="ocp-4-8-etcd-zap-logger" class="discrete">etcd now uses the zap logger</h4>
<div class="paragraph">
<p>In OpenShift Container Platform 4.8, etcd now uses zap as the default logger instead of capnslog. Zap is a structured logger that provides machine consumable JSON log messages. You can use <code>jq</code> to easily parse these log messages.</p>
</div>
<div class="paragraph">
<p>If you have a log consumer that is expecting the capnslog format, you might need to adjust it for the zap logger format.</p>
</div>
<div class="listingblock">
<div class="title">Example capnslog format (OpenShift Container Platform 4.7)</div>
<div class="content">
<pre class="highlightjs highlight nowrap"><code class="language-terminal hljs" data-lang="terminal">2021-06-03 22:40:16.984470 W | etcdserver: read-only range request "key:\"/kubernetes.io/operator.openshift.io/clustercsidrivers/\" range_end:\"/kubernetes.io/operator.openshift.io/clustercsidrivers0\" count_only:true " with result "range_response_count:0 size:8" took too long (100.498102ms) to execute</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example zap format (OpenShift Container Platform 4.8)</div>
<div class="content">
<pre class="highlightjs highlight nowrap"><code class="language-terminal hljs" data-lang="terminal">{"level":"warn","ts":"2021-06-14T13:13:23.243Z","caller":"etcdserver/util.go:163","msg":"apply request took too long","took":"163.262994ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/kubernetes.io/namespaces/default\" serializable:true keys_only:true ","response":"range_response_count:1 size:53"}</code></pre>
</div>
</div>
<h4 id="ocp-4-8-daemonset-merge-lso" class="discrete">Multiple daemon sets merged for LSO</h4>
<div class="paragraph">
<p>In OpenShift Container Platform 4.8, multiple daemon sets are merged for Local Storage Object (LSO). When you create a local volume custom resource, only <code>daemonset.apps/diskmaker-manager</code> is created.</p>
</div>
<h4 id="ocp-4-8-bound-svc-acct-token-vol" class="discrete">Bound service account token volumes are enabled</h4>
<div class="paragraph">
<p>Previously, service account tokens were secrets that were mounted into pods. Starting with OpenShift Container Platform 4.8, projected volumes are used instead. As a result of this change, service account tokens no longer have an underlying corresponding secret.</p>
</div>
<div class="paragraph">
<p>Bound service account tokens are audience-bound and time-bound. For more information, see <a href="#../authentication/bound-service-account-tokens.adoc#bound-service-account-tokens" class="page unresolved">Using bound service account tokens</a>.</p>
</div>
<div class="paragraph">
<p>Additionally, the kubelet refreshes tokens automatically after they reach 80% of duration, and <code>client-go</code> watches for token changes and reloads automatically. The combination of these two behaviors means that most usage of bound tokens is no different from usage of legacy tokens that never expire. Non-standard usage outside of <code>client-go</code> might cause issues.</p>
</div>
<h4 id="ocp-4-8-operator-sdk-v-1-8-0" class="discrete">Operator SDK v1.8.0</h4>
<div class="paragraph">
<p>OpenShift Container Platform 4.8 supports Operator SDK v1.8.0. See <a href="#../cli_reference/osdk/cli-osdk-install.adoc#cli-osdk-install" class="page unresolved">Installing the Operator SDK CLI</a> to install or update to this latest version.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Operator SDK v1.8.0 supports Kubernetes 1.20.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>If you have any Operator projects that were previously created or maintained with Operator SDK v1.3.0, see <a href="#../operators/operator_sdk/osdk-upgrading-projects.adoc#osdk-upgrading-projects" class="page unresolved">Upgrading projects for newer Operator SDK versions</a> to ensure your projects are upgraded to maintain compatibility with Operator SDK v1.8.0.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="ocp-4-8-deprecated-removed-features"><a class="anchor" href="#ocp-4-8-deprecated-removed-features"></a>Deprecated and removed features</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Some features available in previous releases have been deprecated or removed.</p>
</div>
<div class="paragraph">
<p>Deprecated functionality is still included in OpenShift Container Platform and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments. For the most recent list of major functionality deprecated and removed within OpenShift Container Platform 4.8, refer to the table below. Additional details for more fine-grained functionality that has been deprecated and removed are listed after the table.</p>
</div>
<div class="paragraph">
<p>In the table, features are marked with the following statuses:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>GA</strong>: <em>General Availability</em></p>
</li>
<li>
<p><strong>TP</strong>: <em>Technology Preview</em></p>
</li>
<li>
<p><strong>DEP</strong>: <em>Deprecated</em></p>
</li>
<li>
<p><strong>REM</strong>: <em>Removed</em></p>
</li>
</ul>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 1. Deprecated and removed features tracker</caption>
<colgroup>
<col style="width: 50%;">
<col style="width: 16.6666%;">
<col style="width: 16.6666%;">
<col style="width: 16.6668%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Feature</th>
<th class="tableblock halign-left valign-top">OCP 4.6</th>
<th class="tableblock halign-left valign-top">OCP 4.7</th>
<th class="tableblock halign-left valign-top">OCP 4.8</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>OperatorSource</code> objects</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">REM</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">REM</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">REM</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Package manifest format (Operator Framework)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DEP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DEP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">REM</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>oc adm catalog build</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DEP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DEP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">REM</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>--filter-by-os</code> flag for <code>oc adm catalog mirror</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DEP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">REM</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">v1beta1 CRDs</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DEP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DEP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DEP</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Docker Registry v1 API</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DEP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DEP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DEP</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Metering Operator</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DEP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DEP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DEP</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Scheduler policy</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DEP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DEP</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>ImageChangesInProgress</code> condition for Cluster Samples Operator</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DEP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DEP</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>MigrationInProgress</code> condition for Cluster Samples Operator</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DEP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DEP</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Use of <code>v1</code> in <code>apiVersion</code> for OpenShift Container Platform resources</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DEP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DEP</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Use of <code>dhclient</code> in Red Hat Enterprise Linux CoreOS (RHCOS)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DEP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DEP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DEP</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Cluster Loader</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DEP</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">External provisioner for AWS EFS</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">REM</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">REM</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">REM</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>lastTriggeredImageID</code> field in the <code>BuildConfig</code> spec for Builds</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DEP</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Jenkins Operator</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DEP</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">HPA custom metrics adapter based on Prometheus</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">REM</p></td>
</tr>
</tbody>
</table>
<div class="sect2">
<h3 id="ocp-4-8-deprecated-features"><a class="anchor" href="#ocp-4-8-deprecated-features"></a>Deprecated features</h3>
<div class="sect3">
<h4 id="ocp-4-8-descheduler-apigroup-deprecated"><a class="anchor" href="#ocp-4-8-descheduler-apigroup-deprecated"></a>Descheduler operator.openshift.io/v1beta1 API group is deprecated</h4>
<div class="paragraph">
<p>The <code>operator.openshift.io/v1beta1</code> API group for the descheduler is deprecated and might be removed in a future release. Use the <code>operator.openshift.io/v1</code> API group instead.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-dhclient-deprecated"><a class="anchor" href="#ocp-4-8-dhclient-deprecated"></a>Use of dhclient in Red Hat Enterprise Linux CoreOS (RHCOS) is deprecated</h4>
<div class="paragraph">
<p>Starting with OpenShift Container Platform 4.6, Red Hat Enterprise Linux CoreOS (RHCOS) switched to using <code>NetworkManager</code> in the <code>initramfs</code> to configure networking during early boot. As part of this change, the use of the <code>dhclient</code> binary for DHCP was deprecated. Use the <code>NetworkManager</code> internal DHCP client for networking configuration instead. The <code>dhclient</code> binary will be removed from Red Hat Enterprise Linux CoreOS (RHCOS) in a future release. See <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1908462"><strong>BZ#1908462</strong></a> for more information.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-cluster-loader-deprecated"><a class="anchor" href="#ocp-4-8-cluster-loader-deprecated"></a>Cluster Loader is deprecated</h4>
<div class="paragraph">
<p>Cluster Loader is now deprecated and will be removed in a future release.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-builds-lasttriggeredimageid-parameter"><a class="anchor" href="#ocp-4-8-builds-lasttriggeredimageid-parameter"></a>The lastTriggeredImageID parameter in builds is deprecated</h4>
<div class="paragraph">
<p>This release deprecates the <code>lastTriggeredImageID</code> in the <code>ImageChangeTrigger</code> object, which is one of the <code>BuildTriggerPolicy</code> types that can be set on a <code>BuildConfig</code> spec.</p>
</div>
<div class="paragraph">
<p>OpenShift Container Platform version 4.9 will remove support for <code>lastTriggeredImageID</code> and ignore it. Then, image change triggers will not start a build based on changes to the <code>lastTriggeredImageID</code> field in the <code>BuildConfig</code> spec. Instead, the image IDs that trigger a build will be recorded in the status of the <code>BuildConfig</code> object, which most users cannot change.</p>
</div>
<div class="paragraph">
<p>Therefore, update scripts and jobs that inspect <code>buildConfig.spec.triggers[i].imageChange.lastTriggeredImageID</code> accordingly. (<a href="https://issues.redhat.com/browse/BUILD-213"><strong>BUILD-213</strong></a>)</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-builds-jenkins-operator"><a class="anchor" href="#ocp-4-8-builds-jenkins-operator"></a>The Jenkins Operator (Technology Preview) is deprecated</h4>
<div class="paragraph">
<p>This release deprecates the Jenkins Operator, which was a Technology Preview feature. A future version of OpenShift Container Platform will remove the Jenkins Operator from OperatorHub in the OpenShift Container Platform web console interface. Then, upgrades for the Jenkins Operator will no longer be available, and the Operator will not be supported.</p>
</div>
<div class="paragraph">
<p>Customers can continue to deploy Jenkins on OpenShift Container Platform using the templates provided by the Samples Operator.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="ocp-4-8-removed-features"><a class="anchor" href="#ocp-4-8-removed-features"></a>Removed features</h3>
<div id="ocp-4-8-storage-aws-efs-removed-2nd" class="paragraph">
<div class="title">External provisioner for AWS EFS (Technology Preview) feature has been removed</div>
<p>The Amazon Web Services (AWS) Elastic File System (EFS) Technology Preview feature has been removed and is no longer supported.</p>
</div>
<div class="sect3">
<h4 id="ocp-4-8-images-removed-from-samples-imagestreams"><a class="anchor" href="#ocp-4-8-images-removed-from-samples-imagestreams"></a>Images removed from samples imagestreams</h4>
<div class="paragraph">
<p>The following images are no longer included in the samples imagestreams provided with OpenShift Container Platform:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="nowrap">registry.redhat.io/rhscl/nodejs-10-rhel7
registry.redhat.io/ubi7/nodejs-10
registry.redhat.io/rhscl/perl-526-rhel7
registry.redhat.io/rhscl/postgresql-10-rhel7
registry.redhat.io/rhscl/ruby-25-rhel7
registry.redhat.io/ubi7/ruby-25
registry.redhat.io/rhdm-7/rhdm-decisioncentral-rhel8:7.9.0
registry.redhat.io/rhdm-7/rhdm-kieserver-rhel8:7.9.0
registry.redhat.io/rhpam-7/rhpam-businesscentral-monitoring-rhel8:7.9.0
registry.redhat.io/rhpam-7/rhpam-businesscentral-rhel8:7.9.0
registry.redhat.io/rhpam-7/rhpam-smartrouter-rhel8:7.9.0</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-pkgman-format"><a class="anchor" href="#ocp-4-8-pkgman-format"></a>Package manifest format for Operators no longer supported</h4>
<div class="paragraph">
<p>Support for the legacy package manifest format for Operators is removed in OpenShift Container Platform 4.8 and later. This removal of support includes custom catalogs that were built with the legacy format and Operator projects initially created in the legacy format with the Operator SDK. The bundle format is the preferred Operator packaging format for Operator Lifecycle Manager (OLM) starting in OpenShift Container Platform 4.6.</p>
</div>
<div class="paragraph">
<p>For more information about working with the bundle format, see <a href="#../operators/admin/olm-managing-custom-catalogs.adoc#olm-managing-custom-catalogs" class="page unresolved">Managing custom catalogs</a> and <a href="#../operators/operator_sdk/osdk-pkgman-to-bundle.adoc#osdk-pkgman-to-bundle" class="page unresolved">Migrating package manifest projects to bundle format</a>.</p>
</div>
<div class="paragraph">
<p>In addition, the following commands related to the format have been removed from OpenShift CLI (<code>oc</code>) and the Operator SDK CLI:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>oc adm catalog build</code></p>
</li>
<li>
<p><code>operator-sdk generate packagemanifest</code></p>
</li>
<li>
<p><code>operator-sdk run packagemanifest</code></p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-hpa-prometheus"><a class="anchor" href="#ocp-4-8-hpa-prometheus"></a>Support for HPA custom metrics adapter based on Prometheus is removed</h4>
<div class="paragraph">
<p>This release removes the Prometheus Adapter, which was a Technology Preview feature.</p>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-oauth-secure-token-annotation"><a class="anchor" href="#ocp-4-8-oauth-secure-token-annotation"></a>Secure token storage annotation recognition is removed</h4>
<div class="paragraph">
<p>The <code>authentication</code> and <code>openshift-apiserver</code> Operators now ignore the <code>oauth-apiserver.openshift.io/secure-token-storage</code> annotation when picking the audit policy of a cluster. Audit policies now use <code>secure-</code> by default. For more information, see <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1879182"><strong>BZ#1879182</strong></a>.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="ocp-4-8-bug-fixes"><a class="anchor" href="#ocp-4-8-bug-fixes"></a>Bug fixes</h2>
<div class="sectionbody">
<div class="paragraph">
<p><strong>assisted-installer</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, the <code>assisted-service</code> container did not wait for <code>postgres</code> to start up and be ready to accept connections. The <code>assisted-service</code> container attempted to establish a database connection, failed, and the <code>assisted-service</code> container failed and restarted. This issue has been fixed by the <code>assisted-service</code> container attempting to connect to the database for up to 10 seconds. If <code>postgres</code> starts and is ready to accept connection within 10 seconds, the <code>assisted-service</code> container connects without going into an error state. If the <code>assisted-service</code> container is unable to connect to <code>postgres</code> within 10 seconds, it goes into an error state, restarts, and tries again. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1941859"><strong>BZ#1941859</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Bare Metal Hardware Provisioning</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, Ironic failed to download an image for installation because Ironic uses HTTPS by default and did not have the correct certificate bundle available. This issue is fixed by setting the image download as <code>Insecure</code> to request a transfer without the certificate. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1953795"><strong>BZ#1953795</strong></a>)</p>
</li>
<li>
<p>Previously, when using dual-stack networking, worker node host names sometimes did not match the host name that Ironic inspected before deployment. This caused nodes to need manual approval. This has been fixed. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1955114"><strong>BZ#1955114</strong></a>)</p>
</li>
<li>
<p>Previously, in UEFI mode, the <code>ironic-python-agent</code> created a UEFI bootloader entry after downloading the RHCOS image. When using an RHCOS image based on RHEL 8.4, the image could fail to boot using this entry. If the entry installed by Ironic was used when booting the image, the boot could fail and output a BIOS error screen. This is fixed by the <code>ironic-python-agent</code> configuring the boot entry based on a CSV file located in the image, instead of using a fixed boot entry. The image boots properly without error. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1972213"><strong>BZ#1972213</strong></a>)</p>
</li>
<li>
<p>Previously, a node sometimes selected the incorrect IP version upon startup (IPv6 instead of IPv4, or vice versa). The node would fail to start because it did not receive an IP address. This is fixed by the Cluster Bare Metal Operator passing the IP option to the downloader (<code>ip=dhcp</code> or <code>ip=dhcp6</code>), so this is set correctly at startup and the node starts as expected. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1946079"><strong>BZ#1946079</strong></a>)</p>
</li>
<li>
<p>Previously, the image caching mechanism in Ironic was disabled to enable a direct connection to the HTTP server that hosts the virtualmedial iso to prevent local storage issues. Non-standard compliant HTTP clients and redfish implementations caused failures on BMC connections. This has been fixed by reverting to the default Ironic behavior where the virtualmedia iso is cached and served from the Ironic conductor node. Issues caused by non-standard compliant HTTP clients and redfish implementations have been fixed. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1962905"><strong>BZ#1962905</strong></a>)</p>
</li>
<li>
<p>Previously, the machine instance <code>state</code> annotation was not set. Consequently, the <strong>STATE</strong> column was empty. With this update, the machine instance <code>state</code> annotation is now set, and information in the <strong>STATE</strong> column automatically populates. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1857008"><strong>BZ#1857008</strong></a>)</p>
</li>
<li>
<p>Because newer ipmitool packages default to using cipher suite 17, older hardware that does not support cipher suite 17 fails during deployment. When cipher suite 17 is not supported by the hardware, Ironic now uses cipher suite 3 so that deployments on older hardware using ipmitool should succeed. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1897415"><strong>BZ#1897415</strong></a>)</p>
</li>
<li>
<p>Previously, in some cases, adoption occurred before the image cache was populated, which resulted in permanent adoption failure, and no retry was attempted. This caused the control plane bare metal hosts to report <code>adoption failed.</code> With this update, adoption of externally provisioned hosts automatically retries after adoption failure until control plane hosts are correctly adopted. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1905577"><strong>BZ#1905577</strong></a>)</p>
</li>
<li>
<p>Previously, the Custom Resource (CR) required the Baseboard Management Controller (BMC) details. However, with assisted installers this information was not provided. This update allows the CR to bypass the BMC details when the operator is not creating the nodes. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1913112"><strong>BZ#1913112</strong></a>)</p>
</li>
<li>
<p>When provisioning an image to nodes, qemu-image was restricted to 1G of RAM, which could cause the qemu-img to crash. This fix increases the limit to 2G so that qemu-img now completes provisioning reliably. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1917482"><strong>BZ#1917482</strong></a>)</p>
</li>
<li>
<p>Because the redfish/v1/SessionService URL requires authentication, Ironic would generate an authentication error when accessing the site. Because there was no functional problem when this error message was reported by Ironic, it has been removed. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1924816"><strong>BZ#1924816</strong></a>)</p>
</li>
<li>
<p>For some drives, the partition, for example <code>/dev/sda1</code>, did not have a read-only file. The base device, for example, <code>/dev/sda</code> had this file, however. Therefore, Ironic could not determine that the partition was read-only, which could cause metadata cleaning to fail on that drive. This update ensures that the partition is detected as read-only, and includes an additional check for the base device. As a result, metadata cleaning is not performed on the read-only partition and metadata cleaning no longer fails. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1935419"><strong>BZ#1935419</strong></a>)</p>
</li>
<li>
<p>When a Baremetal IPI was deployed with a proxy configured, the internal machine-os image download was directed through the proxy. This corrupted the image and prevented it from being downloaded. This update fixes the internal image traffic to <code>no_proxy</code>, so that the image download no longer uses a proxy. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1962592"><strong>BZ#1962592</strong></a>)</p>
</li>
<li>
<p>Previously, bare metal deployments failed if large packet transfers between Ironic and the RAM disk resulted in connection failures. With this update, Ironic queries the RAM disk for information to work around the connection error, allowing deployments to succeed. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1957976"><strong>BZ#1957976</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Builds</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, after <a href="https://https://access.redhat.com/security/cve/cve-2021-3344">CVE-2021-3344</a> was fixed, builds did not automatically mount entitlement keys on the OpenShift Container Platform node. As a result, when the entitlement certificates were stored on the host or node, the fix prevented entitled builds from working seamlessly. The failure to bring in entitlement certificates stored on the host or node was fixed with <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1945692">BZ#1945692</a> in 4.7.z and <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1946363">BZ#1946363</a> in 4.6.z; however, those fixes introduced a benign warning message for builds running on Red Hat Enterprise Linux CoreOS (RHCOS) worker nodes. The current release fixes this issue by allowing builds to automatically mount entitlements only on RHEL worker nodes, and avoiding mount attempts on RHCOS worker nodes. Now, there will not be any benign warnings around entitlement mounts when running builds on RHCOS nodes.
(<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1951084"><strong>BZ#1951084</strong></a>)</p>
</li>
<li>
<p>Some users pulling images from Docker Hub can encounter the following error:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight nowrap"><code class="language-terminal hljs" data-lang="terminal">container image registry lookup failed...toomanyrequests: You have reached your pull rate limit</code></pre>
</div>
</div>
<div class="paragraph">
<p>This error happens because the <code>docker.io</code> login they used to call the <code>oc new-app</code> does not have sufficient paid support with <code>docker.io</code>. The resulting application is subject to image pull throttling, which can produce failures. The current release updates the <code>oc new-app</code> help to remind users how defaulting works for the image registry and repository specs, so users can, when possible, use non-default image references to avoid similar errors.
(<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1928850"><strong>BZ#1928850</strong></a>)</p>
</div>
</li>
<li>
<p>Previously, builds did not perform an error check to see if an image push had failed. As a result, builds always logged the <code>Successfully pushed</code> message. Now, builds check if an error has occurred, and only log the <code>Successfully pushed</code> message after an image push has succeeded.
(<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1947164"><strong>BZ#1947164</strong></a>)</p>
</li>
<li>
<p>Previously, the documentation and <code>oc explain</code> help text did not convey that the <code>buildArgs</code> field in the <code>BuildConfig</code> object does not support the <code>valueFrom</code> field of its underlying Kubernetes <code>EnvVar</code> type. As a result, users believed it was supported and tried to use it. The current release updates the documentation and help text, so it is more apparent that the <code>BuildConfig</code> object&#8217;s <code>buildArgs</code> field does not support the <code>valueFrom</code> field.
(<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1956826"><strong>BZ#1956826</strong></a>)</p>
</li>
<li>
<p>When builds interact with image registries, such as pulling base images, intermittent communications issues can produce build failures. The current release increases the number of retries to these interactions. Now, OpenShift Container Platform builds are more resilient when they encounter intermittent communication issues with image registries.
(<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1937535"><strong>BZ#1937535</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Cloud Compute</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, <code>Cluster Image Registry Operator</code> considered <code>user_domain_name</code> an immutable field and would not modify it after installation. This resulted in a refusal to accept changes to <code>user_domain_name</code> and resulting credentials. This update marks <code>user_domain_name</code> as mutable and does not store it in the image registry config. This allows <code>user_domain_name</code> and all other <code>auth</code> parameters to be modified after installation. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1937464"><strong>BZ#1937464</strong></a>)</p>
</li>
<li>
<p>Previously, a proxy update caused a full cluster configuration update, including API server restart, during continuous integration (CI) runs. Consequently, some clusters in the Machine API Operator would time out because of unexpected API server outages. This update separates proxy tests and adds postconditions so that clusters in the Machine API Operator become stable again during CI runs. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1913341"><strong>BZ#1913341</strong></a>)</p>
</li>
<li>
<p>Previously, deleting a machine stuck in <code>Insufficient disk space on datastore</code> took longer than expected because there was no distinction between various vCenter task types. With this update, the machine controller deletion procedure checks the vCenter task type, and no longer blocks the deletion of the machine controller. As a result, the machine controller is deleted quickly. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1918101"><strong>BZ#1918101</strong></a>)</p>
</li>
<li>
<p>Previously, scaling from zero annotations requeued even if the instance type was missing. Consequently, there were constant requeue and error space messages in the MachineSet controller logs. With this update, users can set the annotation manually if the instance type is not resolved automatically. As a result, scaling from zero for unknown instance types works if users manually provide the annotation. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1918910"><strong>BZ#1918910</strong></a>)</p>
</li>
<li>
<p>Previously, HTTP responses were not closed properly by the Machine API termination handler. Consequently, goroutines were leaked in <code>net.http</code> read and write loops, which led to high memory usage. This update ensures that HTTP responses are always closed properly. As a result, memory usage is now stable. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1934021"><strong>BZ#1934021</strong></a>)</p>
</li>
<li>
<p>Previously, multiple client sets created inside of the MachineSet controller caused slow startup times, which resulted in pods failing readiness checks in some large clusters. Consequently, the MachineSet controller would get stuck in an endless loop. This update fixes the MachineSet controller so that it uses a single client. As a result, the MachineSet controller behaves as expected. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1934216"><strong>BZ#1934216</strong></a>)</p>
</li>
<li>
<p>Previously, instances took longer to boot when an upgrade was performed by the Machine Config Daemon on the first boot. Consequently, worker nodes were stuck in restart loops, and machine healthchecks (MCHs) removed the worker nodes because they did not properly start. With this update, MHCs no longer remove nodes that have not started correctly. Instead, MHCs only remove nodes when explicitly requested. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1939054"><strong>BZ#1939054</strong></a>)</p>
</li>
<li>
<p>Previously, a certificate signing request (CSR) approval was delayed for an unknown reason. Consequently, new machines appearing in the cluster during installation were not approved quickly enough, prolonging cluster installation. To mitigate occasional API server unavailability in early installation phases, this update changes the cache resync period from 10 hours to 10 minutes. As a result, control plane machines are now approved faster so that cluster installation is no longer prolonged. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1940972"><strong>BZ#1940972</strong></a>)</p>
</li>
<li>
<p>Previously, the default Google Cloud Platform (GCP) image was out of date and referenced a version from the OpenShift Container Platform 4.6 release that did not support newer Ignition versions. Consequently, new machines in clusters that used the default GCP image were not able to boot OpenShift Container Platform 4.7 and later. With this update, the GCP image is updated to match the release version. As a result, new machines can now boot with the default GCP image. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1954597"><strong>BZ#1954597</strong></a>)</p>
</li>
<li>
<p>Previously, due to a strict check of the virtual machine&#8217;s (VM) ProvisioningState value, the VM would sometimes fail during an existence check. With this update, the check is more lenient so that only deleted machines go into a <code>Failed</code> phase during an existence check. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1957349"><strong>BZ#1957349</strong></a>)</p>
</li>
<li>
<p>Previously, if you deleted a control plane machine using <code>oc delete machine</code> in an AWS cluster, the machine was not removed from the load balancers. As a result, the load balancer continued to serve requests to the removed control plane machine. With this fix, when you remove a control plane machine, the load balancer no longer serves requests to the machine. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1880757"><strong>BZ#1880757</strong></a>)</p>
</li>
<li>
<p>Previously, when deleting an unreachable machine, the vSphere Virtual Machine Disk (VMDK) that is created for persistent volumes and attached to the node was incorrectly deleted. As a result, the data on the VMDK was unrecoverable. With this fix, the vSphere cloud provider checks for and detaches these disks from the node if the kubelet is not reachable. As a result, you can delete an unreachable machine without losing the VMDK. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1883993"><strong>BZ#1883993</strong></a>)</p>
</li>
<li>
<p>Previously, because a generated list of AWS instance types was out of date, some newer Amazon Web Services (AWS) instance types were not able to scale from zero when using the Cluster Autoscaler Operator and machine sets with zero replicas. The list of AWS instance types is now updated to include newer instance types. With this fix, more instance types are available to the Cluster Autoscaler Operator for scaling from zero replicas. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1896321"><strong>BZ#1896321</strong></a>)</p>
</li>
<li>
<p>Previously, pod disruption budgets did not drain pods on an unreachable node due to missing upstream eviction API features. As a result, machines on unreachable nodes could take excessive amounts of time to be removed after being deleted. Now, the grace period timeout is changed to 1 second when deleting machines on unreachable nodes. With this fix, the Machine API can successfully drain and delete nodes that are unreachable. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1905709"><strong>BZ#1905709</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Cloud Credential Operator</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, the Cloud Credential Operator repeated an <code>unsupported platform type: BareMetal</code> warning on bare metal platforms. With this update, bare metal platforms are no longer treated as unknown platforms. As a result, misleading logging messages are reduced. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1864116"><strong>BZ#1864116</strong></a>)</p>
</li>
<li>
<p>Previously, a recurring error message stored in the <code>credentialsRequest</code> custom resources (CRs) of the Cloud Credential Operator led to excessive CPU usage and logging in some error scenarios, such as Amazon Web Services (AWS) rate limiting. This update removes the request ID coming back from the cloud provider so that error messages are stored in conditions where users can more easily find them, and eliminates recurring error messages in the <code>credentialsRequest</code> CR. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1910396"><strong>BZ#1910396</strong></a>)</p>
</li>
<li>
<p>Previously, both the Cloud Credential Operator (CCO) and the Cluster Version Operator (CVO) reported if the CCO deployment was unhealthy. This resulted in double reporting if there was an issue. With this release, the CCO no longer reports if its deployment is unhealthy. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1957424"><strong>BZ#1957424</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Cluster Version Operator</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, the Cluster Version Operator evaluated both the <code>Available</code> and <code>Degraded</code> parameters when setting the <code>cluster_operator_up</code> metric, which caused the <code>ClusterOperatorDown</code> alert to be displayed for Operators with <code>Available=True</code> or <code>Degraded=True</code>, even though <code>Available=True</code> did not match the alert description of "has not been available". With this fix, the Cluster Version Operator now ignores the <code>Degraded</code> parameter when setting the <code>cluster_operator_up</code> metric. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1834551"><strong>BZ#1834551</strong></a>)</p>
</li>
<li>
<p>Previously, when Prometheus was installed on the cluster, important platform topology metrics were not available and a CI error would occur if the installer metric that was generated with the invoker was set to <code>""</code>. The possible race condition in which informers were not synced before metrics were served that was causing the error has now been fixed. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1871303"><strong>BZ#1871303</strong></a>)</p>
</li>
<li>
<p>Previously, manifests with multiple tolerations for the same key, such as the Cluster Version Operator&#8217;s own deployment), would accept only the last entry read and overwrite prior entries. This caused <code>in-cluster tolerations</code> to diverge from the manifest&#8217;s listed tolerations. With this update, the Cluster Version Operator now considers tolerations matching when they are completely equal. This allows the Cluster Version Operator to keep all tolerations present in the manifest for the <code>in-cluster resource</code>. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1941901"><strong>BZ#1941901</strong></a>)</p>
</li>
<li>
<p>Previously, the Cluster Version Operator did not reconcile <code>env</code> and <code>envFrom</code> for manifests that did not set those properties. This meant the Cluster Version Operator did not properly manage container environments. This update improves the Cluster Version Operator so that it now clears <code>env</code> and <code>envFrom</code> if they are unset in the manifest. This allows the cluster to automatically recover from invalid <code>cluster-admin</code> changes to these properties. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1951339"><strong>BZ#1951339</strong></a>)</p>
</li>
<li>
<p>Previously, manifests with multiple tolerations for the same key, such as the <code>cluster-version-operator</code> deployment object, would accept only the last entry read and overwrite prior entries. This caused in-cluster tolerations to diverge from the manifest&#8217;s listed tolerations. With this update, the Cluster Version Operator now considers tolerations to match when they are equal. This allows the Cluster Version Operator to keep all tolerations present in the manifest for the in-cluster resource. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1941901"><strong>BZ#1941901</strong></a>)</p>
</li>
<li>
<p>Previously, the Cluster Version Operator reported a <code>ClusterOperatorDegraded</code> alert when <code>ClusterOperator</code> resources were degraded for 10 minutes. This alert was sometimes occurred prematurely during installation as resources were still being created. This update changes the 10-minute period to 30 minutes, providing enough time for the installation to progress without premature <code>ClusterOperatorDegraded</code> alerts. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1957991"><strong>BZ#1957991</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Compliance Operator</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, when users ran a compliance check, <code>NON-COMPLIANT</code> results were given with no indication of required remediation steps for the user to act upon. This release provides an <code>instructions</code> key that allows users to review the steps needed to verify a rule. This allows users and auditors to verify that an Operator is checking a correct value. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1919367"><strong>BZ#1919367</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Console Kubevirt Plug-in</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, in a web console form that helped users add boot sources to virtualization templates, an explanatory text gave information only for Fedora, regardless of what operating system the template used. This update adds a fix that provides examples that are specific to the template&#8217;s operating system so users have relevant guidance. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1908169"><strong>BZ#1908169</strong></a>)</p>
</li>
<li>
<p>Previously, in a web console wizard that helped users create virtual machine templates, imprecise language made it unclear whether an operation applied to the template or to a virtual machine. This fix clarifies the description so users can make an informed decision. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1922063"><strong>BZ#1922063</strong></a>)</p>
</li>
<li>
<p>Previously, a vague error message in the web console caused unnecessary confusion for some users who tried to add a network interface to a virtual machine they were creating from a template. This update adds detail to the error message so users can troubleshoot the error more easily. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1924788"><strong>BZ#1924788</strong></a>)</p>
</li>
<li>
<p>Previously, when you tried to create a virtual machine from a Red Hat Enterprise Linux (RHEL) 6 template in the web console, a pop-up window gave information about how to define the support level, even through RHEL 6 is not supported. This fix changes the text in this window to make it clear that RHEL 6 is not supported. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1926776"><strong>BZ#1926776</strong></a>)</p>
</li>
<li>
<p>Previously, a drop-down list in the web console was obscured by button elements, leaving users unable to select certain operating systems when creating a virtual machine. This fix includes an adjustment to the button elements' <code>z-index</code> value, which corrects the error and lets users select any of the available operating systems. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1930015"><strong>BZ#1930015</strong></a>)</p>
</li>
<li>
<p>Previously, if you used the web console&#8217;s new virtual machine wizard on a cluster with no defined storage classes, the web console got stuck in an infinite loop and crashed. This fix removes the storage class drop-down list in instances where no storage classes are defined. As a result, the web console does not crash. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1930064"><strong>BZ#1930064</strong></a>)</p>
</li>
<li>
<p>Previously, text in a button element did not clearly describe the button&#8217;s function, which is to remove a VM template from a list of favorites. This fix updates the text to clarify what the button does. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1937941"><strong>BZ#1937941</strong></a>)</p>
</li>
<li>
<p>Previously, for virtual machines that have a <code>RerunOnFailure</code> run strategy, stopping the virtual machine resulted in several UI elements becoming unresponsive, preventing users from reading status information or restarting the virtual machines. This update fixes the unresponsive elements so users can use those features. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1951209"><strong>BZ#1951209</strong></a>)</p>
</li>
<li>
<p>Previously, for clusters that were configured to have a separate <code>/var</code> partition, querying the file system only returned the size of disks mounted in the root directory, excluding the size of the <code>/var</code> partition. This fix changes how that query runs, and users can now determine the total size of the file system on a cluster. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1960612"><strong>BZ#1960612</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Console Storage Plug-in</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, the OpenShift Container Storage Operator displayed an error message when the correct storage class was not available. This update removes the error message and disables the <strong>Next</strong> button until the correct storage class is available. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1924641"><strong>BZ#1924641</strong></a>)</p>
</li>
<li>
<p>Previously, when a user clicked the browser&#8217;s back button while creating an internal-attached storage cluster, the installation wizard restarted the process. This update fixes the issue. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1928008"><strong>BZ#1928008</strong></a>)</p>
</li>
<li>
<p>When you add a node to local volume discovery, you can now see a list of existing nodes, which reduces unnecessary navigation. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1947311"><strong>BZ#1947311</strong></a>)</p>
</li>
<li>
<p>Previously, the <strong>Create Storage Cluster</strong> wizard let you enable an arbiter zone that had an undefined value. The fix in this update filters out undefined values so arbiter zones can be created only with defined values. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1926798"><strong>BZ#1926798</strong></a>)</p>
</li>
<li>
<p>Previously, quick start cards were displayed incorrectly in the web console because of inconsistencies in how product titles were spelled and how the registered trade mark symbol was used. In this update, the product names are spelled correctly, and the registered trademark symbol appears consistently in the first card only. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1931760"><strong>BZ#1931760</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>DNS</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1936587"><strong>BZ#1936587</strong></a> set the global CoreDNS cache max TTL to 900 seconds. Consequently, NXDOMAIN records received from upstream resolvers were cached for 900 seconds. This update explicitly caches negative DNS response records for a maximum of 30 seconds. As a result, resolving NXDOMAINs are no longer cached for 900 seconds. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1943578"><strong>BZ#1943578</strong></a>)</p>
</li>
<li>
<p>The fix for <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1953097">BZ#1953097</a> enabled the CoreDNS <code>bufsize</code> plug-in with a size of 1232 bytes. Some primitive DNS resolvers are not capable of receiving DNS response messages over UDP that are greater than 512 bytes. Consequently, some DNS resolvers, such as Gos internal DNS library, are unable to receive verbose DNS responses from the DNS Operator. This update sets the CoreDNS <code>bufsize</code> to 512 bytes for all servers. As a result, UDP DNS messages are now properly received. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1966116"><strong>BZ#1966116</strong></a>)</p>
</li>
<li>
<p>Previously, cluster upstream resolver returned DNS responses that exceeded 512 bytes via UDP. Consequently, coreDNS returned <code>SERVFAIL</code> or other error messages and forced the client to retry over TCP. This update enabled the coreDNS bufisze plug-in with a UDP buffer size of 1232 bytes. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1949361"><strong>BZ#1949361</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>etcd</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, there was a transport leak in the etcd Operator, which caused memory usage to grow over time. The memory leak has been fixed. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1925586"><strong>BZ#1925586</strong></a>)</p>
</li>
<li>
<p>Previously, the <code>etcdInsufficientMembers</code> alert fired incorrectly. With this release, the alert is updated to include the pod label in addition to the instance label, so that the alert only fires when quorum is lost. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1929944"><strong>BZ#1929944</strong></a>)</p>
</li>
<li>
<p>Previously, the readiness probe was not reporting the correct readiness due to the introduction of SO_REUSEADDR socket options, which caused the etcd pod to show as ready even though the etcd-quorum-guard failed. The readiness probe checks were updated to account for these options, and the etcd readiness probe now properly reflects the readiness of the operand. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1946607"><strong>BZ#1946607</strong></a>)</p>
</li>
<li>
<p>Previously, the <code>spec.loglevel</code> field did not set the <code>log-level</code> flag on the etcd operand, so users could not change the etcd log level. Users can now set the log levels as follows:</p>
<div class="openblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p><code>Debug</code>, <code>Trace</code>, and <code>TraceAll</code> log levels map to the etcd <code>debug</code> log level</p>
</li>
<li>
<p><code>Default</code> or <code>Normal</code> log levels map to the etcd <code>info</code> log level</p>
</li>
</ul>
</div>
</div>
</div>
<div class="paragraph">
<p>For more information, see <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1948553"><strong>BZ#1948553</strong></a>.</p>
</div>
</li>
<li>
<p>Previously, following an etcd process, the next process did not start until the relevant ports were released. With the addition of <code>SO_REUSEADDR</code> to this process, the ports can be reused immediately. For more information, see <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1927942"><strong>BZ#1927942</strong></a>.</p>
</li>
<li>
<p>Previously, the etcd-endpoint&#8217;s ConfigMap was left empty if the <code>network.Status.ServiceNetwork</code> field was unpopulated. As a result, the etcd Operator failed to scale up. A new feature in OpenShift Container Platform 4.8 allows the etcd Operator to scale up when the <code>network.Status.ServiceNetwork</code> field is unpopulated. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1902247"><strong>BZ#1902247</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Image Registry</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, the image pruner stopped when it failed to delete an image. As a result, when two image pruners were trying to delete an image concurrently, one of them failed with the <code>not found</code> error. With this update, <code>not found</code> errors are ignored, which allows the image pruner to tolerate concurrent deletions. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1890828"><strong>BZ#1890828</strong></a>)</p>
</li>
<li>
<p>Previously, a lack of route status inclusion during the Image Registry Operator status assessment meant that the Image Registry Operator was not degraded, even with routes in the <code>degraded</code> state. With this fix, the Image Registry Operator now fetches all configured routes and evaluates their statuses when assessing its own status. With this update, if any of the routes are <code>degraded</code>, the Image Registry Operator reports itself as <code>degraded</code> with an error message. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1902076"><strong>BZ#1902076</strong></a>)</p>
</li>
<li>
<p>Previously, an automatically created Docker config secret did not include credentials for integrated internal registry routes. Because no credentials were present for accessing the registry through any of its routes, pods attempting to reach the registry failed due to lack of authentication. This fix includes all configured registry routes in the default Docker credential secret. Now, pods can reach the integrated registry by any of its routes, as credentials now contain an entry for each route. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1911470"><strong>BZ#1911470</strong></a>)</p>
</li>
<li>
<p>Previously the image registry was ignoring cluster wide <code>ImageContentSourcePolicy</code> (ICSP) rules. During pull-through, images mirrors were ignored, which caused pull failures in disconnected clusters. With this update, the registry pulls from mirrors if ICSP rules exist for the target repository. As a result, pulling an image from configured mirrors does not fail. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1918376"><strong>BZ#1918376</strong></a>)</p>
</li>
<li>
<p>Previously, the Image Registry Operator did not update the <code>.status.readyReplicas</code> field of the config resource, so its value was always <code>0</code>. With this fix, the Image Registry Operator writes the number of ready image registry replicas from the deployment into the config. Now, this field shows how many image registry replicas are ready. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1923811"><strong>BZ#1923811</strong></a>)</p>
</li>
<li>
<p>Azure recommends users use Storage Accounts <code>v2</code> instead of <code>v1</code>. Under certain security profiles, administrators can force Azure to not accept the creation of <code>v1</code> Storage Accounts. Because the image registry depends on <code>v1</code> Storage Accounts, a cluster install would fail in such environments. With this fix, during cluster bootstrap, the Image Registry Operator now attempts to create and use <code>V2</code> Storage Accounts. Clusters running on <code>v1</code> continue using <code>V1</code> Storage Accounts. Installation succeeds and Image Registry Operator now reports <code>Available</code>. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1929654"><strong>BZ#1929654</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>ImageStreams</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, performance was sometimes slow when importing multiple images from a stream. With this release, the number of concurrent requests to the image registry is increased from five to 50, resulting in improved performance. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1954715"><strong>BZ#1954715</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Insights Operator</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, the Insights Operator did not collect Cluster Version Operator (CVO) pods or events in the <code>openshift-cluster-version</code> namespace. As a result, the Insights Operator did not display information about any problems that the CVO might experience, and users could not get diagnostic information about the CVO. The Insights Operator is now updated to gather the CVO pods and events from the <code>openshift-cluster-operator</code> namespace so that issues with the the CVO are reported by the Insights Operator. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1942271"><strong>BZ#1942271</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Installer</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, DNSmasq required specifying the prefix length when an IPv6 network was anything other than /64. Consequently, control plane hosts failed to PXE boot. This update includes the subnet prefix length in the DNSmasq configuration. As a result, control plane hosts will now DHCP and PXE boot on IPv6 networks of any prefix length.  (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1927068"><strong>BZ#1927068</strong></a>)</p>
</li>
<li>
<p>When installing to vSphere, the bootstrap machine sometimes did not correctly update the name servers in the <code>/etc/resolv.conf</code> file. As a result, the bootstrap machine could not access the temporary control plane, and installations failed. This fix includes changes that makes finding the right line to update more reliable. With the bootstrap manager now able to access its temporary control plane, installations are able to succeed. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1967355"><strong>BZ#1967355</strong></a>)</p>
</li>
<li>
<p>Previously, the installer did not take into account the region where the bootstrap Ignition config should be located when generating its URL. Consequently, the bootstrap machine could not fetch the config from the provided URL because it was incorrect. This update takes the user&#8217;s region into account when generating the URL and selects the correct public endpoint. As a result, the installer always generates correct bootstrap Ignition URLs. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1934123"><strong>BZ#1934123</strong></a>)</p>
</li>
<li>
<p>Previously, the default version of Azure&#8217;s Minimum TLS was 1.0 when creating a storage account. Consequently, policy checks would fail. This update configures the openshift-installer Azure client to set the Minimum TLS version to 1.2 when creating a storage account. As a result, policy checks now pass. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1943157"><strong>BZ#1943157</strong></a>)</p>
</li>
<li>
<p>Previously, private clusters deployed using IPI on Azure had an inbound NSG rule that allowed SSH to the bootstrap node. This allowance could trigger Azure&#8217;s security policy. With this update, that NSG rule has been removed. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1943219)"><strong>BZ#1943219</strong></a>)</p>
</li>
<li>
<p>Previously, the installer did not recognize the <code>ap-northeast-3</code> AWS region. With this update, the installer allows installs to unknown regions that fit the pattern for known partitions, which allows users to create infrastructure in the <code>ap-northeast-3</code> AWS region. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1944268)"><strong>BZ#1944268</strong></a>)</p>
</li>
<li>
<p>Previously, on-premise platforms lacked the capability to create internal load balancers. With this update, a check has been added when users create manfiests to ensure that this strategy is only used on cloud platforms such as AWS, Azure, and GCP. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1953035)"><strong>BZ#1953035</strong></a>)</p>
</li>
<li>
<p>Previously, when naming Google Cloud Platform resources, a filter prevented certain names using the word <code>Google</code> from being used. This update adds a check in the installer on the cluster name that allows some variations of the word <code>Google</code> to be used when setting a name. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1955336)"><strong>BZ#1955336</strong></a>)</p>
</li>
<li>
<p>Previously, a bare metal installation with installer-provisioned infrastructure required that the installer process was able to communicate with the provisioning network. Now, the installer process can communicate with the virtual IP for the API server. This change enables cases when the provisioning network is not routable and the installer process is run from a remote location, such as Hive for Red Hat OpenStack Platform (RHOSP) or Red Hat Advanced Cluster Management. You might need to adjust your firewall rules to allow communication with TCP ports <code>6385</code> and <code>5050</code> on virtual IP for the API server. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1932799"><strong>BZ#1932799</strong></a>)</p>
</li>
<li>
<p>Previously, when the installation on Red Hat OpenStack Platform (RHOSP) was provided an ID of a subnet that did not exist in the <code>platform.openstack.machinesSubnet</code> field, the <code>openshift-install</code> command produced a SIGSEGV and backtrace. Now, the <code>openshift-install</code> command is revised so that it produces an error like the following message:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight nowrap"><code class="language-terminal hljs" data-lang="terminal">FATAL failed to fetch Metadata: failed to load asset "Install Config": platform.openstack.machinesSubnet: Not found: "&lt;network-ID&gt;"</code></pre>
</div>
</div>
<div class="paragraph">
<p>(<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1957809"><strong>BZ#1957809</strong></a>)</p>
</div>
</li>
<li>
<p>Previously, the installation on Red Hat OpenStack Platform (RHOSP) failed unless the RHOSP HTTPS certificate was imported to the hosting device. Now, a successful installation occurs when the <code>cacert</code> value in <code>cloud.yaml</code> is set to the RHOSP HTTPS certificate. Importing the certificate to the host is no longer required. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1786314"><strong>BZ#1786314</strong></a>)</p>
</li>
<li>
<p>Previously, an installation might fail due to inaccurate external network entries in <code>proxy.config.openshift.io</code>. A validation check now identifies these inaccuracies to enable correction. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1873649"><strong>BZ#1873649</strong></a>)</p>
</li>
<li>
<p>Previously vague or confusing Terraform component descriptions are now replaced with clearer information. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1880758"><strong>BZ#1880758</strong></a>)</p>
</li>
<li>
<p>A previous change to gophercloud/utils introduced a custom HTTP client that used a self-signed certificate. Because this change removed settings from <code>DefaultTransport</code>, including those for proxy environment variables, this caused failures for installations that used both self-signed certificates and proxies. In this update, the custom HTTP client inherits settings from <code>DefaultTransport</code>, so now OpenShift Container Platform can be installed with self-signed certificates and proxies. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1925216"><strong>BZ#1925216</strong></a>)</p>
</li>
<li>
<p>Previously, the installer did not take into account <code>defaultMachineSet</code> values in the install config during its validation which caused the installer to fail. This update applies the default values to the install config and starts validating empty fields. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1903055"><strong>BZ#1903055</strong></a>)</p>
</li>
<li>
<p>Previously, <code>soft-anti-affinity</code> required the client to set a minimum Nova microversion. Most versions of Ansible OS server module did not automatically require the client to set a minimum. As a result, the soft-anti-affinity commands were likely to fail. This update fixes the use of the Python OpenStack client to set a Nova microversion when dealing with soft-anti-affinity. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1910067"><strong>BZ#1910067</strong></a>)</p>
</li>
<li>
<p>Previously, OpenStack UPI playbooks did not tag all resources that were created. Consequently, the <code>openshift-install destroy</code> command failed to properly identify all of the cluster resources and looped over resource deletion until it reached a timeout, which left resources behind. This update adds missing tag instructions to OpenStack UPI playbooks. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1916593"><strong>BZ#1916593</strong></a>)</p>
</li>
<li>
<p>Previously, <code>e2e-gcp-upi</code> did not succeed because of a Python package error which resulted in a failure. With this update, you can set the correct Python version, pip version, and the <code>CLOUDSDK_PYTHON</code> for gsutil to resolve the package error. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1917931"><strong>BZ#1917931</strong></a>)</p>
</li>
<li>
<p>Previously, pip version 21 did not support installed Python version 2. As a consequence, this lead to an error resolving all dependent packages required to setup the container. In this update, the pip version is fixed to a value less than 21 to avoid the problem. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1922235"><strong>BZ#1922235</strong></a>)</p>
</li>
<li>
<p>Previously, the installer collected information about the cloud twice. Consequently, there were double the number of requests to  OpenStack API, which created an additional load on the cloud and increased the installation time. This update fixes the issue by collecting information about the cloud before it checks for quota and then reuses the same information for validations. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1923038"><strong>BZ#1923038</strong></a>)</p>
</li>
<li>
<p>Previously, when deploying with IPv6 provision network with subnet other than /64, the DNSmasq required specifying the prefix length. Consequently, the hosts failed to PXE boot when using a non-/64 network. This update includes the prefix length in the DNSmasq configuration. As a result, the hosts succeed on DHCP and PXE boot on IPv6 networks of any prefix length. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1925291"><strong>BZ#1925291</strong></a>)</p>
</li>
<li>
<p>Previously, the OpenShift Container Platform installer was not reporting IAM permission issues when removing the <code>Shared Subnet</code> tag even though the logging indicated that they were removed. This update checks the results for untagging and logging errors. The logs now indicate the status of untagging shared resources. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1926547"><strong>BZ#1926547</strong></a>)</p>
</li>
<li>
<p>Previously, the Azure clusters were created with the disk type of Premium_LRS and with an instance type that did not support PremiumIO capabilities which caused the cluster to fail. This update checks to see if the instance type picked has the PremiumIO capabilities only if the disk type is Premium_LRS which is the default disk type. The code queries the Azure subscription and region to get the information required and returns an error if the conditions are not met. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1931115"><strong>BZ#1931115</strong></a>)</p>
</li>
<li>
<p>Previously, the API VIP could become unavailable on the bootstrap when the API server restarted, which made the provisioning services unavailable and caused provisioning to fail. The provisioning services (Ironic) are now included in the VIP health checks, and the API VIP remains available. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1949859"><strong>BZ#1949859</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>kube-apiserver</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, Google Cloud Platform (GCP) load balancer health checkers left stale conntrack entries on the host, which caused network interruptions to the API server traffic that used the GCP load balancers. The health check traffic no longer loops through the host, so there is no longer network disruption against the API server. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1925698"><strong>BZ#1925698</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Machine Config Operator</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously,  the <code>drain timeout</code> and pool degrading period was too short and would cause alerts prematurely on a normal cluster that needed more time. With this update, the time needed before a timeout reports a failure is extended. This provides the Cluster Operator with more realistic and useful alerts without prematurely degrading performance of a normal cluster. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1968019"><strong>BZ#1968019</strong></a>)</p>
</li>
<li>
<p>Previously, while creating a new virtual machine from VMware vSphere with the OpenShift Installer Provisioned Infrastructure (IPI), the node failed to join the cluster. This occurred when Dynamic Host Configuration Protocol (DHCP), entered a host name in place of the name provided by IDI. This has been resolved. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1920807"><strong>BZ#1920807</strong></a>)</p>
</li>
<li>
<p>Previously, installation might fail if the network was enabled before the host name was set. This prevented the node from joining the cluser and forced a five minute delay before another attempt could be made. This is now resolved and the node automatically joins the cluster during the first attempt. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1899187"><strong>BZ#1899187</strong></a>)</p>
</li>
<li>
<p>Previously, users were able to delete the core user and related SSH keys, although the keys remained. With this update, users cannot delete the core user.
(<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1885186"><strong>BZ#1885186</strong></a>)</p>
</li>
<li>
<p>When upgrading from 4.6 to 4.7, the host name set by the <strong>vsphere-hostname</strong> service was applied only when a node was installed. If the host name was not statically set prior to upgrading, the host name could be lost. This update removes the condition which allowed the <strong>vsphere-hostname</strong> service to run only when a node is installed. As a result, vSphere host names are no longer lost when upgrading. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1942207"><strong>BZ#1942207</strong></a>)</p>
</li>
<li>
<p>Due to a bug in <code>keepalived</code> 2.0.10, if a liveness probe killed a <code>keepalived</code> container, any virtual IP addresses (VIPs) that were assigned to the system remained and were not cleaned up when <code>keepalived</code> restarted. As a result, multiple nodes could hold the same VIP. Now, the VIPs are removed when <code>keepalived</code> is started. As a result, VIPs are held by a single node. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1931505"><strong>BZ#1931505</strong></a>)</p>
</li>
<li>
<p>Previously, rpm-ostree related operations were not handled properly on non-CoreOS nodes such as Red Hat Enterprise Linux CoreOS (RHCOS). As a result, RHEL nodes were degraded when an operation, such as kernel switching, was applied in the pool that contained RHEL nodes. With this update, the Machine Config Daemon logs a message whenever a non-supported operation is performed on non-CoreOS nodes. After logging the message, it returns nil instead of an error. RHEL nodes in the pool now proceed as expected when an unsupported operation is performed by the Machine Config Daemon. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1952368"><strong>BZ#1952368</strong></a>)</p>
</li>
<li>
<p>Previously, empty static pod files were being written to the <code>/etc/kubernetes/manifests</code> directory. As a result, the kubelet log was reporting errors that could cause confusion with some users. Empty manifests are now moved to a different location when they are not needed. As a result, the errors do not appear in the kubelet log. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1927042"><strong>BZ#1927042</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Metering Operator</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, the Reporting Operator incorrectly handled <code>Report</code> custom resources (CRs) that contained a user-provided retention period when reconciling events. Consequently, an expired <code>Report</code> CR would cause the Reporting Operator to continually loop, as the affected custom resources are requeued indefinitely.  This update avoids requeueing expired <code>Report</code> CRs that have specified a retention period. As a result, the Reporting Operator correctly handles events for expired <code>Report</code> CRs. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1926984"><strong>BZ#1926984</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Monitoring</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, the <code>mountstats</code> collector for the <code>node-exporter</code> daemontset caused high memory usage on nodes with NFS mount points. With this update, users can now disable the <code>mountstats</code> collector to reduce memory usage. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1955467"><strong>BZ#1955467</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Networking</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, an incorrect <code>keepalived</code> setting sometimes resulted in the VIP ending up on an incorrect system and unable to move back to the correct system. With this update, the incorrect <code>keepalived</code> setting is removed so that the VIP ends up on the correct system. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1916890"><strong>BZ#1916890</strong></a>)</p>
</li>
<li>
<p>Due to iptables rewriting rules, clients that used a fixed source port to connect to a service via both the service IP and a pod IP might have encountered problems with port conflicts. With this update, an additional OVS rule is inserted to notice when port conflicts occur and to do an extra SNAT to avoid said conflicts. As a result, there are no longer port conflicts when connecting to a service. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1910378"><strong>BZ#1910378</strong></a>)</p>
</li>
<li>
<p>Previously, IP port 9 between control plane nodes and egress-assigned nodes was blocked by the internal firewall. This caused the assignment of IP addresses to egress nodes to fail. This update enables access between control plane and egress nodes via IP port 9. As a result, the assignment of IP addresses to egress nodes is now successfully permitted. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1942856"><strong>BZ#1942856</strong></a>)</p>
</li>
<li>
<p>Previously, UDP services traffic could be blocked because of stale connection tracking entries that were no longer valid. This pevented access to a server pod after it was cycled for <code>NodePort</code> service. With this update, the connection tracking entries are purged in the case of <code>NodePort</code> service cycling, which allows new network traffic to reach cycled endpoints. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1949063"><strong>BZ#1949063</strong></a>)</p>
</li>
<li>
<p>Previously, OVN-Kubernetes network provider ignored network policies with multiple <code>ipBlocks</code>. Every ipBlock after the first one was ignored, resulting in pods being unable to reach all of the configured IP addresses. The code for generating OVN ACLs from Kubernetes network policies has been corrected. As a result, network policies with multiple <code>ipBlocks</code> now work correctly. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1953680"><strong>BZ#1953680</strong></a>)</p>
</li>
<li>
<p>Previously, when using the OVN-Kubernetes cluster network provider, a Kubernetes service without any endpoints erroneously accepted connections. With this update, a load balancer is no longer created for services without endpoints and therefore traffic is no longer accepted. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1918442"><strong>BZ#1918442</strong></a>)</p>
</li>
<li>
<p>Previously, the Container Network Interface (CNI) plug-in for Multus did not understand IPv6 addresses that started with any number of zeros. With this update, the CNI plug-in works with IPv6 that start with values greater than zero. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1919048"><strong>BZ#1919048</strong></a>)</p>
</li>
<li>
<p>Previously, a race condition might be triggered if the SR-IOV Network Operator initiated a reboot when a change in the machine config policy also triggered a reboot. If this occurred, the node was left in an indeterminate state. With this update, that situation is avoided. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1921321"><strong>BZ#1921321</strong></a>)</p>
</li>
<li>
<p>Previously, when creating a new user-provisioned cluster with the Kuryr cluster network provider, the OpenStack subset used by the cluster nodes might be undetected, which caused the cluster installation to time out. With this update, the subnet is correctly detected and a user-provisioned installation succeeds. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1927244"><strong>BZ#1927244</strong></a>)</p>
</li>
<li>
<p>Previously, when upgrading from OpenShift Container Platform 4.6 to OpenShift Container Platform 4.7, the Cluster Network Operator (CNO) incorrectly marked itself as having completed its upgrade to the next version. If the upgrade subsequently failed then the CNO reported itself as <code>degraded</code>, but erroneously as being at version 4.7. With this update, the CNO waits for the cluster network provider images to upgrade successfully before reporting the CNO upgrade as successful. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1928157"><strong>BZ#1928157</strong></a>)</p>
</li>
<li>
<p>Previously, when using the OVN-Kubernetes cluster network provider, the endpoint slice controller might not run if the Kubernetes version included a minor version that contained non-numeric characters. With this update, the endpoint slice controller is enabled by default. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1929314"><strong>BZ#1929314</strong></a>)</p>
</li>
<li>
<p>When using the Kuryr cluster network provider, Neutron Ports created subsequent to the installation were named with a different pattern than Neutron Ports created during installation. As a result, Neutron Ports created after installation were not added to the default load balancer. With this update, Kuryr detects Neutron Ports created with either naming convention. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1933269"><strong>BZ#1933269</strong></a>)</p>
</li>
<li>
<p>Previously, Open Virtual Network (OVN) changed the source IP addresses of hairpin traffic packets to the IP address of the load balancer, which sometimes blocked traffic when a network policy was in use. With this update, Kuryr opens traffic to the IP addresses of all services in a network policy&#8217;s namespace, and hairpin traffic flows freely. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1920532"><strong>BZ#1920532</strong></a>)</p>
</li>
<li>
<p>Previously, when starting a single-stack IPv6 cluster on nodes with IPv4 address, the kubelet might have used the IPv4 IP instead of the IPv6 IP for the node IP. Consequently, host network pods would have IPv4 IPs rather than IPv6 IPs, which made them unreachable from IPv6-only pods. This update fixes the node-IP-picking code, which results in the kubelet using the IPv6 IPs. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1939740"><strong>BZ#1939740</strong></a>)</p>
</li>
<li>
<p>Previously, and for unknown reasons, a kubelet could register the wrong IP address for a node. As a consequence, the node would be in a <code>NotReady</code> state until it was rebooted. Now, the systemd manager configuration is reloaded with the valid IP address as an environment variable, meaning that nodes no longer enter a <code>NotReady</code> state because a kubelet registered the wrong IP address. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1940939"><strong>BZ#1940939</strong></a>)</p>
</li>
<li>
<p>Previously, refactoring for a shadowed variable caused a regression related to the use of the checkpoint file, and SR-IOV pod sandboxes would not start. A check for the path of the kubelet socket was not properly accounted for during the refactor. The fix properly restores the check for the kubelet socket path, and now the SR-IOV pod sandboxes are properly created. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1968625"><strong>BZ#1968625</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Node</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, Reliable Autonomic Distributed Object Store (RADOS) Block Devices (RBDs) were visible in unprivileged container pods running <code>lsbkk</code>. This has been fixed and RBDs no longer are visible in unprivileged container pods running <code>lsblk</code>. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1772993"><strong>BZ#1772993</strong></a>).</p>
</li>
<li>
<p>Previously, during a cluster upgrade, the <code>/etc/hostname</code> file was altered by CRI-O, which caused the nodes to fail and to return when rebooting. This update adds special handling in CRI-O to leave the <code>/etc/hosts</code> file alone during upgrade, which allows upgraded nodes to boot without trouble. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1921937"><strong>BZ#1921937</strong></a>)</p>
</li>
<li>
<p>Previously, CRI-O was taking too long to create a pod after the network had been provisioned. This would trigger a bug in the network cleanup code, causing network resources not to properly clean up after network resources have been provisioned. This update changes the code to properly clean up networking resources, even if a command has timed out. This allows the cluster to continue normal network operation even if pod creation was taking too long. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1957224"><strong>BZ#1957224</strong></a>)</p>
</li>
<li>
<p>Previously, node reboots using a <code>CNI</code> plug-in would not complete successfully.  CRI-O was modified to call <code>CNI DEL</code> on all containers that were running before reboot. This update cleans up <code>CNI</code> resources and allows a successful reboot. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1948137"><strong>BZ#1948137</strong></a>)</p>
</li>
<li>
<p>Previously, a <code>CNI DEL</code> request would not be recalled if it failed, because <code>CNI</code> cleanup operations would not check for cleanup failure. Now, CRI-O recalls <code>CNI DEL</code> requests until they succeed, correctly cleaning up <code>CNI</code> resources. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1948047"><strong>BZ#1948047</strong></a>)</p>
</li>
<li>
<p>Previously, a reboot request to a container or image could cause failure if the reboot occurred while the container or image was being committed to the disk. This caused apparent corruption of storage for the container and created failures to pull the image or recreate containers from the image. This update detects when a node has rebooted and clears the container storage if true. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1942536"><strong>BZ#1942536</strong></a>)</p>
</li>
<li>
<p>Previously, <code>runc</code> took on the permissions of the entity that ran it. However, permissions on the <code>workdir</code> are set by the <code>container</code> user. When those permissions differ, container creation errors occured and caused failure of the container startup. This patch updates <code>runc</code> to <code>chdir</code> to the <code>workdir</code> multiple times, in case only one time fails. This ensures that creation of the container will succeed. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1934177"><strong>BZ#1934177</strong></a>)</p>
</li>
<li>
<p>Previously, the CRI-O logs did not contain information about the source where images were pulled from. With this fix, the log pull source is added to the info level of the CRI-O logs. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1881694"><strong>BZ#1881694</strong></a>)</p>
</li>
<li>
<p>Previously, when pods were created and deleted rapidly, a pod might not have enough time to complete the pod sandbox creation before the pod started deletion. As a result, pod deletion could fail with a `ErrCreatePodSandbox`error. This error is now ignored if a pod is terminating. As a result, pod termination no longer fails if a pod could not complete the pod sandbox creation. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1908378"><strong>BZ#1908378</strong></a>)</p>
</li>
<li>
<p>Previously, the Machine Config Operator (MCO) did not accept <strong>trace</strong> as a valid log level. As a result, the MCO could not provide a method to enable trace-level logging, even though CRI-O supports it. The MCO is now updated to support the <strong>trace</strong> log level. As a result, users can see a trace log level through the MCO configurations. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1930620"><strong>BZ#1930620</strong></a>)</p>
</li>
<li>
<p>Previously, the kubelet tried to get the status of images that are not completely pulled. As a result, <code>crictl</code> reports a <code>error locating item named "manifest"</code> error for these images. CRI-O is now updated to not list images that do not have a manifest. As a result, <code>crictl</code> no longer reports these errors. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1942608"><strong>BZ#1942608</strong></a>)</p>
</li>
<li>
<p>Previously, outdated status messages were not removed. Because of this, the Machine Config Operator (MCO) was sometimes unable to locate the proper machine config pool. With this release, a cleanup function is added to limit the number of statuses. As a result,  the MCO keeps at most 3 different kubeletConfig status. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1950133"><strong>BZ#1950133</strong></a>)</p>
</li>
<li>
<p>Previously, when upgrading from OpenShift Container Platform version 4.6.25, in clusters with more than one <code>kubeletconfig</code> CR or <code>ContainerRuntimeConfig</code> CR, the Machine Config Operator (MCO) could generate duplicate machine configs for the same configuration. Consequently, the upgrade failed because the MCO would use the old controller version (IGNITIONVERSION 3.1.0). This update cleans up outdated duplicate machine configs and allows proper upgrading from version 4.6.25. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1955517"><strong>BZ#1955517</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>oauth-apiserver</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, some OAuth server metrics were not initialized properly and did not appear in searches in the Prometheus UI. The missing OAuth server metrics are now initialized properly and appear in the Prometheus UI metrics searches. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1892642"><strong>BZ#1892642</strong></a>)</p>
</li>
<li>
<p>Previously, if a custom security context constraint (SCC) contained a combination of the <code>defaultAllowPrivilegeEscalation: false</code> and <code>allowPrivilegedContainer: true</code> fields, the security context mutator mutated the privileged <code>openshift-apiserver</code> and <code>oauth-apiserver</code> pods to a state that failed API validation. The pods failed to start, which sometimes caused an OpenShift API outage. The security context mutator now ignores the <code>defaultAllowPrivilegeEscalation</code> field for containers that are already privileged, and custom SCCs that include those fields do not prevent the pods from starting. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1934400"><strong>BZ#1934400</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>oc</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, when running the <code>oc explain</code> command, the resource group name was not automatically detected if it was provided as part of the resource string. If two resources in different groups had the same resource name, the highest priority definition was returned unless the group was stated through the <code>--api-version</code> parameter. Now, if the <code>--api-version</code> parameter is not included, a prefix check is run against the resource string to detect the group name. The explanation returned by the command relates to the matching resource in the stated group. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1725981"><strong>BZ#1725981</strong></a>)</p>
</li>
<li>
<p>Previously, the <code>oc image extract</code> command did not extract files from the root directory of an image. The command has been updated and can now be used to extract files from the image root directory. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1919032"><strong>BZ#1919032</strong></a>)</p>
</li>
<li>
<p>Previously, the <code>oc apply</code> command would fetch the OpenAPI specification on each invocation. The OpenAPI specification is now cached when the command is first run. The cached OpenAPI specification is reused when the <code>oc apply</code> command is run multiple times and the network load is reduced. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1921885"><strong>BZ#1921885</strong></a>)</p>
</li>
<li>
<p>Previously, the authorization header created during image mirroring could exceed the header size limit for some registries. This would cause an error during the mirroring operation. Now, the <code>--skip-multiple-scopes</code> option is set to <code>true</code> for the <code>oc adm catalog mirror</code> command, to help prevent the authorization header from exceeding the header size limits. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1946839"><strong>BZ#1946839</strong></a>)</p>
</li>
<li>
<p>Previously, the <code>storageClassName</code> attribute was not added to a <code>PersistentVolumeClaim</code> object when the <code>oc volume set</code> command included the <code>--claim-class</code> option. The value of the <code>--claim-class</code> option was added to the <code>volume.beta.kubernetes.io/storage-class</code> annotation instead. This would cause snapshots for the volume to fail due to a dependency on the <code>storageClassName</code> attribute. Now, the <code>oc volume set</code> command applies the value of the <code>--claim-class</code> option to the <code>storageClassName</code> attribute in the <code>PersistentVolumeClaim</code> object, and volume snapshots can reference the attribute value. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1954124"><strong>BZ#1954124</strong></a>)</p>
</li>
<li>
<p>Previously, the output of <code>oc adm top --help</code> stated that the <code>oc adm top</code> command could display CPU, memory, and storage resource usage for pods and nodes. The <code>oc adm top</code> command does not display storage resource usage. Now, the storage reference is not included in the <code>oc adm top --help</code> output. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1959648"><strong>BZ#1959648</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Operator Lifecycle Manager (OLM)</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, <code>CustomResourceDefinition</code> (CRD) objects applied as part of an Operator installation could sometimes satisfy the installation requirements of a newer version of the same Operator. Consequently, during an Operator upgrade, the version being replaced could be prematurely removed. In some cases, the upgrade would stop. With this update, the CRDs that are created or updated as part of the Operator bundle installation are annotated to indicate their bundle of origin. These annotations are used by the <code>ClusterServiceVersion</code> (CSV) object to distinguish between pre-existing CRDs and same-bundle CRDs. As a result, upgrades will not complete until the current version&#8217;s CRDs have been applied. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1947946"><strong>BZ#1947946</strong></a>)</p>
</li>
<li>
<p>Previously, pods that ran an index referenced by a <code>CatalogSource</code> object did not have <code>readOnlyRootFileSystem: false</code> explicitly set in their <code>securityContext</code> field. Consequently, if a security context constraint (SCC) existed that enforced <code>readOnlyRootFileSystem: true</code> and matched the <code>securityContext</code> of the that pod, it would be assigned to that pod and cause it to fail repeatedly. This update explicitly sets <code>readOnlyRootFileSystem: false</code> in the <code>securityContext</code> field. As a result, pods that are referenced by a <code>CatalogSource</code> object are no longer matched to SCCs that enforce a read-only root file system and no longer fail. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1961472"><strong>BZ#1961472</strong></a>)</p>
</li>
<li>
<p>Operator Lifecycle Manager (OLM) previously did not allow skipped versions to be installed if the version was specified in the <code>startingCSV</code> field during initial installations. This caused those skipped versions to be unable to be installed, even if users wanted to install them regardless of reasons why they are skipped. This fix updates OLM to allow users to install skipped versions only during initial installation by using the <code>startingCSV</code> specification in <code>Subscription</code> objects; users still cannot upgrade to skipped versions, as expected. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1906056"><strong>BZ#1906056</strong></a>)</p>
</li>
<li>
<p>Because <code>k8s.io/apiserver</code> was not handling context errors for the webhook authorizer, context errors, such as timeouts, caused the authorizer to panic. This fix increments the API server version to include an upstream fix for the issue, and as a result, the authorizer can gracefully handle context errors. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1913525"><strong>BZ#1913525</strong></a>)</p>
</li>
<li>
<p>Previously, the <code>oc adm catalog mirror</code> command could not be easily used to mirror Operator catalogs across an airgapped environment. With this enhancement, the contents of a catalog can be mirrored to a file system, placed onto removable media, and then mirrored back from the file system to a registry for usage by an airgapped cluster. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1919168"><strong>BZ#1919168</strong></a>)</p>
</li>
<li>
<p>The Catalog Operator previously created bundle unpacking jobs for an install plan without setting a timeout. In the case of a non-existent or deleted bundle image, this caused the job to run forever and the install plan would stay in the <code>Installing</code> phase with no indication of the job&#8217;s pod failing to resolve the image. With this fix, the Catalog Operator now sets a default <code>10m</code> timeout on bundle unpacking jobs, which can be configured by using <code>--bundle-unpack-timeout</code> flag. As a result, bundle unpacking jobs fail after the configured timeout, and the installation also transitions to a <code>Failed</code> phase with the reason visible in <code>status.conditions</code> and <code>status.bundleLookups.conditions</code> properties. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1921264"><strong>BZ#1921264</strong></a>)</p>
</li>
<li>
<p>Operators that were installed on clusters prior to OpenShift Container Platform 4.6 were previously not identified as coming from a given Operator package for the purposes of dependency resolution and upgrade selection. This caused existing Operator installations to conflict with the criteria of their own subscription, which blocked upgrades and dependency resolution within the namespace. This fix updates OLM to infer the package name and version for Operators that are referenced by a subscription. As a result, upgrades and dependency resolution proceed as expected. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1921953"><strong>BZ#1921953</strong></a>)</p>
</li>
<li>
<p>The <code>Info</code> log level used for transient errors caused verbose OLM Operator logs for the default configuration. This fix changes the transient error log level to <code>debug</code>. As a result, fewer non-critical logs are visible for the <code>debug</code> configuration. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1925614"><strong>BZ#1925614</strong></a>)</p>
</li>
<li>
<p>Previously, the <code>spec.config.resources</code> section of a <code>Subscription</code> object was always applied to the installed deployment, even when it was unset or empty. This caused resources defined in the cluster service version (CSV) to be ignored, and only the resources defined in the <code>spec.config.resources</code> section of the <code>Subscription</code> object were used. This fix updates OLM to override deployment-specific resources only when the <code>spec.config.resources</code> section is set to a non-nil or non-empty value. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1926893"><strong>BZ#1926893</strong></a>)</p>
</li>
<li>
<p>During dependency and upgrade resolution, subscription uniqueness was previously based on the subscribed package name. If two subscriptions in a namespace subscribe to the same package, they are treated as a single subscription internally, resulting in unexpected behaviors. With this fix, subscriptions are now uniquely identified internally within a namespace by <code>.metadata.name</code> instead of <code>.spec.name</code>. As a result, upgrade and dependency resolution behavior is consistent for namespaces containing multiple <code>Subscription</code> objects with the same <code>.spec.name.</code> (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1932001"><strong>BZ#1932001</strong></a>)</p>
</li>
<li>
<p>When less than one minute remains before an upcoming catalog update polling attempt, the interval jitter function truncates the resync interval down to zero. This caused the Operator Catalog to enter a hot-loop, wasting CPU cycles. This fix increases precision of the jitter function used to calculate resync delays. As a result, the Catalog Operator remains mostly idle until the next catalog update poll.
(<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1932182"><strong>BZ#1932182</strong></a>)</p>
</li>
<li>
<p>During an Operator upgrade, the owner reference of any associated <code>ServiceAccount</code> object was updated to point to the new <code>ClusterServiceVersion</code> (CSV) object instead of the old one. This could cause a race condition to occur between the OLM Operator, which reconciles CSVs, and the Catalog Operator, executes install plans, marking the old CSV as <code>Pending/RequirementsNotMet</code> due to the service account ownership change. This blocked upgrade completion while the new CSV waited indefinitely for the old CSV to indicate a healthy status. With this fix, instead of updating owner references in one step, the second owner is now appended to any existing owners. As a result, the same service account can satisfy requirements for both the old and the new CSV. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1934080"><strong>BZ#1934080</strong></a>)</p>
</li>
<li>
<p>Cluster service versions (CSV) previously required associated service accounts to either have no <code>ownerReferences</code> values set or to have an <code>ownerReferences</code> value set to the related CSV. This caused <code>default</code> service account, which is not created as part of Operator installation, to be unsatisfied as a CSV requirement if its <code>metadata.ownerReferences</code> field was not empty. With this fix, CSVs now require associated service accounts to either have no <code>ownerReferences</code> values set to CSVs or to have an <code>ownerReference</code> value set to the related CSV. As a result, service accounts with only non-CSV <code>ownerReferences</code> values can satisfy the requirements of any CSV. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1935909"><strong>BZ#1935909</strong></a>)</p>
</li>
<li>
<p>Prior to OpenShift Container Platform 4.5, the default catalogs deployed and managed by the Marketplace Operator in the <code>openshift-marketplace</code> namespace were created by <code>OperatorSource</code> objects, which was the API exposed by the Marketplace Operator. Appropriate metrics and alerting were instrumented to indicate an error encountered by the Operator sources. In OpenShift Container Platform 4.6, the <code>OperatorSource</code> resource was removed after being deprecated for several releases, and the Marketplace Operator instead directly created OLM&#8217;s <code>CatalogSource</code> resource. However, the same metrics and alerting instrumentation was not done for catalog sources deployed in the <code>openshift-marketplace</code> namespace. Therefore, errors encountered by the default catalog sources were not highlighted with Prometheus alerting. This fix introduces the new <code>catalogsource_ready</code> metric in OLM, which is used by the Marketplace Operator to fire alerts whenever the metric for a default catalog source indicates that a catalog source is in an unready state. As a result, Prometheus alerts are now provided for unready default catalog sources in the <code>openshift-marketplace</code> namespace. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1936585"><strong>BZ#1936585</strong></a>)</p>
</li>
<li>
<p>Previously, when a candidate Operator dependency was available from its default channel and a non-default channel, the Operator Lifecycle Manager (OLM) could generate a subscription that arbitrarily specified either of the two channels. Now, Operator dependencies are satisfied by candidates from the default channel first and then from other channels. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1945261"><strong>BZ#1945261</strong></a>)</p>
</li>
<li>
<p>Previously, it was possible that a cluster service version (CSV) was copied as a component of multiple Operators. This could happen when a namespace was added to an Operator group after the Operator was installed. This behavior affected memory use and CPU load. Now, CSVs do not appear in the <code>status.components</code> field of an Operator with a reason of <code>Copied</code>, and performance is not affected. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1946838"><strong>BZ#1946838</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Operator SDK</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, some resources were caught in infinite loops because <code>ManagedFields</code> was being processed during reconciliation. This fix updates <code>operator-lib</code> to ignore  <code>ManagedFields</code>, resulting in consistently reconciled loops. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1856714"><strong>BZ#1856714</strong></a>)</p>
</li>
<li>
<p>A minimum help message was being printed for the Operator SDK because the default plug-in was not invoked when <code>--help</code> was passed on the command line interface (CLI). This fix invokes the default plug-in and prints a more useful help message when a user runs the <code>operator-sdk init --help</code> command. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1866222"><strong>BZ#166222</strong></a>)</p>
</li>
<li>
<p>Previously, if ran with missing optional validators, <code>operator-sdk bundle</code> would fail rather than issue warnings. This has been corrected. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1921727"><strong>BZ#1921727</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>openshift-apiserver</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, custom security context constraints (SCCs) could have a higher priority than others in a default set. Consequently, those SSCs were sometimes matched to <code>openshift-apiserver</code> pods, which broke their ability to write in their root file system. This bug also caused an outage of some OpenShift APIs. This fix explicitly mentions in the <code>openshift-apiserver</code> pods that the root file system should be writable. As a result, custom SCCs should not prevent <code>openshift-apiserver</code> pods from running. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1942725"><strong>BZ#1942725</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Performance Addon Operator</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, when configuring a container to provide low latency response, dynamic interrupt masks with CRI-O did not match the interrupt mask set by <code>irqbalance</code> system service. Each one set different masks and compromised container latency. This update changes the interrupt mask set by setting CRI-O to match the <code>irqbalance</code> system service. As a result, dynamic interrupt mask handling now works as expected. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1934630"><strong>BZ#1934630</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>RHCOS</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, multipath was enabled too late in the boot process. Consequently, Red Hat Enterprise Linux CoreOS (RHCOS) would return I/O errors in some multipath environments. With this update, multipath is now enabled earlier in the boot process. As a result, RHCOS no longer returns I/O errors in some multipath environments.  (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1954025"><strong>BZ#1954025</strong></a>)</p>
</li>
<li>
<p>Previously, a potential race condition could cause a fetch of the rootfs in a Red Hat Enterprise Linux CoreOS (RHCOS) PXE deployment to fail in some environments. With this fix, a connectivity check has been added that retries before an attempt to pull the rootfs so that access to the remote server and rootfs file is verified before continuing to the point where the <code>coreos-livepxe-rootfs</code> script used to sometimes fail. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1871303"><strong>BZ#1871303</strong></a>)</p>
</li>
<li>
<p>Previously, user presets for <code>MachineConfig</code> were ignored. This meant users could not change the configuration of <code>kdump.service</code>. Now, the priority level of default presets are lower than user configured defaults, so the user configuration can properly override vendor configuration. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1969208"><strong>BZ#1969208</strong></a>)</p>
</li>
<li>
<p>Previously, the <code>coreos-installer</code> would refuse to install onto a disk with a corrupted GUID Partition Table (GPT) because it would try to read the GPT of the target disk before overwriting it with the install image. With this fix, the <code>coreos-installer</code> now successfully installs onto a disk with a corrupt GPT by only reading the GPT of the target disk when it is instructed to preserve existing partitions. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1914976"><strong>BZ#1914976</strong></a>)</p>
</li>
<li>
<p>Previously, installation of clusters on unformatted direct-access storage devices (DASD) would result in the creation of disk sectors written incorrectly by <code>coreos-installer</code>. Now <code>coreos-installer</code> correctly formats new, unformatted DASD drives to 4096 byte sectors. This allows <code>coreos-installer</code> to complete the installation of the OS image to the disk drive. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1905159"><strong>BZ#1905159</strong></a>)</p>
</li>
<li>
<p>Previously, hardware-assisted <code>zlib</code> decompression on s390x z15 systems caused the mounting of the RHEL rootfs image to fail, which resulted in boot failure for REHL s390x z15 nodes using the RHEL 8.3 kernel. The kernel has now been updated to correctly handle <code>zlib</code>-compressed squashfs files when hardware-assisted <code>zlib</code> compression is available. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1903383"><strong>BZ#1903383</strong></a>)</p>
</li>
<li>
<p>Previously, the <code>zipl</code> command configured the disk geometry by assuming a sector size of 512 bytes. As a result, on SCSI disks with 4k sectors, the <code>zipl</code> bootloader configuration contained incorrect offsets and zVM was unable to boot. With this fix, <code>zipl</code> now takes the disk sector size into account so that zVM boots successfully. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1918723"><strong>BZ#1918723</strong></a>)</p>
</li>
<li>
<p>Previously, <code>chrony.config</code> might automatically run multiple time and fail each time but the first. This caused issues because <code>chrony.config</code> configuration is set during the initial run and cannot be changed. These errors are now avoided by limiting the configuration setup process to the first time <code>chrony.config</code> runs. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1924869"><strong>BZ#1924869</strong></a>)</p>
</li>
<li>
<p>Previously, nodes appeared unhealthy and did not operate as expected during periods of high workloads. This resulted from workloads using memory faster than the memory could be reclaimed. With this update, memory reclamation and out-of-memory situations were addressed and these conditions no longer occur during high workload situations. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1931467"><strong>BZ#1931467</strong></a>)</p>
</li>
<li>
<p>Previously, the maximum transmission unit (MTU) specification for a bond interface using kernal arguments did not get assigned properly. This has been corrected. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1932502"><strong>BZ#1932502</strong></a>)</p>
</li>
<li>
<p>Previously, the <code>clevis-luks-askpass.path</code> unit was not enabled by default. This caused non-root <code>LUKS Clevis</code> devices to fail to unlock automatically on reboot. This update enables the <code>clevis-luks-askpass.path</code> unit by default and allows non-root <code>LUKS Clevis</code> devices to unlock automatically on reboot. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1947490"><strong>BZ#1947490</strong></a>)</p>
</li>
<li>
<p>Previously, systemd was excessively reading <code>mountinfo</code> and over-consuming CPU resources, which caused containers to fail to start. This update enables limits when <code>systemd</code> reads <code>mountinfo</code>, which allows containers to start successfully. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1957726"><strong>BZ#1957726</strong></a>)</p>
</li>
<li>
<p>Previously, when the Machine Config Operator (MCO) invoked Ignition at startup to check the Ignition version, Ignition would crash. Consequently, the  MCO would fail start. With this update, the MCO no longer queries the Ignition version, and the MCO starts successfully. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1927731"><strong>BZ#1927731</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Routing</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, the HAProxyDown alert message was vague. Consequently, end users thought the alert meant that router pods, instead of just HAProxy pods, were unavailable. This update makes the HAProxyDown alert message clearer. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1941592"><strong>BZ#1941592</strong></a>)</p>
</li>
<li>
<p>Previously, HAProxy&#8217;s helper function template that was responsible for generating a file for whitelist IPs expected a wrong argument type. Consequently, no whitelist ACL was applied for the backend in long IP lists. With this update, argument types of the helper function template are changed so that whitelist ACL is applied to the backend of long IP lists. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1964486"><strong>BZ#1964486</strong></a>)</p>
</li>
<li>
<p>Previously when creating an Ingress with a custom domain, the Ingress' status was updated by the OpenShift Container Platform Ingress controller with router canonical host name, and used <code>external-dns</code> to sync with Route 53. The problem was that the canonical router host name did not exist in the DNS and was not created by OpenShift Container Platform. OpenShift Container Platform creates the <code>*.apps.&lt;cluster_name&gt;.&lt;base_domain&gt;</code> DNS record and not the <code>apps.&lt;cluster_name&gt;.&lt;base_domain&gt;</code> DNS record. So the canonical router host name was not right. This fix sets the canonical router host name to <code>router-default.apps.&lt;cluster_name&gt;.&lt;base_domain&gt;</code>. Cluster administrators that have automation that takes the canonical host name and prepends a wildcard or a subdomain should be aware that the canonical Ingress host name is set as <code>&lt;ingress-controller-name&gt;.apps.&lt;cluster_name&gt;.&lt;base_domain&gt;</code>. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1901648"><strong>BZ#1901648</strong></a>)</p>
</li>
<li>
<p>Previously, the fix for <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1932401"><strong>BZ#1932401</strong></a> overrode the default Go HTTP client transport. Consequently, cluster-wide proxy settings were not plumbed through to the Ingress Operator pod, which resulted in the failure of canary checks on a cluster with a cluster-wide egress proxy. This update explicitly sets proxy settings in the canary client&#8217;s HTTP transport. As a result, canary checks work with all cluster-wide proxies. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1935528"><strong>BZ#1935528</strong></a>)</p>
</li>
<li>
<p>Previously, the canary DaemonSet did not specify a node selector, so it used the default node selector for the canary namespace. Consequently, the canary DaemonSet could not schedule to infra nodes and in some cases would throw alerts. This update explicitly schedules the canary DaemonSet to infra nodes and tolerates tainted infra nodes. This allows the canary DaemonSet to safely roll out to worker and infra nodes without issues or alerts. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1933102"><strong>BZ#1933102</strong></a>)</p>
</li>
<li>
<p>Previously, when upgrading a cluster from a prior version with an idled workload, the idled workload would not wake on HTTP request once upgraded to OpenShift Container Platform 4.6 or 4.7 due to <code>oc idle</code> feature fixups and reworks. With this update, idling changes are mirrored from endpoints to services on Ingress Operator startup. As a result, unidling workloads after upgrades works as expected. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1925245"><strong>BZ#1925245</strong></a>)</p>
</li>
<li>
<p>Previously, exposing the default Ingress Controller through an external load balancer that redirected all HTTP traffic to HTTPS caused Ingress Canary endpoint checks performed by the Ingress Operator to fail, which would ultimately cause the Ingress Operator to become <code>degraded</code>. This fix converts the cleartext canary route to an edge encrypted route. Now the canary route works though HTTPS only load balancers when insecure traffic is redirected by the load balancer. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1932401"><strong>BZ#1932401</strong></a>)</p>
</li>
<li>
<p>Previously, the Ingress Operator Canary Check Client sent canary requests over HTTP to load balancers that dropped HTTP traffic. This caused the Ingress Operator to become become <code>degraded</code> after canary checks failed. With this fix, instead of relying on a redirect from the router, the Ingress Operator Canary Check Client sends canary check requests over HTTPS from the start. Now, canary checks work for clusters that expose the default Ingress Controller through a load balancer that drops insecure HTTP traffic. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1934773"><strong>BZ#1934773</strong></a>)</p>
</li>
<li>
<p>Previously, the HAProxy template used by <code>openshift-router</code> made repeated calls to a <code>firstMatch()</code> function. That function would parse and recompile a regular expression every time. Parsing and recompiling the regular expression on each call to <code>firstMatch()</code> is expensive, particularly for configurations that have many thousands of routes. With this fix, if the regular expression in the call to <code>firstMatch()</code> has already been seen, then an already compiled version is reused and cached. Now, there is a 60 percent reduction in run time when parsing and evaluating the <code>haproxy-config.template</code>. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1937972"><strong>BZ#1937972</strong></a>)</p>
</li>
<li>
<p>Previously, users could name a route with an invalid host name by using an override annotation. This update fixes the issue. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1925697"><strong>BZ#1925697</strong></a>)</p>
</li>
<li>
<p>Previously, removing <code>selector</code> from a service exposed via a route resulted in the duplication of <code>endpointslices</code> that would be created for the services pods, which would trigger HAProxy reload errors due to duplicate server entries. This update filters out accidental duplicate server lines when writing out the HAProxy config file, so that deleting the selector from a service no longer causes the router to fail. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1961559"><strong>BZ#1961550</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Samples</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, the Cluster Samples Operator could make changes to the controller cache for objects it was watching, which caused errors when Kubernetes managed the controller cache. This update fixes how the Cluster Samples Operator uses information in the controller cache. As a result, the Cluster Samples Operator does not cause errors by modifying controller caches. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1949481"><strong>BZ#1949481</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>service-ca</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>OpenShift Container Platform 4.8 allows users to run <code>service-ca-operator</code> pods as a non-root user to suit their organization&#8217;s needs. When run as a non-root user, the <code>service-ca-operator</code> runs as the following UID and GID:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight nowrap"><code class="language-terminal hljs" data-lang="terminal">uid=1001(1001) gid=1001 groups=1001</code></pre>
</div>
</div>
<div class="paragraph">
<p>(<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1914446"><strong>BZ#1914446</strong></a>)</p>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Storage</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, metrics for <code>block type PVC</code> filesystems were not being reported when requesting a <code>capacity breakdown</code>. This meant users received an inaccurate reporting of metrics  across all their filesystems. With this update, <code>block type PVC</code> are included when requested by Kubelet. This provides an accurate reporting of all filesystem metrics. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1927359"><strong>BZ#1927359</strong></a>)</p>
</li>
<li>
<p>Previously, <code>/var/lib/kubelet</code> was mounted twice in the <code>Cinder CSI Node Controller</code> container. This caused the <code>CSI Node Controller</code> to fail to start with an error indicating <code>/var/lib/kubelet/pods</code> is out of space. The fix removes the duplicate mount of <code>/var/lib/kubelet</code> and <code>/var/lib/kubelet/pods</code>, which allows the <code>CSI Node Controller</code> to run successfully. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1952211"><strong>BZ#1952211</strong></a>)</p>
</li>
<li>
<p>Previously, during Cinder CSI Driver resizing of a persistent volume (PV), the <code>findmnt</code> command received multiple volume mounts and could not choose the correct one, thereby causing resizing to stop. As a result, users would have to extend the file system manually. With this fix, the command now uses the first mount so that the file system is resized along with the PV. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1919291"><strong>BZ#1919291</strong></a>)</p>
</li>
<li>
<p>The Cinder CSI Driver Operator now automatically provisions a default <code>VolumeSnapshotClass</code> object for Cinder CSI when creating a default storage class, rather than having to create the <code>VolumeSnapshotClass</code> object manually. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1905849"><strong>BZ#1905849</strong></a>)</p>
</li>
<li>
<p>Previously, the recycler-pod template was incorrectly placed in the kubelet static manifest directory. This incorrect location produced static pod log messages that indicated a recycler static pod start failure. With this update, the misplaced recycler-pod template has been removed from the static pod manifest directory. As a result, the error messages no longer appear. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1896226"><strong>BZ#1896226</strong></a>)</p>
</li>
<li>
<p>Previously, the Local Storage Operator (LSO) could claim disks belonging to other provisioners because busy disks were erroneously detected as free. Disks are now checked for bind-mounts so that the LSO cannot claim those disks. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1929175"><strong>BZ#1929175</strong></a>)</p>
</li>
<li>
<p>Previously, the Local Storage Operator (LSO) would attempt to create a persistent volume (PV) with an invalid label value, because the device-id contained unsupported characters such as <code>:</code>. This has been corrected by moving the device information from labels to annotations. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1933630"><strong>BZ#1933630</strong></a>)</p>
</li>
<li>
<p>Previously, the Local Storage Operator (LSO) was not cleaning up persistent volumes (PVs) since the deleter was not being enqueued correctly. This caused PVs to remain in the <code>released</code> state. PVs are now enqueued correctly so that they delete properly. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1937145"><strong>BZ#1937145</strong></a>)</p>
</li>
<li>
<p>Previously, the Fibre Channel volume was incorrectly unmounted from a node when a pod was deleted. This happened when a different pod that used the volume was deleted in the API server when the kubelet on the node was not running. With this update, Fibre Channel volumes correctly unmount when a new kubelet starts. Additionally, the volume cannot be mounted to multiple nodes until the new kubelet fully starts and cofnrims that the volume is unmounted, which ensures that Fibre Channel volumes are uncorrupted. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1954509"><strong>BZ#1954509</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Web console (Administrator perspective)</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, when attempting to delete a custom resource within the CNV namespace in the console UI in developer mode, clicking <strong>Delete</strong> in resulted in the <strong>Delete</strong> button hanging in a stuck state. Additionally, an error message that appears when performing the same action in the CLI was not displaying. With this update, the error message displays as expected and the <strong>Delete</strong> button does not stick. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1939753"><strong>BZ#1939753</strong></a>)</p>
</li>
<li>
<p>Previously, OperatorHub Provider Type <code>filter</code> property did not clearly show the relationship to <code>CatalogSource</code>. Because of this problem, users could not tell what the <code>filter</code> criteria meant. This patch updates the Provider Type <code>filter</code> to <code>Source</code>. This more clearly shows the relationship between <code>filter</code> and <code>CatalogSource</code>. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1919406"><strong>BZ#1919406</strong></a>)</p>
</li>
<li>
<p>Previously, the <strong>ResourceListDropdown</strong> component in the <strong>Resources</strong> menu was not internationalized for some languages. With this update, the <strong>Resources</strong> menu is updated to better the user experience for non-English speakers. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1921267"><strong>BZ#1921267</strong></a>)</p>
</li>
<li>
<p>Previously, some menu items, such as <strong>Delete Persistent Volume Claim</strong>, were not internationalized correctly. Now, more menu items are correctly internationalized. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1926126"><strong>BZ#1926126</strong></a>)</p>
</li>
<li>
<p>Previously, some text and warning messages for the <strong>Add HorizontalPodAutoscaler</strong> page were not internationalized. The text is now internationalized. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1926131"><strong>BZ#1926131</strong></a>)</p>
</li>
<li>
<p>Previously, when users created an Operator with the Operator SDK and specified an annotation like <code>xDescriptors={"urn:alm:&lt;&#8230;&#8203;&gt;:hidden"}</code> to hide a field from the Operator instance creation page, the field might still be visible on the page. Now, the hidden fields are omitted from the Operator instance creation page. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1966077"><strong>BZ#1966077</strong></a>)</p>
</li>
<li>
<p>Previously, tables displayed incorrectly on mobile devices. With this update, tables now display correctly. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1927013"><strong>BZ#1927013</strong></a>)</p>
</li>
<li>
<p>Previously, launching the OpenShift Container Platform web console may be slow. With this update, the web console launches quicker. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1927310"><strong>BZ#1927310</strong></a>)</p>
</li>
<li>
<p>Previously, a lack of internationalized notifications to OpenShift Container Platform administrators detracted from the user experience. Now, internationalization is possible. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1927898"><strong>BZ#1927898</strong></a>)</p>
</li>
<li>
<p>Previously, a lack of internationalized duration times on the <strong>Cluster Utilization</strong> dashboard detracted from the user experience. Now, internationalization is possible. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1927902"><strong>BZ#1927902</strong></a>)</p>
</li>
<li>
<p>Previously, errors occurred when Operator Lifecycle Manager (OLM) status descriptors in the OpenShift Container Platform web console were assigned incompatible data types. Validation has been added, eliminating incompatible data types from processing, thus avoiding errors. Logged warnings also identify the incompatible status types. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1927941"><strong>BZ#1927941</strong></a>)</p>
</li>
<li>
<p>The following OpenShift Container Platform web console views now support multi-faceted filtering:</p>
<div class="openblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p><strong>Home</strong> &#8594; <strong>Search</strong> (the <strong>Resources</strong> tab)</p>
</li>
<li>
<p><strong>Home</strong> &#8594; <strong>Events</strong> (the <strong>Resources</strong> tab)</p>
</li>
<li>
<p><strong>Workloads</strong> &#8594; <strong>Pods</strong> (the <strong>Filter</strong> tab)</p>
</li>
</ul>
</div>
</div>
</div>
<div class="paragraph">
<p>For more information, see <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1930007"><strong>BZ#1930007</strong></a>.</p>
</div>
</li>
<li>
<p>The following bug fixes address various translation issues for the OpenShift Container Platform web console:</p>
<div class="ulist">
<ul>
<li>
<p><a href="https://bugzilla.redhat.com/show_bug.cgi?id=1921780"><strong>BZ#1921780</strong></a></p>
</li>
<li>
<p><a href="https://bugzilla.redhat.com/show_bug.cgi?id=1921781"><strong>BZ#1921781</strong></a></p>
</li>
<li>
<p><a href="https://bugzilla.redhat.com/show_bug.cgi?id=1922992"><strong>BZ#1922992</strong></a></p>
</li>
<li>
<p><a href="https://bugzilla.redhat.com/show_bug.cgi?id=1924585"><strong>BZ#1924585</strong></a></p>
</li>
<li>
<p><a href="https://bugzilla.redhat.com/show_bug.cgi?id=1924747"><strong>BZ#1924747</strong></a></p>
</li>
<li>
<p><a href="https://bugzilla.redhat.com/show_bug.cgi?id=1925083"><strong>BZ#1925083</strong></a></p>
</li>
</ul>
</div>
</li>
<li>
<p>Previously, the web console relied on hard-coded channel strings to populate the channel modal dropdown. As a result, users could see channel values that may not be correct for their current version. Now, if the Cluster Version Operator does not supply the correct channels for a given version, the channel modal dropdown changes to a text input field and suggests channels and help text for the user. The console no longer relies on hard coded channel string. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1932281"><strong>BZ#1932281</strong></a>)</p>
</li>
<li>
<p>Previously, timestamps were not correctly formatted for Chinese or Japanese languages. As a result, timestamps were harder to read, which provided a bad user experience. With this update, default timestamp formats are used for Chinese and Japanese in <code>Moment.js</code>, which provides a better user experience. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1932453"><strong>BZ#1932453</strong></a>)</p>
</li>
<li>
<p>Previously, the <code>rowFilters</code> prop in the FilterToolbar component did not accept the <code>null</code> value. So if the <code>rowFilters</code> prop was undefined, the uncaught exception was thrown. Now, when the <code>rowFilters</code> prop is referenced in the FilterToolbar component, the <code>null</code> value is accepted. As a result, the FilterToolbar does not throw exceptions when <code>rowFilters</code> prop is undefined. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1937018"><strong>BZ#1937018</strong></a>)</p>
</li>
<li>
<p>Previously, the wrong style of help text was applied to the field level help instances. Now, the correct style of help text is shown for the field level help instances and is consistent across the console. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1942749"><strong>BZ#1942749</strong></a>).</p>
</li>
<li>
<p>Previously, the Operator Lifecycle Manamgent (OLM) status conditions descriptors were rendered as normal detail items on the resource details page. As a result, the <strong>Conditions</strong> table was rendered at half width. With this update, conditions descriptors are rendered as a full-width table below the normal <strong>Conditions</strong> table on the <strong>Operand</strong> details page. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1943238"><strong>BZ#1943238</strong></a>)</p>
</li>
<li>
<p>Previously, the word "Ingresses" was translated for Chinese users, but the user experience was bad. Now the word "Ingress" is not translated. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1945816"><strong>BZ#1945816</strong></a>)</p>
</li>
<li>
<p>Previously, the word "Operators" was translated for Chinese users, but the plural translation resulted in a bad user experience. Now the word "Operators" is not translated. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1945818"><strong>BZ#1945818</strong></a>)</p>
</li>
<li>
<p>Previously, an incorrect code was causing the <code>User</code> and <code>Group</code> details to show unrelated subjects. Now, a code has been added to filter by <code>User</code> or <code>Group</code>, so the <code>User</code> and <code>Group</code> details show related subjects. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1951212"><strong>BZ#1951212</strong></a>)</p>
</li>
<li>
<p>Previously, the pod Containers text was not internationalized, so there was a poor user experience. Now the pod Containers text has been internationalized, so the user experience is improved. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1937102"><strong>BZ#1937102</strong></a>)</p>
</li>
<li>
<p>Previously, the <code>PackageManifest</code> list page items did not link to the details page, so users could not easily drill down into individual <code>PackageManifest</code> items from the list page. Now, each <code>PackageManifest</code> item is linked to the details page that matches the convention of the other list pages. Users can easily access the PackageManifest details page from the list page. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1938321"><strong>BZ#1938321</strong></a>)</p>
</li>
<li>
<p>The <strong>Completions</strong> column of the <strong>Jobs</strong> table sorted by the number of <strong>Desired</strong> completions instead of the number of <strong>Succeeded</strong> completions. The data is presented as <strong># Succeeded of # Desired</strong>, so when sorting by that column the results looked confusing because the data was sorted by the second number. The <strong>Jobs</strong> <strong>Completions</strong> column now sorts on the <strong># Succeeded</strong> for better understanding. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1902003"><strong>BZ#1902003</strong></a>)</p>
</li>
<li>
<p>The input labels in the <strong>Manage Columns</strong> modal were not clickable buttons, so you could not click them to manage the columns. With this bug fix, the labels are now clickable buttons that you can use to manage columns. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1908343"><strong>BZ#1908343</strong></a>)</p>
</li>
<li>
<p>CSI provisioners were not listed when creating a storage class on the Google Cloud Platform. With this bug fix, the issue is resolved. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1910500"><strong>BZ#1910500</strong></a>)</p>
</li>
<li>
<p>Previously, if the user clicked into a <strong>Cluster Role</strong> from the <strong>User Management &#8594; Roles</strong> list view, the back link from the details page is <strong>Cluster Roles</strong>, which provides a generic list view of <strong>Cluster Roles</strong>. This caused backward web console navigation to redirect to the incorrect page. With this release, the back link directs the user to the <strong>Role/Bindings</strong> list view from the <strong>Cluster Role/RoleBinding</strong> details page. This allows the user to correctly navigate backward in the web console. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1915971"><strong>BZ#1915971</strong></a>)</p>
</li>
<li>
<p>Previously, <strong>Created date time</strong> was not displayed in a readable format, which made it difficult to understand and use the time shown in UTC. With this release, the displayed time is reformatted so that UTC is readable and understandable. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1917241"><strong>BZ#1917241</strong></a>)</p>
</li>
<li>
<p>Previously, pod requests and limit calculations in the web console were incorrect. This was a result of not excluding completed pods or init containers. With this release, pods that are not needed in the calculation are excluded, which improves the accuracy of the results of the web console calculation for pod requests. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1918785"><strong>BZ#1918785</strong></a>)</p>
</li>
<li>
<p>Previously, parsing undefined values resulted in a not a number (NaN) exception and the <strong>Chart</strong> tooltip showed a box with no values. With this release, a start date is specified when fetching data so that the <strong>Chart</strong> toolip shows correct values. This change ensures that the results are synced and that undefined values are not parsed. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1906304"><strong>BZ#1906304</strong></a>)</p>
</li>
<li>
<p>During a previous bug fix, the download link for pod logs was changed to a standard HTML anchor element with an empty download attribute. Consequently, the download file lost the default file name format. This update adds a file name to the anchor element download attribute so that a default file name, formatted as <code>&lt;pod-name&gt;-&lt;container-name&gt;.log</code>, is used when downloading pod logs. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1945630"><strong>BZ#1945630</strong></a>)</p>
</li>
<li>
<p>Previously, when a user had permission to create a resource but not permission to edit it, the web console YAML editor was incorrectly set to read-only mode. The editor content is now editable by users with create access for the resource. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1824911"><strong>BZ#1824911</strong></a>)</p>
</li>
<li>
<p>Previously, the web console showed times in the 12-hour format in most places, and the 24-hour format in others. Additionally, the year was not displayed for dates more than one year in the past. With this release, dates and times are formatted consistently and match the user locale and language preference settings, and the year is displayed for dates more than one year in the past. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1862084"><strong>BZ#1862084</strong></a>)</p>
</li>
<li>
<p>Previously, the web console was polling the <code>ClusterVersion</code> resource for users who didn&#8217;t have the authority to view those events. This would output large numbers of errors in the console pod log. To avoid this, checking the user&#8217;s persmissions before polling the resource is required, which eliminates unnecessary errors in the console pod log. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1848151"><strong>BZ#1848151</strong></a>)</p>
</li>
<li>
<p>Previously, keyboard users of the YAML editor were unable to exit the editor. The <code>view shortcuts</code> popover outside of the editor was unavailable inside the editor for access by the user. With this update, users can display <code>Accessibility help</code> above the editor using the <code>opt + F1</code> keystrokes. This change allows keyboard users of the YAML editor to exit the editor using the correct keystrokes. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1874931"><strong>BZ#1874931</strong></a>)</p>
</li>
<li>
<p>After the 4.x release of OpenShift Container Platform (OCP), binary secret files uploaded to the OCP 4 web console failed to load. This caused the installation to fail. With OpenShift Container Platform 4.8, this capability has been restored to the OCP 4 web console. Input of the required secret can now be accomplished using binary file format. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1879638"><strong>BZ#1879638</strong></a>)</p>
</li>
<li>
<p>Previously, the fix for <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1871996"><strong>BZ#1871996</strong></a> to properly create RoleBinding links consistently resulted in the inability to select the binding type when a namespace was selected. Consequently, users with an active namespace could not create a cluster RoleBinding without changing the active namespace to <code>All namespaces</code>. This update reverts part of the changes for BZ#1871996 so that users can create a cluster role binding regardless of active namespace. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1927882"><strong>BZ#1927882</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Web console (Developer perspective)</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, when the label changed for making a service cluster local on the Developer Console, users were not able to create a Knative service. This update to the Knative service uses the latest supported label for <code>cluster-local</code> in order to enable users to create a Knative service as cluster-local from Developer Console.
(<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1969951"><strong>1969951</strong></a>)</p>
</li>
<li>
<p>Previously, the colors for the <strong>Low</strong> and <strong>Medium</strong> severity issues of the Image Manifest Vulnerabilities (IMVs) did not match the color representation shown in the (<a href="https://quay.io/">Quay.io</a>) interface. As a result, when the user changed the severity order of vulnerabilities to <strong>High</strong>, the IMVs ordered the issues incorrectly. This created confusion when reviewing the IMVs. The current release fixes this issue. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1942716"><strong>BZ#1942716</strong></a>)</p>
</li>
<li>
<p>Previously, the <strong>Topology</strong> view in the Developer perspective did not load if OpenShift namespace templates were not available because the Samples Operator was not installed. This update fixes the issue. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1949810"><strong>BZ#1949810</strong></a>)</p>
</li>
<li>
<p>Previously, when you imported a devfile, the web console ignored the <code>build guidance</code> placeholder container which provided the configuration for environment variables, ports, and limits. The new deployment had a second container which could not start because the placeholder image could not be fetched and it missed the required configurations. Now, the <code>build guidance</code> container is dropped from the new deployment, and the container adds the environment variables, ports, and limit configurations. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1952214"><strong>BZ#1952214</strong></a>)</p>
</li>
<li>
<p>Previously, when switching to the <strong>Developer</strong> perspective in another tab and reloading the project details, the routes tied to the perspective were not rendered and resulted in a <code>404</code> error. This update loads all inactive routes and switches to the correct perspective. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1929769"><strong>BZ#1929769</strong></a>)</p>
</li>
<li>
<p>Previously, when an error occurred due to a user not having the required access for a namespace, the <strong>Workload</strong> drop-down menu in the <strong>Monitoring</strong> dashboard page continuously displayed a loading-in-progress icon. The current release fixes this issue. Now, the <strong>Monitoring</strong> dashboard page displays an error message indicating that a <strong>Forbidden</strong> error has occurred. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1930546"><strong>BZ#1930546</strong></a>)</p>
</li>
<li>
<p>Previously, an API server could fail to create a resource, which would return a 409 status code when there was a conflict updating a <code>resource quota</code> resource. Consequently, the resource would fail to create, and you might have had to retry the API request. With this update, the <code>OpenShift Console</code> web application attempts to retry the request 3 times when receiving a 409 status code, which is often sufficient for completing the request. In the event that a 409 status code continues to occur, an error will be displayed in the console. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1920699"><strong>BZ#1920699</strong></a>)</p>
</li>
<li>
<p>Previously, when selecting the <strong>YAML</strong> tab, the <code>metadata.managedFields</code> section did not collapse immediately. This was due to an issue with the <strong>Form</strong> or <strong>YAML</strong> switcher for pages such as <strong>Pipeline Builder</strong> and <strong>Edit HorizontalPodAutoscaler</strong> (HPA). As a result, the part of the document where you tried to type collapsed. The <code>metadata.managedFields</code> section remained as is, and the cursor was reset to the starting position to the top left of the <strong>YAML</strong> editor. The current release fixes this issue. Now, upon loading the <strong>YAML</strong>, the <code>metadata.managedFields</code> section collapses immediately. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1932472"><strong>BZ#1932472</strong></a>)</p>
</li>
<li>
<p>Previously, pipelines created in the <strong>Git Import</strong> flow for private repositories failed to run. This happened because the pipeline <code>ServiceAccount</code> object did not use secrets created by the <strong>Git Import</strong> flow for private Git repositories. With this update, you can add a secret name to the annotations of the pipeline <code>ServiceAccount</code> object, and add pipeline-specific annotations to the provided secret. As a result, pipelines for private Git repositories run successfully. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1970470"><strong>BZ#1970470</strong></a></p>
</li>
<li>
<p>Previously, when users inserted a formatted <strong>YAML</strong> snippet in the <strong>YAML</strong> editor, the new selection did not match the new content in the snippet. The indentation was removed, and some random letters were seen in the selection. The current release fixes this issue. Now, the cursor remains in the position where it started and adds the missing indentation for the cursor end position. After inserting the <strong>YAML</strong> snippet, the new selection matches the new content. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1952545"><strong>BZ#1952545</strong></a>)</p>
</li>
<li>
<p>Previously, annotations were passed to the specification of the Knative service as well as to the metadata. As a result, decorators were shown for associated revisions of Knative service in <strong>Topology</strong>. This release fixes this issue by passing annotations only to the Knative service metadata. Now, decorators are shown only for the Knative service in <strong>Topology</strong> and not associated revisions. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1954959"><strong>BZ#1954959</strong></a>)</p>
</li>
<li>
<p>Previously, if you created a pipeline with parameters that had empty strings, for example, <code></code>, the fields in the OpenShift Container Platform web console would not accept the empty strings. The current release fixes this issue. Now, <code></code> is supported as a valid default property within the parameters section. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1951043"><strong>BZ#1951043</strong></a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Windows Containers</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, the load balancer service became unstable when users would scale additional Windows nodes. With this update, the load balancer service is stabilized, which allows users to add multiple Windows nodes without erratic performance. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1905950"><strong>BZ#1905950</strong></a>)</p>
</li>
</ul>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, the <code>kube-proxy</code> service crashed unexpectedly after the load balancer was created if it was created after the Windows pods were running. With this update, the kube-proxy service does not crash when recreating the load balancer service. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1939968"><strong>BZ#1939968</strong></a>)</p>
</li>
<li>
<p>Previously, empty IP address values in the load balancer&#8217;s Ingress broke the data path. As a result, the Windows service was unreachable. With this update, the Windows service is reachable even if the IP address value is empty. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1952914)"><strong>BZ#1952914</strong></a>)</p>
</li>
</ul>
</div>
<div class="ulist">
<ul>
<li>
<p>Previously, when users created a Windows pod with a projected volume, the pod would remain stuck in the <code>ContainerCreating</code> phase. With this update, the Windows pod creation successfully proceeds to the <code>Running</code> phase. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1973580"><strong>BZ#1973580</strong></a>)</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="ocp-4-8-technology-preview"><a class="anchor" href="#ocp-4-8-technology-preview"></a>Technology Preview features</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red Hat Customer Portal for these features:</p>
</div>
<div class="paragraph">
<p><a href="https://access.redhat.com/support/offerings/techpreview">Technology Preview Features Support Scope</a></p>
</div>
<div class="paragraph">
<p>In the table below, features are marked with the following statuses:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>TP</strong>: <em>Technology Preview</em></p>
</li>
<li>
<p><strong>GA</strong>: <em>General Availability</em></p>
</li>
<li>
<p><strong>-</strong>: <em>Not Available</em></p>
</li>
<li>
<p><strong>DEP</strong>: <em>Deprecated</em></p>
</li>
</ul>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 2. Technology Preview tracker</caption>
<colgroup>
<col style="width: 57.1428%;">
<col style="width: 14.2857%;">
<col style="width: 14.2857%;">
<col style="width: 14.2858%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Feature</th>
<th class="tableblock halign-left valign-top">OCP 4.6</th>
<th class="tableblock halign-left valign-top">OCP 4.7</th>
<th class="tableblock halign-left valign-top">OCP 4.8</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Precision Time Protocol (PTP)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>oc</code> CLI plug-ins</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Descheduler</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">OVN-Kubernetes Pod network provider</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">HPA for memory utilization</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Service Binding</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Log forwarding</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Monitoring for user-defined projects</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Raw Block with Cinder</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">CSI volume snapshots</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">CSI volume cloning</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">CSI volume expansion</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">vSphere Problem Detector Operator</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">-</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">CSI Azure Disk Driver Operator</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">-</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">-</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">CSI GCP PD Driver Operator</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">-</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">CSI OpenStack Cinder Driver Operator</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">-</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">CSI AWS EBS Driver Operator</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">CSI automatic migration</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">-</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">-</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Red Hat Virtualization (oVirt) CSI Driver Operator</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">CSI inline ephemeral volumes</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">CSI vSphere Driver Operator</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">-</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">-</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Automatic device discovery and provisioning with Local Storage Operator</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">OpenShift Pipelines</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">OpenShift GitOps</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">OpenShift sandboxed containers</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">-</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">-</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Vertical Pod Autoscaler</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Cron jobs</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">PodDisruptionBudget</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Operator API</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Adding kernel modules to nodes with kvc</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Egress router CNI plug-in</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">-</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Scheduler profiles</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">-</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Non-preempting priority classes</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">-</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Kubernetes NMState Operator</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">-</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Assisted Installer</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">-</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">AWS Security Token Service (STS)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">-</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Kdump</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">-</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">OpenShift Serverless</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">-</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">-</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GA</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Serverless functions</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">-</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">-</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Jenkins Operator</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TP</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DEP</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="sect1">
<h2 id="ocp-4-8-known-issues"><a class="anchor" href="#ocp-4-8-known-issues"></a>Known issues</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>In OpenShift Container Platform 4.1, anonymous users could access discovery endpoints. Later releases revoked this access to reduce the possible attack surface for security exploits because some discovery endpoints are forwarded to aggregated API servers. However, unauthenticated access is preserved in upgraded clusters so that existing use cases are not broken.</p>
<div class="paragraph">
<p>If you are a cluster administrator for a cluster that has been upgraded from OpenShift Container Platform 4.1 to 4.8, you can either revoke or continue to allow unauthenticated access. It is recommended to revoke unauthenticated access unless there is a specific need for it. If you do continue to allow unauthenticated access, be aware of the increased risks.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If you have applications that rely on unauthenticated access, they might receive HTTP <code>403</code> errors if you revoke unauthenticated access.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Use the following script to revoke unauthenticated access to discovery endpoints:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight nowrap"><code class="language-bash hljs" data-lang="bash">## Snippet to remove unauthenticated group from all the cluster role bindings
$ for clusterrolebinding in cluster-status-binding discovery system:basic-user system:discovery system:openshift:discovery ;
do
### Find the index of unauthenticated group in list of subjects
index=$(oc get clusterrolebinding ${clusterrolebinding} -o json | jq 'select(.subjects!=null) | .subjects | map(.name=="system:unauthenticated") | index(true)');
### Remove the element at index from subjects array
oc patch clusterrolebinding ${clusterrolebinding} --type=json --patch "[{'op': 'remove','path': '/subjects/$index'}]";
done</code></pre>
</div>
</div>
<div class="paragraph">
<p>This script removes unauthenticated subjects from the following cluster role bindings:</p>
</div>
<div class="openblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p><code>cluster-status-binding</code></p>
</li>
<li>
<p><code>discovery</code></p>
</li>
<li>
<p><code>system:basic-user</code></p>
</li>
<li>
<p><code>system:discovery</code></p>
</li>
<li>
<p><code>system:openshift:discovery</code></p>
</li>
</ul>
</div>
</div>
</div>
<div class="paragraph">
<p>(<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1821771"><strong>BZ#1821771</strong></a>)</p>
</div>
</li>
</ul>
</div>
<div class="ulist">
<ul>
<li>
<p>The <code>oc annotate</code> command does not work for LDAP group names that contain an equal sign (<code>=</code>), because the command uses the equal sign as a delimiter between the annotation name and value. As a workaround, use <code>oc patch</code> or <code>oc edit</code> to add the annotation. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1917280"><strong>BZ#1917280</strong></a>)</p>
</li>
<li>
<p>When powering on a virtual machine on vSphere with user-provisioned infrastructure, the process of scaling up a node might not work as expected. A known issue in the hypervisor configuration causes machines to be created within the hypervisor but not powered on. If a node appears to be stuck in the <code>Provisioning</code> state after scaling up a machine set, you can investigate the status of the virtual machine in the vSphere instance itself. Use the VMware commands <code>govc tasks</code> and <code>govc events</code> to determine the status of the virtual machine. Check for a similar error message to the following:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight nowrap"><code class="language-terminal hljs" data-lang="terminal">Invalid memory setting: memory reservation (sched.mem.min) should be equal to memsize(8192).</code></pre>
</div>
</div>
<div class="paragraph">
<p>You can attempt to resolve the issue with the steps in this <a href="https://kb.vmware.com/s/article/2002779">VMware KBase article</a>. For more information, see the Red Hat Knowledgebase solution <a href="https://access.redhat.com/solutions/5785341">UPI vSphere Node scale-up doesn&#8217;t work as expected</a>. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1918383"><strong>BZ#1918383</strong></a>)</p>
</div>
</li>
<li>
<p>The installation of RHCOS on a RHEL KVM installation on IBM Z fails when using an ECKD type DASD as a VirtIO block device. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1960485"><strong>BZ#1960485</strong></a>)</p>
</li>
<li>
<p>An Open Virtual Network (OVN) bug causes persistent connectivity issues with Octavia load balancers. When Octavia load balancers are created, OVN might not plug them into some Neutron subnets. These load balancers might be unreachable for some of the Neutron subnets. This problem affects Neutron subnets, which are created for each OpenShift namespace, at random when Kuryr is configured. As a result, when this problem occurs the load balancer that implements OpenShift <code>Service</code> objects will be unreachable from OpenShift namespaces affected by the issue. Because of this bug, OpenShift Container Platform 4.8 deployments that use Kuryr SDN are not recommended on Red Hat OpenStack Platform (RHOSP) 16.1 with OVN and OVN Octavia configured until the bug is fixed. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1937392"><strong>BZ#1937392</strong></a>)</p>
</li>
<li>
<p>The Console Operator does not properly update the <code>Ingress</code> resource with the <code>componentRoutes</code> conditions for either of the console&#8217;s routes (<code>console</code> or <code>downloads</code>). (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1954148"><strong>BZ#1954148</strong></a>)</p>
</li>
<li>
<p>If you are using OpenShift sandboxed containers, you cannot use the <code>hostPath</code> volume in a OpenShift Container Platform cluster to mount a file or directory from the host nodes file system into your pod. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1904609"><strong>BZ#1904609</strong></a>)</p>
</li>
<li>
<p>If you are running Fedora on OpenShift sandboxed containers, you need a workaround to install some packages. Some packages, like <code>iputils</code>, require file access permission changes that OpenShift Container Platform does not grant to containers by default. To run containers that require such special permissions, it is necessary to add an annotation to the YAML file describing the workload, which tells <code>virtiofsd</code> to accept such file permissions for that workload. The required annotations are:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight nowrap"><code class="language-yaml hljs" data-lang="yaml">io.katacontainers.config.hypervisor.virtio_fs_extra_args: [ "-o", "modcaps=+sys_admin", "-o", "xattr" ]</code></pre>
</div>
</div>
<div class="paragraph">
<p>(<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1915377"><strong>BZ#1915377</strong></a>)</p>
</div>
</li>
<li>
<p>In the 4.8 release, adding a value to <code>kataConfgPoolSelector</code> by using the OpenShift Container Platform web console causes <code>scheduling.nodeSelector</code> to be populated with an empty value. Pods that use <code>RuntimeClass</code> with the value of <code>kata</code> might be scheduled to nodes that do not have the Kata Containers runtime installed.</p>
<div class="paragraph">
<p>To work around this issue, specify the<code>nodeSelector</code>value manually in the<code>RuntimeClass</code> <code>kata</code> by running the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight nowrap"><code class="language-terminal hljs" data-lang="terminal">$ oc edit runtimeclass kata</code></pre>
</div>
</div>
<div class="paragraph">
<p>The following is an example of a <code>RuntimeClass</code> with the correct <code>nodeSelector</code> statement.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight nowrap"><code class="language-yaml hljs" data-lang="yaml">apiVersion: node.k8s.io/v1
handler: kata
kind: RuntimeClass
metadata:
  creationTimestamp: "2021-06-14T12:54:19Z"
  name: kata
overhead:
  podFixed:
    cpu: 250m
    memory: 350Mi
scheduling:
  nodeSelector:
    custom-kata-pool: "true"</code></pre>
</div>
</div>
<div class="paragraph">
<p>(<a href="https://issues.redhat.com/browse/KATA-764"><strong>KATA-764</strong></a>)</p>
</div>
</li>
<li>
<p>The OpenShift sandboxed containers Operator details page on Operator Hub contains a few missing fields. The missing fields do not prevent you from installing the OpenShift sandboxed containers Operator in 4.8.</p>
<div class="paragraph">
<p>(<a href="https://issues.redhat.com/browse/KATA-826"><strong>KATA-826</strong></a>)</p>
</div>
</li>
<li>
<p>Creating multiple <code>KataConfig</code> custom resources results in a silent failure. The OpenShift Container Platform web console does not provide a prompt to notify the user that creating more than one custom resource has failed.</p>
<div class="paragraph">
<p>(<a href="https://issues.redhat.com/browse/KATA-725"><strong>KATA-725</strong></a>)</p>
</div>
</li>
<li>
<p>Sometimes the Operator Hub in the OpenShift Container Platform web console does not display icons for an Operator.</p>
<div class="paragraph">
<p>(<a href="https://issues.redhat.com/browse/KATA-804"><strong>KATA-804</strong></a>)</p>
</div>
</li>
<li>
<p>The OVN-Kubernetes network provider does not support the <code>externalTrafficPolicy</code> feature for <code>NodePort</code>- and <code>LoadBalancer</code>-type services.  The <code>service.spec.externalTrafficPolicy</code> field determines whether traffic for a service is routed to node-local or cluster-wide endpoints. Currently, such traffic is routed by default to cluster-wide endpoints, and there is no way to limit traffic to node-local endpoints. This will be resolved in a future release. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1903408"><strong>BZ#1903408</strong></a>)</p>
</li>
<li>
<p>Currently, a Kubernetes port collision issue can cause a breakdown in pod-to-pod communication, even after pods are redeployed. For detailed information and a workaround, see the Red Hat Knowledge Base solution <a href="https://access.redhat.com/solutions/5940711">Port collisions between pod and cluster IPs on OpenShift 4 with OVN-Kubernetes</a>. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1939676"><strong>BZ#1939676</strong></a>, <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1939045"><strong>BZ#1939045</strong></a>)</p>
</li>
<li>
<p>For clusters that use the OVN-Kubernetes network provider and whose compute nodes run RHEL 7.9, upgrading from OpenShift Container Platform 4.7 to OpenShift Container Platform 4.8 is blocked by <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1976232"><strong>BZ#1976232</strong></a>. To upgrade to release 4.8, you must wait for the 4.8 patch that includes the fix for this bug. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1976232"><strong>BZ#1976232</strong></a>)</p>
</li>
<li>
<p>For clusters that use the OVN-Kubernetes network provider and upgrade from OpenShift Container Platform 4.7 to OpenShift Container Platform 4.8, a bug in OVN-Kubernetes can sometimes cause the pod IP address to become stale. The bug is a rarely experienced race conditition. As a consequence, during the upgrade to the 4.8 release, nodes fail to drain and some Operators report a status of <code>Degraded</code>. As a workaround, identify the pods that are stuck in the <code>CrashLoopBackOff</code> state and that did not complete the upgrade. Delete each pod with the <code>oc delete &lt;pod-name&gt;</code> command. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1974403"><strong>BZ#1974403</strong></a>)</p>
</li>
<li>
<p>The description for the <code>tlsSecurityProfile</code> field of the <code>kubeletconfig</code> resource (for example when using the <code>oc explain</code> command) does not list the correct ciphers for the TLS security profiles. As a workaround, review the list of ciphers in the <code>/etc/kubernetes/kubelet.conf</code> file of an affected node. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1971899"><strong>BZ#1971899</strong></a>)</p>
</li>
<li>
<p>When running CNF tests in regular mode on a single node, the logic in place to understand if the cluster is ready is missing details. In particular, creating an SR-IOV network will not create a network attachment definition until at least one minute elapses. All the DPDK tests fail in cascade. Run the CNF tests in regular mode skipping the DPDK feature when running against an installation on a single node, with the <code>-ginkgo.skip</code> parameter. Run CNF tests in Discovery mode to execute tests against an installation on a single node. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1970409"><strong>BZ#1970409</strong></a>)</p>
</li>
<li>
<p>Currently, CNF-tests does not support secure boot with MLX NICs for SR-IOV and DPDK tests. You can run the CNF tests skipping the SR-IOV feature when running against a secure boot enabled environment in regular mode, with the <code>-ginkgo.skip</code> parameter. Running in Discovery mode is the recommended way to execute tests against a secure boot enabled environment with MLX cards. This will be resolved in a future release. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1975708"><strong>BZ#1975708</strong></a>)</p>
</li>
<li>
<p>When the <code>ArgoCD</code> Operator is subscribed to and an ArgoCD and AppProject are started, launching the example application named <code>guestbook</code> fails because the image does not work in more restrictive OpenShift Container Platform environments. As a temporary workaround, users can ensure the <code>ArgoCD</code> Operator works properly by deploying the following example:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight nowrap"><code class="language-yaml hljs" data-lang="yaml">PROJ=younamespace
cat &gt; $PROJ-app.yaml &lt;&lt;EOF
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: simple-restricted-webserver
  namespace: $PROJ
spec:
  destination:
    namespace: $PROJ
    server: https://kubernetes.default.svc
  project: default
  source:
    path: basic-nginx
    repoURL: 'https://github.com/opdev/argocd-example-restricted-apps.git'
    targetRevision: HEAD
EOF
oc create -f $PROJ-app.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>For more information, see <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1812212"><strong>BZ#1812212</strong></a>.</p>
</div>
</li>
<li>
<p>If you have the console open in multiple tabs, some sidebar links in the <strong>Developer</strong> perspective do not directly link to the project, and there is an unexpected change in the selected project. This will be resolved in a future release. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1839101"><strong>BZ#1839101</strong></a>)</p>
</li>
<li>
<p>Creating a passthrough route using Ingress fails when using <code>pathType: Prefix</code>. Instead, you can create a passthrough route by setting <code>pathType</code> to <code>ImplementationSpecific</code> and setting <code>path</code> to <code>''</code>:</p>
<div class="listingblock">
<div class="title">Sample Ingress YAML file</div>
<div class="content">
<pre class="highlightjs highlight nowrap"><code class="language-YAML hljs" data-lang="YAML">apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress7
  namespace: test-ingress
  annotations:
    route.openshift.io/termination: passthrough
spec:
  rules:
  - host: &lt;ingress-psql-example-test-ingress.apps&gt;
    http:
      paths:
      - path: ''
        pathType: ImplementationSpecific
        backend:
          service:
            name: &lt;ingress-psql-example&gt;
            port:
              number: 8080</code></pre>
</div>
</div>
<div class="paragraph">
<p>For more information, see <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1878685"><strong>BZ#1878685</strong></a>.</p>
</div>
</li>
<li>
<p>Currently, in the <strong>Search</strong> page, the <strong>Pipelines</strong> resources table is not immediately updated after you apply or remove the <strong>Name</strong> filter. However, if you refresh the page and expand the <strong>Pipelines</strong> section, the <strong>Name</strong> filter is applied. The same behavior is seen when you remove the <strong>Name</strong> filter. This will be resolved in a future release. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1901207"><strong>BZ#1901207</strong></a>).</p>
</li>
<li>
<p>Documentation now describes that the <code>ProvisioningNetworkCIDR</code> value in the <code>Provisioning</code> custom resource. This limits the IPv6 provisioning networks to a limit of /64 due to <code>dnsmasq</code>. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1947293)"><strong>BZ#1947293</strong></a></p>
</li>
<li>
<p>To assist with troubleshooting, logs collected on boostratp failures by the installer now include IP addresses and routes for the control plane and bootstrap hosts. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1956079)"><strong>BZ#1956079</strong></a>)</p>
</li>
<li>
<p>When using a self-signed Amazon Commercial Cloud Services cluster, you cannot pull from or push to an internal image registry. As a workaround, you must set <code>spec.disableRedirect</code> to <code>true</code> in the <code>configs.imageregistry/cluster</code> resource. This lets you pull the image layers from the image registry rather than directly from S3 storage. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1924568"><strong>BZ#1924568</strong></a>)</p>
</li>
<li>
<p>Previously, the topology URLs created for deployments using Bitbucket repository in the OpenShift Container Platform web console did not work if they included a branch name that contained a slash character. This was due to an issue with the Bitbucket API (<a href="https://jira.atlassian.com/browse/BCLOUD-9969">BCLOUD-9969</a>). The current release mitigates this issue. If a branch name contains a slash, the topology URLs point to the default branch page for the repository. This issue will be fixed in a future release of OpenShift Container Platform. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1969535"><strong>BZ#1969535</strong></a>).</p>
</li>
<li>
<p>Installing OpenShift Container Platform (OCP) version 4.6 on Red Hat Virtualization (RHV) requires RHV version 4.4. If you are running an earlier version of OCP on RHV 4.3, do not update it to OCP version 4.6. Red Hat has not tested running OCP version 4.6 on RHV version 4.3 and does not support this combination. For additional information about tested integrations, see <a href="https://access.redhat.com/articles/4763741">OpenShift Container Platform 4.x Tested Integrations (for x86_x64)</a>.</p>
</li>
<li>
<p>The <code>operator-sdk pkgman-to-bundle</code> command exits with an error when run with the <code>--build-cmd</code> flag. For more information, see (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1967369"><strong>BZ#1967369</strong></a>).</p>
</li>
<li>
<p>Currently, the prerequisites in the web console quick start cards appear as a paragraph instead of a list. This will be resolved in a future release. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1905147"><strong>BZ#1905147</strong></a>)</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="ocp-4-8-asynchronous-errata-updates"><a class="anchor" href="#ocp-4-8-asynchronous-errata-updates"></a>Asynchronous errata updates</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Security, bug fix, and enhancement updates for OpenShift Container Platform 4.8 are released as asynchronous errata through the Red Hat Network. All OpenShift Container Platform 4.8 errata is <a href="https://access.redhat.com/downloads/content/290/">available on the Red Hat Customer Portal</a>. See the <a href="https://access.redhat.com/support/policy/updates/openshift">OpenShift Container Platform Life Cycle</a> for more information about asynchronous errata.</p>
</div>
<div class="paragraph">
<p>Red Hat Customer Portal users can enable errata notifications in the account settings for Red Hat Subscription Management (RHSM). When errata notifications are enabled, users are notified via email whenever new errata relevant to their registered systems are released.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Red Hat Customer Portal user accounts must have systems registered and consuming OpenShift Container Platform entitlements for OpenShift Container Platform errata notification emails to generate.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>This section will continue to be updated over time to provide notes on enhancements and bug fixes for future asynchronous errata releases of OpenShift Container Platform 4.8. Versioned asynchronous releases, for example with the form OpenShift Container Platform 4.8.z, will be detailed in subsections. In addition, releases in which the errata text cannot fit in the space provided by the advisory will be detailed in subsections that follow.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>For any OpenShift Container Platform release, always review the instructions on <a href="#../updating/updating-cluster.adoc#updating-cluster" class="page unresolved">updating your cluster</a> properly.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="ocp-4-8-1-ga"><a class="anchor" href="#ocp-4-8-1-ga"></a>RHSA-2021:2438 - OpenShift Container Platform 4.8.2 image release, bug fix, and security update advisory</h3>
<div class="paragraph">
<p>Issued: 2021-07-27</p>
</div>
<div class="paragraph">
<p>OpenShift Container Platform release 4.8.2, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the <a href="https://access.redhat.com/errata/RHSA-2021:2438">RHSA-2021:2438</a> advisory. The RPM packages that are included in the update are provided by the <a href="https://access.redhat.com/errata/RHSA-2021:2437">RHSA-2021:2437</a> advisory.</p>
</div>
<div class="paragraph">
<p>Space precluded documenting all of the container images for this release in the advisory. See the following article for notes on the container images in this release:</p>
</div>
<div class="paragraph">
<p><a href="https://access.redhat.com/solutions/6164202">OpenShift Container Platform 4.8.2 container image list</a></p>
</div>
</div>
<div class="sect2">
<h3 id="ocp-4-8-3"><a class="anchor" href="#ocp-4-8-3"></a>RHBA-2021:2896 - OpenShift Container Platform 4.8.3 bug fix update</h3>
<div class="paragraph">
<p>Issued: 2021-08-02</p>
</div>
<div class="paragraph">
<p>OpenShift Container Platform release 4.8.3 is now available. The list of bug fixes that are included in the update is documented in the <a href="https://access.redhat.com/errata/RHBA-2021:2157">RHBA-2021:2896</a> advisory. The RPM packages that are included in the update are provided by the <a href="https://access.redhat.com/errata/RHBA-2021:2158">RHBA-2021:2899</a> advisory.</p>
</div>
<div class="paragraph">
<p>Space precluded documenting all of the container images for this release in the advisory. See the following article for notes on the container images in this release:</p>
</div>
<div class="paragraph">
<p><a href="https://access.redhat.com/solutions/6221811">OpenShift Container Platform 4.8.3 container image list</a></p>
</div>
<div class="sect3">
<h4 id="ocp-4-8-3-bug-fixes"><a class="anchor" href="#ocp-4-8-3-bug-fixes"></a>Bug fixes</h4>
<div class="ulist">
<ul>
<li>
<p>Previously, a bug in the lock implementation for the <code>nmstate-handler</code> pod caused multiple nodes to gain control. This update fixes the lock implementation so that only one node is in control of the lock. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1954309"><strong>BZ#1954309</strong></a>)</p>
</li>
<li>
<p>Previously, there was an incorrect toleration setting on the <code>nmstate-handler</code> pod, which made network configuration on nodes with the <code>nmstate</code> Operator impossible. With this update, the handler pod allows toleration on all nodes. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1960446"><strong>BZ#1960446</strong></a>)</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="ocp-4-8-3-upgrading"><a class="anchor" href="#ocp-4-8-3-upgrading"></a>Upgrading</h4>
<div class="paragraph">
<p>To upgrade an existing OpenShift Container Platform 4.8 cluster to this latest release, see <a href="#../updating/updating-cluster-cli.adoc#update-service-overview_updating-cluster-cli" class="page unresolved">Updating a cluster by using the CLI</a> for instructions.</p>
</div>
</div>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <p>This page was built using the Antora default UI.</p>
  <p>The source code for this UI is licensed under the terms of the MPL-2.0 license.</p>
</footer>
<script src="../../../_/js/site.js"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
